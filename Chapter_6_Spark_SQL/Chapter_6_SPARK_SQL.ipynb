{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**6.1 SQL**](#6.1-SQL)   \n",
    "[**6.2 Spark SQL**](#6.2-Spark-SQL)   \n",
    "[**6.3 Running Spark SQL Queries**](#6.3-Running-Spark-SQL-Queries)   \n",
    "[**6.3.1 Spark SQL CLI**](#6.3.1-Spark-SQL-CLI)   \n",
    "[**6.3.2 Spark Programmatic SQL Interface**](#6.3.2-Spark-Programmatic-SQL-Interface)   \n",
    "[**6.4 SparkSQL JDBC/ODBC**](#6.4-SparkSQL-JDBC/ODBC)   \n",
    "[**6.5 Thrift Server**](#6.5-Thrift-Server)   \n",
    "[**6.6 Catalog**](#6.6-Catalog)   \n",
    "[**6.7 Database**](#6.7-Database)   \n",
    "[**6.7.1 Create Database**](#6.7.1-Create-Database)   \n",
    "[**6.7.2 Set Database**](#6.7.2-Set-Database)   \n",
    "[**6.7.3 List Database**](#6.7.3-List-Database)     \n",
    "[**6.7.4 Display Current Database**](#6.7.4-Display-Current-Database)  \n",
    "[**6.7.5 Use Default Database**](#6.7.5-Use-Default-Database)   \n",
    "[**6.7.6 Drop Database**](#6.7.6-Drop-Database)   \n",
    "[**6.8 Tables**](#6.8-Tables)   \n",
    "[**6.8.1 Managed Tables**](#6.8.1-Managed-Tables)   \n",
    "[**6.8.2 Unmanaged Tables**](#6.8.2-Unmanaged-Tables)     \n",
    "[**6.9 Create Tables**](#6.9-Create-Tables)  \n",
    "[**6.9.1 Create Managed Tables**](#6.9.1-Create-Managed-Tables)   \n",
    "[**6.9.2 Create Unmanaged Tables**](#6.9.2-Create-Unmanaged-Tables)   \n",
    "[**6.10 Describe Table**](#6.10-Describe-Table)   \n",
    "[**6.11 Display Table**](#6.11-Display-Table)   \n",
    "[**6.12 Drop Table**](#6.12-Drop-Table)   \n",
    "[**6.13 Refresh Table Metadata**](#6.13-Refresh-Table-Metadata)   \n",
    "[**6.14 Cache Table**](#6.14-Cache-Table)   \n",
    "[**6.15 Views**](#6.15-Views)   \n",
    "[**6.15.1 Creating Views**](#6.15.1-Creating-Views)   \n",
    "[**6.15.2 Creating Temporary Views**](#6.15.2-Creating-Temporary-Views)   \n",
    "[**6.15.3 Creating Global Temporary Views**](#6.15.3-Creating-Global-Temporary-Views)   \n",
    "[**6.15.4 Overwrite Views**](#6.15.4-Overwrite-Views)     \n",
    "[**6.15.5 Explain Views**](#6.15.5-Explain-Views)  \n",
    "[**6.15.6 Drop Views**](#6.15.6-Drop-Views)   \n",
    "[**6.16 Select Statements**](#6.16-Select-Statements)   \n",
    "[**6.17 Case Statements**](#6.17-Case-Statements)   \n",
    "[**6.18 Complex Types**](#6.18-Complex-Types)   \n",
    "[**6.18.1 Structs**](#6.18.1-Structs)   \n",
    "[**6.18.2 Lists**](#6.18.2-Lists)   \n",
    "[**6.18.3 Maps**](#6.18.3-Maps)   \n",
    "[**6.19 Functions**](#6.19-Functions)   \n",
    "[**6.20 User-Defined Functions**](#6.20-User-Defined-Functions)   \n",
    "[**6.21 Subqueries**](#6.21-Subqueries)   \n",
    "[**6.22 Interoperate SQL and DataFrames**](#6.22-Interoperate-SQL-and-DataFrames)   \n",
    "[**6.23 Catalog API**](#6.23-Catalog-API)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 SQL\n",
    "Structured Query Language (SQL) is a domain-specific language (DSL) used in relational databases and often in NoSQL databases. To understand more about SQL, checkout the [SQL tutorial link](https://github.com/analyticstensor/sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Spark SQL\n",
    "[Spark SQL](https://spark.apache.org/sql/) is one of the important feature in Spark. Spark supports ANSI SQL 2003 command. Spark SQL helps data analysts, engineer and scientists to utilize the Spark's computation power through either Thrift Server or Spark SQL interface for building datasets and pipelines. Facebook has achieved huge performance gain by converting Hive jobs into Spark Job. [Check out the article](https://engineering.fb.com/core-data/apache-spark-scale-a-60-tb-production-use-case/). Spark SQL is designed for OLAP not OLTP, since OLTP requires extremely low-latency query to execute in milliseconds. [Delta Lake](https://delta.io/) is incubator for supporting RBMDS feature implemented in Databricks. Spark SQL provides several functionality: \n",
    "* It is used to execute SQL queries.\n",
    "* It can read and write data from several formats such as CSV, JSON, Parquet, Avro, ORC, Hive Tables.\n",
    "* It provides an interactive shell to issue SQL queries.\n",
    "* It allows to query data through JDBC/ODBC connectors from external BI data sources such as Tableau, Power BI or any RDBMS such as MySQL, PostgreSQL, MSSQL etc.\n",
    "* It provides an engine upon high-level abstraction API i.e. DataFrames and Datasets.\n",
    "\n",
    "Spark SQL can easily connect to [Hive](https://hive.apache.org/). Hive metastore is used to store metadata of all the Hive tables. Spark SQL can connect seamlessily to Hive metastore to access information. Some configuration need to made before connection to Hive. [Please checkout the link](http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Running Spark SQL Queries\n",
    "Spark SQL queries can be executed multiple ways:\n",
    "* Spark SQL CLI\n",
    "* Spark Programmatic SQL Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Spark SQL CLI\n",
    "Spark SQL CLI is used to executed SQL queries in local mode from command line interface. In CLI, it cannot communicate with Thrift JDBC server. For executing Spark SQL CLI used the command below. Make sure to check environment variable is set if not use absolute Spark path.\n",
    "\n",
    "`spark-sql` # if spark home is set in environment variable  \n",
    "`SPARK_INSTALL_PATH/bin/spark-sql` # if spark home is not set in environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing command successfully, you'll see similar screen shown below.\n",
    "![Figure: 1 Spark CLI](spark_cli.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get help in Spark SQL CLI**  \n",
    "\n",
    "`spark-sql --help`  # display spark help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing command successfully, you'll see similar screen shown below.\n",
    "![Figure: 2 Spark CLI Help](spark_cli_help.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Spark Programmatic SQL Interface\n",
    "Spark provides API to execute SQL in ad hoc. The steps to execute SQL queries are:-\n",
    "1. Create SparkSession\n",
    "2. Apply `sql()` method in sparksession object.\n",
    "\n",
    "The parameter to `sql()` is SQL statement to be executed and return type is DataFrame. Similar to DataFrame, it uses lazily evaluation. We can easily pass complex SQL statement. Through `sql()` method we can perform same task done in DataFrame. So, it is handy if we need to apply complex logic which cannot be easily achieved in DataFrame. Multiline queries can be pass using `multiline string` used in Python. Moreover, the special and powerful feature is that we can interoperate between SQL and DataFrames. i.e. We can apply SQL method and manipulate the same resultset using DataFrame methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark session in Python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"SparkSQLApplicationwithHive\") \\\n",
    "     .enableHiveSupport() \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.67:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkSQLApplicationwithHive</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f96cc76e9b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply SQL method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|analytics tensor|\n",
      "+----------------+\n",
      "|analytics tensor|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 'analytics tensor'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|four|\n",
      "+----+\n",
      "|   4|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 2*2 as four\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single line SQL Statement**  \n",
    "\n",
    "`spark.sql(\"SELECT * from employees\").show()` # this statement will not execute since table doesn't exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi line SQL Statement**  \n",
    "\n",
    "\n",
    "`spark.sql(\"\"\"SELECT first_name, last_name from employees join dept_emp\n",
    "  on employees.emp_no = dept_emp.emp_no where dept_no in ('d001', 'd005'\n",
    "  'd007')\"\"\").show()` # this statement will not execute since table doesn't exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interoperate SQL and DataFrame**\n",
    "\n",
    "`spark.sql(\"\"\"SELECT first_name, last_name from employees join dept_emp\n",
    "                  on employees.emp_no = dept_emp.emp_no where dept_no in ('d001', 'd005'\n",
    "                  'd007') \\\n",
    "           \"\"\") \\\n",
    "      .where(\"first_name like 'Sri%'\") \\\n",
    "      .count()` # this statement will not execute since table doesn't exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 SparkSQL JDBC/ODBC\n",
    "Spark provides both JDBC and ODBC interface for connecting to Spark driver to execute Spark SQL queries. The connections is made through Thrift Server. For example:  \n",
    "* Business Intelligence tools like Tableau, PowerBI etc. wants to connect to Spark to create data visualization dashboard. \n",
    "* Web application wants to run machine learning model using computation in Spark cluster for massive data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Thrift Server\n",
    "[Thrift](https://cwiki.apache.org/confluence/display/Hive/HiveServer) JDBC or ODBC server is used to connect to Spark. Thrift JDBC/ODBC server allows JDBC/ODBC clients to execute SQL queries over JDBC and ODBC protocols in Spark. Spark Thirft server share the same `sparkcontext`. Spark Thrift server is a Spark standalone application.  \n",
    "\n",
    "We are refering to HiveServer2 in Hive that uses Thrift server. Use the command below to start JDBC/ODBC services located in Spark directory.  \n",
    "\n",
    "`./sbin/start-thriftserver.sh`  # to start JDBC/ODBC services.  \n",
    "`./sbin/start-thriftserver.sh --help`  # to get help.  \n",
    "\n",
    "This script also accepts `bin/spark-submit` command. The server listens to `localhost` and port `10000` by default. It can be override by using method below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using environment configuration**\n",
    "\n",
    "`export HIVE_SERVER2_THRIFT_PORT=port_number   \n",
    "export HIVE_SERVER2_THRIFT_BIND_HOST=host_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using System properties**\n",
    "\n",
    "`./sbin/start-thriftserver.sh \\\n",
    "    --hiveconf hive.server2.thrift.port=port_number \\\n",
    "    --hiveconf hive.server2.thrift.bind.host=host_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Beeline](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients) is a JDBC client used to run interactive queries on the command line.\n",
    "\n",
    "`beeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing command successfully, you'll see similar screen shown below.\n",
    "![Figure: 3 Spark CLI Help](spark_beeline.png)  \n",
    "Connect using the `!connect jdbc:hive2://localhost:10000`. Type your machine username and empty password."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6 Catalog\n",
    "Catalog is the highest level abstraction in Spark SQL. Catalog is an abstraction for storing metadata of data stored in tables and information related to databases, tables, functions and views. Catalog is available in the `org.apache.spark.sql.catalog.Catalog` [package](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/sql/catalog/Catalog.html). Several task such as listing/describing databases, tables, functions etc. can be found through catalog fuctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7 Database\n",
    "Database stores and manages tables, views, functions etc. Spark creates default database name `default`. We can create our own database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7.1 Create Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CREATE DATABASE analytics_tensor` # create database in sql cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS analytics_tensor\").show() # create database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7.2 Set Database\n",
    "Use current database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE analytics_tensor\").show() # set current database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7.3 List Database\n",
    "Display list of database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SHOW DATABASES` # in sql cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|       namespace|\n",
      "+----------------+\n",
      "|analytics_tensor|\n",
      "|         default|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show() # show databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7.4 Display Current Database\n",
    "Display current database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SELECT current_database()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|  analytics_tensor|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_database()\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7.5 Use Default Database\n",
    "Set `default` database as current database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`USE default`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7.6 Drop Database\n",
    "Dropping database. It removes all the object from current database such as tables, views, functions etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DROP DATABASE IF EXISTS analytics_tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS analytics_tensor CASCADE\").show() # cascade drops the table in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS analytics_tensor\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|       namespace|\n",
      "+----------------+\n",
      "|analytics_tensor|\n",
      "|         default|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS analytics_tensor\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|       namespace|\n",
      "+----------------+\n",
      "|analytics_tensor|\n",
      "|         default|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.8 Tables\n",
    "Tables is used to store data and metadata. Metadata holds information about a table and data like schema, table name, column names, partitions, location of data for table etc. Spark uses Apache Hive metastore to stores metadata in Hive warehouse default location. i.e. `/user/hive/warehouse`. We can change the default location by setting configuration `spark.sql.warehouse.dir` to other location. Hive also uses `/user/hive/warehouse` to stores all its metadata. Spark SQL is fully compatible with Hive SQL (HiveQL). So, we can copy and paste Hive query into Spark SQL.\n",
    "Spark supports two types of tables:  \n",
    "* Managed Tables\n",
    "* Unmanaged Tables\n",
    "\n",
    "Hive features can be found on this [link](https://spark.apache.org/docs/1.5.2/sql-programming-guide.html#supported-hive-features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.8.1 Managed Tables\n",
    "Managed tables are the table where Spark manages both the metadata and data on the file store. The file can be HDFS, S3 or Azure Blob. Since Spark manages the table, if we drop the table both `metadata` and `data` will be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.8.2 Unmanaged Tables\n",
    "Unmanaged tables are the table where Spark only manages metadata. User need to specify the data location while creating table. Since Spark only manages the metadata of table, if we drop the table only `metadata` will be dropped but the data file will still exists in specified location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.9 Create Tables\n",
    "Table can be create multiple ways in Spark. We will demonstrate various method to create both managed and unmanaged table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.9.1 Create Managed Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# create table with SQL CLI  \n",
    "`CREATE TABLE country(\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    capital STRING)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# create table with Programmatic SQL Interface  \n",
    "\n",
    "`spark.sql(\"CREATE TABLE IF NOT EXISTS country(id INT, name STRING, capital STRING)\")`\n",
    "\n",
    "If the above code will give errors such as:  \n",
    "\n",
    "***AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);;\n",
    "'CreateTable `country12`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Ignore***  \n",
    "\n",
    "**Note:** We neeed to enable hive support while creating SparkSession by adding `enableHiveSupport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS analytics_tensor.country12(id INT, name STRING, capital STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------+\n",
      "|        database|tableName|isTemporary|\n",
      "+----------------+---------+-----------+\n",
      "|analytics_tensor|country12|      false|\n",
      "+----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN analytics_tensor\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------+\n",
      "|        database|tableName|isTemporary|\n",
      "+----------------+---------+-----------+\n",
      "|analytics_tensor|country12|      false|\n",
      "+----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES FROM analytics_tensor\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE analytics_tensor\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create student record**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harry</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paul</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  score\n",
       "0   john     90\n",
       "1  harry     95\n",
       "2   paul    100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create list of lists \n",
    "student_score = [['john', 90], ['harry', 95], ['paul', 100]] \n",
    "  \n",
    "# create dataframe \n",
    "student_score_df = pd.DataFrame(student_score, columns = ['name', 'score']) \n",
    "  \n",
    "# print dataframe. \n",
    "student_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,score\r\n",
      "john,90\r\n",
      "harry,95\r\n",
      "paul,100\r\n"
     ]
    }
   ],
   "source": [
    "# write dataframe to file\n",
    "file_path = '/tmp/student_score.csv'\n",
    "student_score_df.to_csv(file_path, sep=',', header=True, index=False)\n",
    "!cat $file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS student_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table with DataFrame API\n",
    "schema = \"name STRING, score INT\"\n",
    "student_df = spark.read.csv(file_path, schema=schema, header=True)\n",
    "student_df.write.saveAsTable(\"student_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-----------+\n",
      "|        database|    tableName|isTemporary|\n",
      "+----------------+-------------+-----------+\n",
      "|analytics_tensor|    country12|      false|\n",
      "|analytics_tensor|student_score|      false|\n",
      "+----------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| name|score|\n",
      "+-----+-----+\n",
      "| john|   90|\n",
      "|harry|   95|\n",
      "| paul|  100|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM STUDENT_SCORE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| name|score|\n",
      "+-----+-----+\n",
      "|harry|   95|\n",
      "| paul|  100|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from student_score WHERE score >=95\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS student_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create table using CTAS or select query\n",
    "spark.sql(\"CREATE TABLE student_name USING parquet as SELECT * FROM student_score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-----------+\n",
      "|        database|    tableName|isTemporary|\n",
      "+----------------+-------------+-----------+\n",
      "|analytics_tensor|    country12|      false|\n",
      "|analytics_tensor| student_name|      false|\n",
      "|analytics_tensor|student_score|      false|\n",
      "+----------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create table using CTAS or select query if not exists\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS student_name1 USING ORC AS SELECT name FROM student_score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create table by partitioning column\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS partition_student USING parquet\n",
    "            PARTITIONED BY (score)\n",
    "            AS SELECT name, score from student_score\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|col_name               |data_type|comment|\n",
      "+-----------------------+---------+-------+\n",
      "|name                   |string   |null   |\n",
      "|score                  |int      |null   |\n",
      "|# Partition Information|         |       |\n",
      "|# col_name             |data_type|comment|\n",
      "|score                  |int      |null   |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table partition_student\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-----------+\n",
      "|        database|        tableName|isTemporary|\n",
      "+----------------+-----------------+-----------+\n",
      "|analytics_tensor|        country12|      false|\n",
      "|analytics_tensor|partition_student|      false|\n",
      "|analytics_tensor|     student_name|      false|\n",
      "|analytics_tensor|    student_name1|      false|\n",
      "|analytics_tensor|    student_score|      false|\n",
      "+----------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------------------------+\n",
      "|key                               |value                      |\n",
      "+----------------------------------+---------------------------+\n",
      "|spark.app.id                      |local-1606786342664        |\n",
      "|spark.app.name                    |SparkSQLApplicationwithHive|\n",
      "|spark.driver.host                 |192.168.1.67               |\n",
      "|spark.driver.port                 |50394                      |\n",
      "|spark.executor.id                 |driver                     |\n",
      "|spark.master                      |local                      |\n",
      "|spark.rdd.compress                |True                       |\n",
      "|spark.serializer.objectStreamReset|100                        |\n",
      "|spark.sql.catalogImplementation   |hive                       |\n",
      "|spark.submit.deployMode           |client                     |\n",
      "|spark.submit.pyFiles              |                           |\n",
      "|spark.ui.showConsoleProgress      |true                       |\n",
      "+----------------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Spark doesn't support temporary tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.9.2 Create Unmanaged Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read built-in iris dataset from seaborn module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa\n",
       "5           5.4          3.9           1.7          0.4  setosa\n",
       "6           4.6          3.4           1.4          0.3  setosa\n",
       "7           5.0          3.4           1.5          0.2  setosa\n",
       "8           4.4          2.9           1.4          0.2  setosa\n",
       "9           4.9          3.1           1.5          0.1  setosa"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head(10)\n",
    "#iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export dataset to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length,sepal_width,petal_length,petal_width,species\r\n",
      "5.1,3.5,1.4,0.2,setosa\r\n",
      "4.9,3.0,1.4,0.2,setosa\r\n",
      "4.7,3.2,1.3,0.2,setosa\r\n",
      "4.6,3.1,1.5,0.2,setosa\r\n",
      "5.0,3.6,1.4,0.2,setosa\r\n",
      "5.4,3.9,1.7,0.4,setosa\r\n",
      "4.6,3.4,1.4,0.3,setosa\r\n",
      "5.0,3.4,1.5,0.2,setosa\r\n",
      "4.4,2.9,1.4,0.2,setosa\r\n"
     ]
    }
   ],
   "source": [
    "iris_dataset_path = '/tmp/iris_dataset.csv'\n",
    "iris.to_csv(iris_dataset_path, sep=',', header=True, index=False)\n",
    "!head -n 10 $iris_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# create table with SQL CLI  \n",
    "`CREATE TABLE iris(\n",
    "    sepal_length INT,\n",
    "    sepal_width INT COMMENT \"sepal width\",\n",
    "    petal_length INT,\n",
    "    petal_width INT,\n",
    "    species STRING )\n",
    "    USING csv \n",
    "    OPTIONS (header true, path '/tmp/iris_dataset.csv')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create unmanaged table with Programmatic SQL Interface\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS iris\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE iris(\n",
    "    sepal_length FLOAT,\n",
    "    sepal_width FLOAT COMMENT \"sepal width\",\n",
    "    petal_length FLOAT,\n",
    "    petal_width FLOAT,\n",
    "    species STRING )\n",
    "    USING csv \n",
    "    OPTIONS (header true, path \"/tmp/iris_dataset.csv\")\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     150|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM iris\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-----------+\n",
      "|        database|        tableName|isTemporary|\n",
      "+----------------+-----------------+-----------+\n",
      "|analytics_tensor|        country12|      false|\n",
      "|analytics_tensor|             iris|      false|\n",
      "|analytics_tensor|partition_student|      false|\n",
      "|analytics_tensor|     student_name|      false|\n",
      "|analytics_tensor|    student_name1|      false|\n",
      "|analytics_tensor|    student_score|      false|\n",
      "+----------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: string (nullable = true)\n",
      " |-- sepal_width: string (nullable = true)\n",
      " |-- petal_length: string (nullable = true)\n",
      " |-- petal_width: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create iris DataFrame read from csv file\n",
    "iris_df = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(\"/tmp/iris_dataset.csv\")\n",
    "iris_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to create external table\n",
    "spark.sql(\"DROP TABLE IF EXISTS iris_dataset_20200628\")\n",
    "iris_df.write.option('path', \"/tmp/iris_dataset/2020-06-28\").saveAsTable(\"iris_dataset_20200628\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+-----------+\n",
      "|database        |tableName            |isTemporary|\n",
      "+----------------+---------------------+-----------+\n",
      "|analytics_tensor|country12            |false      |\n",
      "|analytics_tensor|iris                 |false      |\n",
      "|analytics_tensor|iris_dataset_20200628|false      |\n",
      "|analytics_tensor|partition_student    |false      |\n",
      "|analytics_tensor|student_name         |false      |\n",
      "|analytics_tensor|student_name1        |false      |\n",
      "|analytics_tensor|student_score        |false      |\n",
      "+----------------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can also use Hive SQL statement to create external table. Refer to [Hive Language Manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual) for Hive Language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# create external table  \n",
    "\n",
    "`CREATE EXTERNAL TABLE student(\n",
    "    student_id int,\n",
    "    student_name string,\n",
    "    student_email string )\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY '\\t'\n",
    "LOCATION '/tmp/student_dataset/'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n",
      "|        database|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|analytics_tensor|           country12|      false|\n",
      "|analytics_tensor|                iris|      false|\n",
      "|analytics_tensor|iris_dataset_2020...|      false|\n",
      "|analytics_tensor|   partition_student|      false|\n",
      "|analytics_tensor|             student|      false|\n",
      "|analytics_tensor|        student_name|      false|\n",
      "|analytics_tensor|       student_name1|      false|\n",
      "|analytics_tensor|       student_score|      false|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE analytics_tensor\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS student\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS student(\n",
    "        student_id int,\n",
    "        student_name string,\n",
    "        student_email string )\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY '\\t'\n",
    "    LOCATION '/tmp/student_dataset/'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE EXTERNAL TABLE `student`(`student_id` INT, `student_name` STRING, `student_email` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '\t',\n",
      "  'field.delim' = '\t'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 'file:/tmp/student_dataset'\n",
      "TBLPROPERTIES (\n",
      "  'transient_lastDdlTime' = '1606789031'\n",
      ")\n",
      "|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CREATE TABLE student\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# create external table using select  \n",
    "\n",
    "`CREATE EXTERNAL TABLE sample_iris\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY '\\t'\n",
    "LOCATION '/tmp/iris_dataset/2020-01-30'\n",
    "AS SELECT * from iris_dataset_20200130`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create external table using select  \n",
    "spark.sql(\"\"\"\n",
    "        CREATE EXTERNAL TABLE sample_iris\n",
    "        ROW FORMAT DELIMITED\n",
    "        FIELDS TERMINATED BY '\\t'\n",
    "        LOCATION '/tmp/iris_dataset/2020-02-01'\n",
    "        AS SELECT * from iris_dataset_20200628\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sample_iris\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform demo for Lab_3 (at the end of section)**\n",
    "* Load data into table from file path.\n",
    "* Insert data into from other table.\n",
    "* Insert data into partition table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.10 Describe Table\n",
    "Used to display table schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DESCRIBE TABLE iris` # used to display table schema of iris  \n",
    "`SHOW PARTITIONS partition_student` # used display partititon schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+\n",
      "|    col_name|data_type|    comment|\n",
      "+------------+---------+-----------+\n",
      "|sepal_length|    float|       null|\n",
      "| sepal_width|    float|sepal width|\n",
      "|petal_length|    float|       null|\n",
      "| petal_width|    float|       null|\n",
      "|     species|   string|       null|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE iris\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "|score=100|\n",
      "|score=90 |\n",
      "|score=95 |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS partition_student\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.11 Display Table\n",
    "Used to display current tables in database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SHOW TABLES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+-----------+\n",
      "|database        |tableName            |isTemporary|\n",
      "+----------------+---------------------+-----------+\n",
      "|analytics_tensor|country12            |false      |\n",
      "|analytics_tensor|iris                 |false      |\n",
      "|analytics_tensor|iris_dataset_20200628|false      |\n",
      "|analytics_tensor|partition_student    |false      |\n",
      "|analytics_tensor|sample_iris          |false      |\n",
      "|analytics_tensor|student              |false      |\n",
      "|analytics_tensor|student_name         |false      |\n",
      "|analytics_tensor|student_name1        |false      |\n",
      "|analytics_tensor|student_score        |false      |\n",
      "+----------------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    tableName|\n",
      "+-------------+\n",
      "|      student|\n",
      "| student_name|\n",
      "|student_name1|\n",
      "|student_score|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES like 'student*'\").select(\"tableName\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.12 Drop Table\n",
    "Used to drop the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DROP TABLE student_name1` # drop table, produce error if table doesn't exists.  \n",
    "`DROP TABLE IF EXISTS student_name1` #  drop table by checking if table exists it won't give error if table doesn't exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS tmp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS student_name1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS country12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS sample_iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n",
      "|        database|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|analytics_tensor|                iris|      false|\n",
      "|analytics_tensor|iris_dataset_2020...|      false|\n",
      "|analytics_tensor|   partition_student|      false|\n",
      "|analytics_tensor|             student|      false|\n",
      "|analytics_tensor|        student_name|      false|\n",
      "|analytics_tensor|       student_score|      false|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN analytics_tensor\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.13 Refresh Table Metadata\n",
    "Used to refresh table metadata. Refreshing metadata helps to read table from recent dataset. Sometime, data are not synched due to caches and re-partition of data so refreshing is required. Refreshing metadata can be performed by:\n",
    "* REFRESH TABLE: It refresh all the cached file associated with the tables.\n",
    "* REPAIR TABLE: It refresh the partitions maintained in the catalog for tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`REFRESH TABLE partition_student`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MSCK REPAIR TABLE partition_student`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"REFRESH TABLE partition_student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"MSCK REPAIR TABLE partition_student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.14 Cache Table\n",
    "Table can be cache and uncache similar to DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CACHE TABLE student`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`UNCACHE TABLE student`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CACHE TABLE student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"UNCACHE TABLE student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15 Views\n",
    "View is an virtual table that specifies a set of transformation on top of existing table. It doesn't store data in file like table but retrieves data from table defined on view. It is just a saved query plans. When data is selected from view it will run query on base table to retrieve data. Basically, view is similar to creating new DataFrame from an existing DataFrame. There are many advantage of views. User doesn't feel any different between table and view while executing query. \n",
    "View can be created in different modes:\n",
    "* global\n",
    "* set to a database\n",
    "* set per session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15.1 Creating Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CREATE VIEW iris_setosa_vw AS\n",
    "    SELECT * FROM iris WHERE species = 'setosa'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS default.iris_setosa_vw AS\n",
    "        SELECT * FROM iris WHERE species = 'setosa'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n",
      "|        database|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|analytics_tensor|                iris|      false|\n",
      "|analytics_tensor|iris_dataset_2020...|      false|\n",
      "|analytics_tensor|   partition_student|      false|\n",
      "|analytics_tensor|         sample_iris|      false|\n",
      "|analytics_tensor|             student|      false|\n",
      "|analytics_tensor|        student_name|      false|\n",
      "|analytics_tensor|       student_score|      false|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------+\n",
      "|database|tableName            |isTemporary|\n",
      "+--------+---------------------+-----------+\n",
      "|default |iris_setosa_june29_vw|false      |\n",
      "|default |iris_setosa_vw       |false      |\n",
      "+--------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES in DEFAULT\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|species|\n",
      "+-------+\n",
      "| setosa|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct species from default.iris_setosa_vw\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   species|\n",
      "+----------+\n",
      "| virginica|\n",
      "|versicolor|\n",
      "|    setosa|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct species from iris\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15.2 Creating Temporary Views\n",
    "Temporary view are only available during current session. They are not registered to a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CREATE TEMP VIEW iris_setosa_tmp_vw AS\n",
    "    SELECT * FROM iris WHERE species = 'setosa'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS iris_setosa_tmp_vw\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TEMP VIEW iris_setosa_tmp_vw AS\n",
    "        SELECT * FROM iris WHERE species = 'setosa'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n",
      "|        database|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|analytics_tensor|                iris|      false|\n",
      "|analytics_tensor|iris_dataset_2020...|      false|\n",
      "|analytics_tensor|   partition_student|      false|\n",
      "|analytics_tensor|         sample_iris|      false|\n",
      "|analytics_tensor|             student|      false|\n",
      "|analytics_tensor|        student_name|      false|\n",
      "|analytics_tensor|       student_score|      false|\n",
      "|                |  iris_setosa_tmp_vw|       true|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|iris_setosa_june2...|      false|\n",
      "| default|      iris_setosa_vw|      false|\n",
      "|        |  iris_setosa_tmp_vw|       true|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES in DEFAULT\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15.3 Creating Global Temporary Views\n",
    "Global temporary are viewable across the entire Spark application. They can be removed at the end of session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CREATE GLOBAL TEMP VIEW iris_setosa_global_tmp_vw AS\n",
    "    SELECT * FROM iris WHERE species = 'setosa'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists iris_setosa_global_tmp_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n",
      "|        database|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|analytics_tensor|                iris|      false|\n",
      "|analytics_tensor|iris_dataset_2020...|      false|\n",
      "|analytics_tensor|   partition_student|      false|\n",
      "|analytics_tensor|         sample_iris|      false|\n",
      "|analytics_tensor|             student|      false|\n",
      "|analytics_tensor|        student_name|      false|\n",
      "|analytics_tensor|       student_score|      false|\n",
      "|                |  iris_setosa_tmp_vw|       true|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Temporary view 'iris_setosa_global_tmp_vw' already exists;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.catalyst.analysis.TempTableAlreadyExistsException: Temporary view 'iris_setosa_global_tmp_vw' already exists;\n\tat org.apache.spark.sql.catalyst.catalog.GlobalTempViewManager.create(GlobalTempViewManager.scala:61)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createGlobalTempView(SessionCatalog.scala:547)\n\tat org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:71)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:69)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:80)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:226)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3407)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3403)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:226)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-665b590b131f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mCREATE\u001b[0m \u001b[0mGLOBAL\u001b[0m \u001b[0mTEMP\u001b[0m \u001b[0mVIEW\u001b[0m \u001b[0miris_setosa_global_tmp_vw\u001b[0m \u001b[0mAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mSELECT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0miris\u001b[0m \u001b[0mWHERE\u001b[0m \u001b[0mspecies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'setosa'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \"\"\")\n\u001b[0m",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Temporary view 'iris_setosa_global_tmp_vw' already exists;"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE GLOBAL TEMP VIEW iris_setosa_global_tmp_vw AS\n",
    "        SELECT * FROM iris WHERE species = 'setosa'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** All the global temporary view will be stored in system temporary database known as `global_temp`. These views are access from different sessions and will be alive until the application ends. To retrive the views created as global temporary we should always specify the `global_temp` database other wise it will provide the error indicating, `table or view not found`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: iris_setosa_global_tmp_vw; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [iris_setosa_global_tmp_vw]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: iris_setosa_global_tmp_vw; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [iris_setosa_global_tmp_vw]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:153)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:66)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-c3fd56f2c6e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * from iris_setosa_global_tmp_vw \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: iris_setosa_global_tmp_vw; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [iris_setosa_global_tmp_vw]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from iris_setosa_global_tmp_vw \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from global_temp.iris_setosa_global_tmp_vw \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+\n",
      "|   database|           tableName|isTemporary|\n",
      "+-----------+--------------------+-----------+\n",
      "|global_temp|iris_setosa_globa...|       true|\n",
      "|           |  iris_setosa_tmp_vw|       true|\n",
      "+-----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN GLOBAL_TEMP\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n",
      "|        database|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|analytics_tensor|                iris|      false|\n",
      "|analytics_tensor|iris_dataset_2020...|      false|\n",
      "|analytics_tensor|   partition_student|      false|\n",
      "|analytics_tensor|         sample_iris|      false|\n",
      "|analytics_tensor|             student|      false|\n",
      "|analytics_tensor|        student_name|      false|\n",
      "|analytics_tensor|       student_score|      false|\n",
      "|                |  iris_setosa_tmp_vw|       true|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15.4 Overwrite Views\n",
    "View can be overwrite on existing view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CREATE OR REPLACE TEMP VIEW iris_setosa_tmp_vw AS\n",
    "    SELECT * FROM iris WHERE species in ('setosa') and sepal_length > 4.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW iris_setosa_tmp_vw AS \n",
    "          SELECT * FROM iris WHERE species in ('setosa') and sepal_length > 4.5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "|         5.4|        3.4|         1.7|        0.2| setosa|\n",
      "|         5.1|        3.7|         1.5|        0.4| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iris_setosa_tmp_vw\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15.5 Explain Views\n",
    "Describe the view explaination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EXPLAIN SELECT * FROM iris_setosa_tmp_vw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\n",
      "*(1) Project [sepal_length#324, sepal_width#325, petal_length#326, petal_width#327, species#328]\n",
      "+- *(1) Filter (((isnotnull(species#328) AND isnotnull(sepal_length#324)) AND (species#328 = setosa)) AND (cast(sepal_length#324 as double) > 4.5))\n",
      "   +- FileScan csv analytics_tensor.iris[sepal_length#324,sepal_width#325,petal_length#326,petal_width#327,species#328] Batched: false, DataFilters: [isnotnull(species#328), isnotnull(sepal_length#324), (species#328 = setosa), (cast(sepal_length#..., Format: CSV, Location: InMemoryFileIndex[file:/tmp/iris_dataset.csv], PartitionFilters: [], PushedFilters: [IsNotNull(species), IsNotNull(sepal_length), EqualTo(species,setosa)], ReadSchema: struct<sepal_length:float,sepal_width:float,petal_length:float,petal_width:float,species:string>\n",
      "\n",
      "|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"EXPLAIN SELECT * FROM iris_setosa_tmp_vw\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\n",
      "*(1) Project [sepal_length#324, sepal_width#325, petal_length#326, petal_width#327, species#328]\n",
      "+- *(1) Filter (((isnotnull(species#328) AND isnotnull(sepal_length#324)) AND (species#328 = setosa)) AND (cast(sepal_length#324 as double) > 4.5))\n",
      "   +- FileScan csv analytics_tensor.iris[sepal_length#324,sepal_width#325,petal_length#326,petal_width#327,species#328] Batched: false, DataFilters: [isnotnull(species#328), isnotnull(sepal_length#324), (species#328 = setosa), (cast(sepal_length#..., Format: CSV, Location: InMemoryFileIndex[file:/tmp/iris_dataset.csv], PartitionFilters: [], PushedFilters: [IsNotNull(species), IsNotNull(sepal_length), EqualTo(species,setosa)], ReadSchema: struct<sepal_length:float,sepal_width:float,petal_length:float,petal_width:float,species:string>\n",
      "\n",
      "|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"EXPLAIN table iris_setosa_tmp_vw\").show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EXPLAIN SELECT * FROM iris WHERE species = 'setosa'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\n",
      "*(1) Project [sepal_length#508, sepal_width#509, petal_length#510, petal_width#511, species#512]\n",
      "+- *(1) Filter (isnotnull(species#512) AND (species#512 = setosa))\n",
      "   +- FileScan csv analytics_tensor.iris[sepal_length#508,sepal_width#509,petal_length#510,petal_width#511,species#512] Batched: false, DataFilters: [isnotnull(species#512), (species#512 = setosa)], Format: CSV, Location: InMemoryFileIndex[file:/tmp/iris_dataset.csv], PartitionFilters: [], PushedFilters: [IsNotNull(species), EqualTo(species,setosa)], ReadSchema: struct<sepal_length:float,sepal_width:float,petal_length:float,petal_width:float,species:string>\n",
      "\n",
      "|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"EXPLAIN SELECT * FROM iris WHERE species = 'setosa'\").show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.15.6 Drop Views\n",
    "Dropping view is similar to dropping the table. Dropping view doesn't drop data, only the metadata of view is dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DROP VIEW IF EXISTS iris_setosa_vw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP VIEW IF EXISTS iris_setosa_vw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.16 Select Statements\n",
    "Spark support ANSI SQL for SELECT expression shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELECT** [**ALL**|**DISTINCT**] named_expression[, named_expression, ...]  \n",
    "&emsp;**FROM** relation[, relation, ...]  \n",
    "&emsp;[lateral_view[, lateral_view, ...]]  \n",
    "&emsp;[**WHERE** boolean_expression]  \n",
    "&emsp;[aggregation [**HAVING** boolean_expression]]  \n",
    "&emsp;[**ORDER BY** sort_expressions]  \n",
    "&emsp;[**CLUSTER BY** expressions]  \n",
    "&emsp;[DISTRIBUTE BY expressions]  \n",
    "&emsp;[SORT BY sort_expressions]  \n",
    "&emsp;[WINDOW named_window[, WINDOW named_window, ...]]  \n",
    "&emsp;[**LIMIT** num_rows]  \n",
    "\n",
    "named_expression:  \n",
    "&emsp;: expression [**AS alias**]    \n",
    "\n",
    "relation:  \n",
    "&emsp;| join_relation  \n",
    "&emsp;| (**table_name**|query|relation) [sample] [**AS alias**]  \n",
    "&emsp;: **VALUES** (expressions)[, (expressions), ...]  \n",
    "&emsp;&emsp; [**AS** **(column_name**[, **column_name**, ...])]  \n",
    "\n",
    "expressions:  \n",
    "&emsp;: expression[, expression, ...]  \n",
    "\n",
    "sort_expressions:  \n",
    "&emsp;: expression [**ASC**|**DESC**][, expression \\[**ASC**|**DESC**\\], ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELECT Statements example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iris\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+------------+\n",
      "| sl| sw|   species|current_date|\n",
      "+---+---+----------+------------+\n",
      "|7.9|3.8| VIRGINICA|  2020-12-05|\n",
      "|7.7|3.8| VIRGINICA|  2020-12-05|\n",
      "|7.7|2.6| VIRGINICA|  2020-12-05|\n",
      "|7.7|2.8| VIRGINICA|  2020-12-05|\n",
      "|7.7|3.0| VIRGINICA|  2020-12-05|\n",
      "|7.6|3.0| VIRGINICA|  2020-12-05|\n",
      "|7.4|2.8| VIRGINICA|  2020-12-05|\n",
      "|7.3|2.9| VIRGINICA|  2020-12-05|\n",
      "|7.2|3.6| VIRGINICA|  2020-12-05|\n",
      "|7.2|3.2| VIRGINICA|  2020-12-05|\n",
      "|7.2|3.0| VIRGINICA|  2020-12-05|\n",
      "|7.1|3.0| VIRGINICA|  2020-12-05|\n",
      "|7.0|3.2|VERSICOLOR|  2020-12-05|\n",
      "|6.9|3.1|VERSICOLOR|  2020-12-05|\n",
      "|6.9|3.2| VIRGINICA|  2020-12-05|\n",
      "|6.9|3.1| VIRGINICA|  2020-12-05|\n",
      "|6.9|3.1| VIRGINICA|  2020-12-05|\n",
      "|6.8|2.8|VERSICOLOR|  2020-12-05|\n",
      "|6.8|3.0| VIRGINICA|  2020-12-05|\n",
      "|6.8|3.2| VIRGINICA|  2020-12-05|\n",
      "|6.7|3.1|VERSICOLOR|  2020-12-05|\n",
      "|6.7|3.0|VERSICOLOR|  2020-12-05|\n",
      "|6.7|3.1|VERSICOLOR|  2020-12-05|\n",
      "|6.7|2.5| VIRGINICA|  2020-12-05|\n",
      "|6.7|3.3| VIRGINICA|  2020-12-05|\n",
      "|6.7|3.1| VIRGINICA|  2020-12-05|\n",
      "|6.7|3.3| VIRGINICA|  2020-12-05|\n",
      "|6.7|3.0| VIRGINICA|  2020-12-05|\n",
      "|6.6|2.9|VERSICOLOR|  2020-12-05|\n",
      "|6.6|3.0|VERSICOLOR|  2020-12-05|\n",
      "|6.5|2.8|VERSICOLOR|  2020-12-05|\n",
      "|6.5|3.0| VIRGINICA|  2020-12-05|\n",
      "|6.5|3.2| VIRGINICA|  2020-12-05|\n",
      "|6.5|3.0| VIRGINICA|  2020-12-05|\n",
      "|6.5|3.0| VIRGINICA|  2020-12-05|\n",
      "|6.4|3.2|VERSICOLOR|  2020-12-05|\n",
      "|6.4|2.9|VERSICOLOR|  2020-12-05|\n",
      "|6.4|2.7| VIRGINICA|  2020-12-05|\n",
      "|6.4|3.2| VIRGINICA|  2020-12-05|\n",
      "|6.4|2.8| VIRGINICA|  2020-12-05|\n",
      "|6.4|2.8| VIRGINICA|  2020-12-05|\n",
      "|6.4|3.1| VIRGINICA|  2020-12-05|\n",
      "|6.3|3.3|VERSICOLOR|  2020-12-05|\n",
      "|6.3|2.5|VERSICOLOR|  2020-12-05|\n",
      "|6.3|2.3|VERSICOLOR|  2020-12-05|\n",
      "|6.3|3.3| VIRGINICA|  2020-12-05|\n",
      "|6.3|2.9| VIRGINICA|  2020-12-05|\n",
      "|6.3|2.7| VIRGINICA|  2020-12-05|\n",
      "|6.3|2.8| VIRGINICA|  2020-12-05|\n",
      "|6.3|3.4| VIRGINICA|  2020-12-05|\n",
      "+---+---+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT sepal_length as sl ,sepal_width as sw, upper(species) as species, current_date()\\\n",
    "          as current_date FROM iris\\\n",
    "          order by sepal_length desc limit 50\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   species|\n",
      "+----------+\n",
      "| virginica|\n",
      "|versicolor|\n",
      "|    setosa|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct species FROM iris\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+----------------+----------------+-----------------+----------------+\n",
      "|   species|min_sepal_len|max_sepal_w|min(sepal_width)|max(sepal_width)|min(petal_length)|max_petal_length|\n",
      "+----------+-------------+-----------+----------------+----------------+-----------------+----------------+\n",
      "| virginica|          4.9|        7.9|             2.2|             3.8|              4.5|             6.9|\n",
      "|versicolor|          4.9|        7.0|             2.0|             3.4|              3.0|             5.1|\n",
      "|    setosa|          4.3|        5.8|             2.3|             4.4|              1.0|             1.9|\n",
      "+----------+-------------+-----------+----------------+----------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        species, min(sepal_length) min_sepal_len, max(sepal_length) max_sepal_w,\n",
    "        min(sepal_width), max(sepal_width),\n",
    "        min(petal_length), max(petal_length) as max_petal_length\n",
    "    FROM iris\n",
    "    GROUP BY species\"\"\"\n",
    "    ).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|sepal_length|\n",
      "+------------+\n",
      "|         4.3|\n",
      "|         4.4|\n",
      "|         4.5|\n",
      "|         4.6|\n",
      "|         4.7|\n",
      "|         4.8|\n",
      "|         4.9|\n",
      "|         5.0|\n",
      "|         5.1|\n",
      "|         5.2|\n",
      "|         5.3|\n",
      "|         5.4|\n",
      "|         5.5|\n",
      "|         5.6|\n",
      "|         5.7|\n",
      "|         5.8|\n",
      "|         5.9|\n",
      "|         6.0|\n",
      "|         6.1|\n",
      "|         6.2|\n",
      "|         6.3|\n",
      "|         6.4|\n",
      "|         6.5|\n",
      "|         6.6|\n",
      "|         6.7|\n",
      "|         6.8|\n",
      "|         6.9|\n",
      "|         7.0|\n",
      "|         7.1|\n",
      "|         7.2|\n",
      "|         7.3|\n",
      "|         7.4|\n",
      "|         7.6|\n",
      "|         7.7|\n",
      "|         7.9|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct sepal_length from iris order by sepal_length\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.17 Case Statements\n",
    "Case statements is used to replace data value based on certain condition either defined in business logic or temporary explicit values for future calcuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SELECT \n",
    "    sepal_length,\n",
    "    CASE WHEN sepal_length >= 4.3 AND sepal_length <5.0 THEN 'Small'\n",
    "         WHEN sepal_length >=5.0 AND sepal_length <5.5 THEN 'Medium'\n",
    "         WHEN sepal_length >=5.5 AND sepal_length <6.0 THEN 'Large'\n",
    "         WHEN sepal_length >=6.0 THEN 'Extra Large'            \n",
    "    ELSE 'UNKNOWN' END AS length_range\n",
    "FROM iris_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|sepal_length|length_range|\n",
      "+------------+------------+\n",
      "|6.7         |Extra Large |\n",
      "|6.5         |Extra Large |\n",
      "|6.0         |Extra Large |\n",
      "|7.0         |Extra Large |\n",
      "|6.0         |Extra Large |\n",
      "|6.8         |Extra Large |\n",
      "|6.0         |Extra Large |\n",
      "|6.3         |Extra Large |\n",
      "|6.7         |Extra Large |\n",
      "|6.0         |Extra Large |\n",
      "|6.3         |Extra Large |\n",
      "|6.7         |Extra Large |\n",
      "|6.1         |Extra Large |\n",
      "|6.1         |Extra Large |\n",
      "|6.2         |Extra Large |\n",
      "|6.1         |Extra Large |\n",
      "|6.3         |Extra Large |\n",
      "|6.6         |Extra Large |\n",
      "|7.1         |Extra Large |\n",
      "|6.5         |Extra Large |\n",
      "+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        sepal_length,\n",
    "        CASE /*WHEN sepal_length >= 4.3 AND sepal_length <5.0 THEN 'Small' -- to comment */\n",
    "             WHEN sepal_length >=5.0 AND sepal_length <5.5 THEN 'Medium'\n",
    "             WHEN sepal_length >=5.5 AND sepal_length <6.0 THEN 'Large'\n",
    "             WHEN sepal_length >=6.0 THEN 'Extra Large'            \n",
    "        ELSE 'UNKNOWN' END AS length_range\n",
    "    FROM iris\n",
    "    order by length_range\n",
    "\"\"\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.18 Complex Types\n",
    "Spark SQL has three core complex types:\n",
    "* structs\n",
    "* lists\n",
    "* maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.18.1 Structs\n",
    "Structs are similar to maps. Both struct and maps are used to query nested data. To create structs from existing table we need to wrap of sets of columns or column expression in parentheses. Both structs and map data type can also be created while defining the table structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE VIEW IF NOT EXISTS struct_example_vw AS\\\n",
    "            SELECT species, (sepal_length, sepal_width) as sepal_info, (petal_length, petal_width) as petal_info\\\n",
    "            FROM iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------+-------+\n",
      "|col_name                    |data_type                                   |comment|\n",
      "+----------------------------+--------------------------------------------+-------+\n",
      "|species                     |string                                      |null   |\n",
      "|sepal_info                  |struct<sepal_length:float,sepal_width:float>|null   |\n",
      "|petal_info                  |struct<petal_length:float,petal_width:float>|null   |\n",
      "|                            |                                            |       |\n",
      "|# Detailed Table Information|                                            |       |\n",
      "|Database                    |analytics_tensor                            |       |\n",
      "|Table                       |struct_example_vw                           |       |\n",
      "|Owner                       |kcmahesh                                    |       |\n",
      "|Created Time                |Sun Dec 06 06:56:03 NPT 2020                |       |\n",
      "|Last Access                 |UNKNOWN                                     |       |\n",
      "+----------------------------+--------------------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED struct_example_vw\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------+-------+\n",
      "|col_name                    |data_type                                   |comment|\n",
      "+----------------------------+--------------------------------------------+-------+\n",
      "|species                     |string                                      |null   |\n",
      "|sepal_info                  |struct<sepal_length:float,sepal_width:float>|null   |\n",
      "|petal_info                  |struct<petal_length:float,petal_width:float>|null   |\n",
      "|                            |                                            |       |\n",
      "|# Detailed Table Information|                                            |       |\n",
      "|Database                    |analytics_tensor                            |       |\n",
      "|Table                       |struct_example_vw                           |       |\n",
      "|Owner                       |kcmahesh                                    |       |\n",
      "|Created Time                |Sun Dec 06 06:56:03 NPT 2020                |       |\n",
      "|Last Access                 |UNKNOWN                                     |       |\n",
      "+----------------------------+--------------------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESC EXTENDED struct_example_vw\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the struct type we need to use dot syntax by specifying column name followed by data_type's name. We can also use `column_name.*` to select all the columns for given struct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+\n",
      "|species|sepal_info|petal_info|\n",
      "+-------+----------+----------+\n",
      "| setosa|[5.1, 3.5]|[1.4, 0.2]|\n",
      "| setosa|[4.9, 3.0]|[1.4, 0.2]|\n",
      "| setosa|[4.7, 3.2]|[1.3, 0.2]|\n",
      "| setosa|[4.6, 3.1]|[1.5, 0.2]|\n",
      "| setosa|[5.0, 3.6]|[1.4, 0.2]|\n",
      "| setosa|[5.4, 3.9]|[1.7, 0.4]|\n",
      "| setosa|[4.6, 3.4]|[1.4, 0.3]|\n",
      "| setosa|[5.0, 3.4]|[1.5, 0.2]|\n",
      "| setosa|[4.4, 2.9]|[1.4, 0.2]|\n",
      "| setosa|[4.9, 3.1]|[1.5, 0.1]|\n",
      "| setosa|[5.4, 3.7]|[1.5, 0.2]|\n",
      "| setosa|[4.8, 3.4]|[1.6, 0.2]|\n",
      "| setosa|[4.8, 3.0]|[1.4, 0.1]|\n",
      "| setosa|[4.3, 3.0]|[1.1, 0.1]|\n",
      "| setosa|[5.8, 4.0]|[1.2, 0.2]|\n",
      "| setosa|[5.7, 4.4]|[1.5, 0.4]|\n",
      "| setosa|[5.4, 3.9]|[1.3, 0.4]|\n",
      "| setosa|[5.1, 3.5]|[1.4, 0.3]|\n",
      "| setosa|[5.7, 3.8]|[1.7, 0.3]|\n",
      "| setosa|[5.1, 3.8]|[1.5, 0.3]|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from struct_example_vw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|sepal_length|petal_width|\n",
      "+------------+-----------+\n",
      "|         5.1|        0.2|\n",
      "|         4.9|        0.2|\n",
      "|         4.7|        0.2|\n",
      "|         4.6|        0.2|\n",
      "|         5.0|        0.2|\n",
      "|         5.4|        0.4|\n",
      "|         4.6|        0.3|\n",
      "|         5.0|        0.2|\n",
      "|         4.4|        0.2|\n",
      "|         4.9|        0.1|\n",
      "|         5.4|        0.2|\n",
      "|         4.8|        0.2|\n",
      "|         4.8|        0.1|\n",
      "|         4.3|        0.1|\n",
      "|         5.8|        0.2|\n",
      "|         5.7|        0.4|\n",
      "|         5.4|        0.4|\n",
      "|         5.1|        0.3|\n",
      "|         5.7|        0.3|\n",
      "|         5.1|        0.3|\n",
      "+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT sepal_info.sepal_length, petal_info.petal_width from struct_example_vw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|\n",
      "+------------+-----------+------------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|\n",
      "|         4.9|        3.0|         1.4|        0.2|\n",
      "|         4.7|        3.2|         1.3|        0.2|\n",
      "|         4.6|        3.1|         1.5|        0.2|\n",
      "|         5.0|        3.6|         1.4|        0.2|\n",
      "|         5.4|        3.9|         1.7|        0.4|\n",
      "|         4.6|        3.4|         1.4|        0.3|\n",
      "|         5.0|        3.4|         1.5|        0.2|\n",
      "|         4.4|        2.9|         1.4|        0.2|\n",
      "|         4.9|        3.1|         1.5|        0.1|\n",
      "|         5.4|        3.7|         1.5|        0.2|\n",
      "|         4.8|        3.4|         1.6|        0.2|\n",
      "|         4.8|        3.0|         1.4|        0.1|\n",
      "|         4.3|        3.0|         1.1|        0.1|\n",
      "|         5.8|        4.0|         1.2|        0.2|\n",
      "|         5.7|        4.4|         1.5|        0.4|\n",
      "|         5.4|        3.9|         1.3|        0.4|\n",
      "|         5.1|        3.5|         1.4|        0.3|\n",
      "|         5.7|        3.8|         1.7|        0.3|\n",
      "|         5.1|        3.8|         1.5|        0.3|\n",
      "+------------+-----------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT sepal_info.*, petal_info.* from struct_example_vw\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.18.2 Lists\n",
    "We can create list using `collect_list` or `collect_set` function which create list with duplicate values and create list without duplicate values respectively. These are aggregate function so it is only used in aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|species   |spl_length                                                                                                                                                                                                                                                                                                  |\n",
      "+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|virginica |[6.3, 5.8, 7.1, 6.3, 6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5, 7.7, 7.7, 6.0, 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2, 7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6.0, 6.9, 6.7, 6.9, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9, 5.1, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9, 5.1]|\n",
      "|versicolor|[7.0, 6.4, 6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 5.0, 5.9, 6.0, 6.1, 5.6, 6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 6.1, 6.4, 6.6, 6.8, 6.7, 6.0, 5.7, 5.5, 5.5, 5.8, 6.0, 5.4, 6.0, 6.7, 6.3, 5.6, 5.5, 5.5, 6.1, 5.8, 5.0, 5.6, 5.7, 5.7, 6.2, 5.1, 5.7]                                                  |\n",
      "|setosa    |[5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 5.4, 5.1, 4.6, 5.1, 4.8, 5.0, 5.0, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 5.5, 4.9, 5.0, 5.5, 4.9, 4.4, 5.1, 5.0, 4.5, 4.4, 5.0, 5.1, 4.8, 5.1, 4.6, 5.3, 5.0]                                                  |\n",
      "+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT species, collect_list(sepal_length) as spl_length from iris\\\n",
    "             GROUP BY species\\\n",
    "          \").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------------------------------------------------------------------------------------+\n",
      "|species   |spl_length                                                                                                    |\n",
      "+----------+--------------------------------------------------------------------------------------------------------------+\n",
      "|virginica |[7.3, 6.0, 7.6, 4.9, 6.8, 7.1, 6.3, 7.9, 5.8, 7.4, 6.1, 7.7, 6.9, 5.6, 7.2, 6.4, 6.7, 5.9, 6.2, 5.1, 5.7, 6.5]|\n",
      "|versicolor|[5.2, 5.4, 6.0, 4.9, 6.8, 5.5, 6.3, 6.6, 5.8, 6.1, 6.9, 5.0, 5.6, 6.4, 6.7, 5.9, 6.2, 7.0, 5.1, 5.7, 6.5]     |\n",
      "|setosa    |[5.2, 5.4, 4.9, 5.5, 4.4, 4.7, 5.8, 5.0, 4.5, 5.3, 4.8, 5.1, 4.3, 5.7, 4.6]                                   |\n",
      "+----------+--------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT species, collect_set(sepal_length) as spl_length from iris\\\n",
    "             GROUP BY species\\\n",
    "          \").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create list manually using `array()` method shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+\n",
      "|species|  items_1|     items_2|\n",
      "+-------+---------+------------+\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "| setosa|[A, B, C]|[10, 20, 30]|\n",
      "+-------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select species, array('A', 'B', 'C') as items_1, array(10,20,30) as items_2 from iris\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE VIEW IF NOT EXISTS list_example_vw as SELECT species, collect_set(sepal_length) as spl_length from iris\\\n",
    "             GROUP BY species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|species                     |string                      |null   |\n",
      "|spl_length                  |array<float>                |null   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Database                    |analytics_tensor            |       |\n",
      "|Table                       |list_example_vw             |       |\n",
      "|Owner                       |kcmahesh                    |       |\n",
      "|Created Time                |Sun Dec 06 07:05:18 NPT 2020|       |\n",
      "|Last Access                 |UNKNOWN                     |       |\n",
      "|Created By                  |Spark 3.0.0-preview         |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESC EXTENDED list_example_vw\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|species   |sorted_spl_length|\n",
      "+----------+-----------------+\n",
      "|virginica |7.9              |\n",
      "|versicolor|7.0              |\n",
      "|setosa    |5.8              |\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select species, array_max(spl_length) as sorted_spl_length from list_example_vw\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.18.3 Maps\n",
    "Maps has keytype and valuetype. Key type and value type can be any data types such as integer, string, array, map and struct. Maps can be created while defining table or can be consturucted by specifying map functions such as `map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP VIEW IF EXISTS map_example_vw\")\n",
    "spark.sql(\"CREATE VIEW IF NOT EXISTS map_example_vw as SELECT map(10, 'John', 20 , 'Bobby') as map_types from iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|map_types                   |map<int,string>             |null   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Database                    |analytics_tensor            |       |\n",
      "|Table                       |map_example_vw              |       |\n",
      "|Owner                       |kcmahesh                    |       |\n",
      "|Created Time                |Sun Dec 06 07:08:09 NPT 2020|       |\n",
      "|Last Access                 |UNKNOWN                     |       |\n",
      "|Created By                  |Spark 3.0.0-preview         |       |\n",
      "|Type                        |VIEW                        |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESC EXTENDED map_example_vw\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------------------------+\n",
      "|map_keys(map_types)|map_values(map_types)|map_concat(map_types)    |\n",
      "+-------------------+---------------------+-------------------------+\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "|[10, 20]           |[John, Bobby]        |[10 -> John, 20 -> Bobby]|\n",
      "+-------------------+---------------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT map_keys(map_types), map_values(map_types), map_concat(map_types) from map_example_vw\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Example with of complex types`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        CREATE TABLE user_profile(\n",
    "        user_id int,\n",
    "        user_name string,\n",
    "        profile_name string,\n",
    "        hobbies array<string>,\n",
    "        liked_image map<string, array<string>>,\n",
    "        -- struct with same column name user_id\n",
    "        friends_info struct<first_name: string, last_name: string, user_name: string, user_id: int>,\n",
    "        -- struct with element of an array\n",
    "        address array<struct<street:string, city: string, country: string>>,\n",
    "        -- struct with map\n",
    "        like_url map<string, struct <year:int, url: string, details: string>>        \n",
    "        )\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------------------------------------------------------+-------+\n",
      "|col_name    |data_type                                                              |comment|\n",
      "+------------+-----------------------------------------------------------------------+-------+\n",
      "|user_id     |int                                                                    |null   |\n",
      "|user_name   |string                                                                 |null   |\n",
      "|profile_name|string                                                                 |null   |\n",
      "|hobbies     |array<string>                                                          |null   |\n",
      "|liked_image |map<string,array<string>>                                              |null   |\n",
      "|friends_info|struct<first_name:string,last_name:string,user_name:string,user_id:int>|null   |\n",
      "|address     |array<struct<street:string,city:string,country:string>>                |null   |\n",
      "|like_url    |map<string,struct<year:int,url:string,details:string>>                 |null   |\n",
      "+------------+-----------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE user_profile\").show(15, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nmismatched input 'user_id' expecting {'(', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 2, pos 5)\n\n== SQL ==\nINSERT INTO user_profile\n    (user_id, user_name, profile_name, hobbies)\n-----^^^\n    -- liked_image, friends_info, address,like_url) \n    values\n    (1, 'bobby', 'Bobby', hobbies\n    -- \"car:['audi', 'bently']\"\n    )\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input 'user_id' expecting {'(', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 2, pos 5)\n\n== SQL ==\nINSERT INTO user_profile\n    (user_id, user_name, profile_name, hobbies)\n-----^^^\n    -- liked_image, friends_info, address,like_url) \n    values\n    (1, 'bobby', 'Bobby', hobbies\n    -- \"car:['audi', 'bently']\"\n    )\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:268)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:135)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:605)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:605)\n\tat sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-9ec7f7a7a779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;34m\"car:['audi', 'bently']\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0;31m \"\"\")\n\u001b[0m",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nmismatched input 'user_id' expecting {'(', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 2, pos 5)\n\n== SQL ==\nINSERT INTO user_profile\n    (user_id, user_name, profile_name, hobbies)\n-----^^^\n    -- liked_image, friends_info, address,like_url) \n    values\n    (1, 'bobby', 'Bobby', hobbies\n    -- \"car:['audi', 'bently']\"\n    )\n"
     ]
    }
   ],
   "source": [
    "hobbies = ['swimming', 'reading', 'hiking']\n",
    "spark.sql(\"\"\"INSERT INTO user_profile\n",
    "    (user_id, user_name, profile_name, hobbies)\n",
    "    -- liked_image, friends_info, address,like_url) \n",
    "    values\n",
    "    (1, 'bobby', 'Bobby', hobbies\n",
    "    -- \"car:['audi', 'bently']\"\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nmismatched input 'user_name' expecting {'(', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 2, pos 5)\n\n== SQL ==\nINSERT INTO user_profile\n    (user_name, profile_name, hobbies) \n-----^^^\n    values\n    ('bobby', 'Bobby', hobbies)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input 'user_name' expecting {'(', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 2, pos 5)\n\n== SQL ==\nINSERT INTO user_profile\n    (user_name, profile_name, hobbies) \n-----^^^\n    values\n    ('bobby', 'Bobby', hobbies)\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:268)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:135)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:605)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:605)\n\tat sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-788573c6a4e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'bobby'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bobby'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhobbies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \"\"\")\n\u001b[0m",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nmismatched input 'user_name' expecting {'(', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 2, pos 5)\n\n== SQL ==\nINSERT INTO user_profile\n    (user_name, profile_name, hobbies) \n-----^^^\n    values\n    ('bobby', 'Bobby', hobbies)\n"
     ]
    }
   ],
   "source": [
    "hobbies = ['swimming', 'reading', 'hiking']\n",
    "spark.sql(\"\"\"INSERT INTO user_profile\n",
    "    (user_name, profile_name, hobbies) \n",
    "    values\n",
    "    ('bobby', 'Bobby', hobbies)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.19 Functions\n",
    "Spark SQL provides various function for manipulating data.  \n",
    "`SHOW FUNCTIONS`: display the functions  \n",
    "`SHOW SYSTEM FUNCTIONS`: display system functions.  \n",
    "`SHOW USER FUNCTIONS`: display user functions.  \n",
    "`SHOW FUNCTIONS \"con*\"`: display functions that only starts with `con` using wildcard\\(\\*\\) characters.  \n",
    "`SHOW FUNCTIONS LIKE \"reg*\"`: display functions that only starts with `reg` using wildcard\\(*) characters. LIKE is an optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpful Link for [functions](https://spark.apache.org/docs/latest/api/sql/index.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|function             |\n",
      "+---------------------+\n",
      "|!                    |\n",
      "|!=                   |\n",
      "|%                    |\n",
      "|&                    |\n",
      "|*                    |\n",
      "|+                    |\n",
      "|-                    |\n",
      "|/                    |\n",
      "|<                    |\n",
      "|<=                   |\n",
      "|<=>                  |\n",
      "|<>                   |\n",
      "|=                    |\n",
      "|==                   |\n",
      "|>                    |\n",
      "|>=                   |\n",
      "|^                    |\n",
      "|abs                  |\n",
      "|acos                 |\n",
      "|acosh                |\n",
      "|add_months           |\n",
      "|aggregate            |\n",
      "|and                  |\n",
      "|any                  |\n",
      "|approx_count_distinct|\n",
      "|approx_percentile    |\n",
      "|array                |\n",
      "|array_contains       |\n",
      "|array_distinct       |\n",
      "|array_except         |\n",
      "|array_intersect      |\n",
      "|array_join           |\n",
      "|array_max            |\n",
      "|array_min            |\n",
      "|array_position       |\n",
      "|array_remove         |\n",
      "|array_repeat         |\n",
      "|array_sort           |\n",
      "|array_union          |\n",
      "|arrays_overlap       |\n",
      "|arrays_zip           |\n",
      "|ascii                |\n",
      "|asin                 |\n",
      "|asinh                |\n",
      "|assert_true          |\n",
      "|atan                 |\n",
      "|atan2                |\n",
      "|atanh                |\n",
      "|avg                  |\n",
      "|base64               |\n",
      "|between              |\n",
      "|bigint               |\n",
      "|bin                  |\n",
      "|binary               |\n",
      "|bit_and              |\n",
      "|bit_count            |\n",
      "|bit_length           |\n",
      "|bit_or               |\n",
      "|bit_xor              |\n",
      "|bool_and             |\n",
      "|bool_or              |\n",
      "|boolean              |\n",
      "|bround               |\n",
      "|cardinality          |\n",
      "|case                 |\n",
      "|cast                 |\n",
      "|cbrt                 |\n",
      "|ceil                 |\n",
      "|ceiling              |\n",
      "|char                 |\n",
      "|char_length          |\n",
      "|character_length     |\n",
      "|chr                  |\n",
      "|coalesce             |\n",
      "|collect_list         |\n",
      "|collect_set          |\n",
      "|concat               |\n",
      "|concat_ws            |\n",
      "|conv                 |\n",
      "|corr                 |\n",
      "|cos                  |\n",
      "|cosh                 |\n",
      "|cot                  |\n",
      "|count                |\n",
      "|count_if             |\n",
      "|count_min_sketch     |\n",
      "|covar_pop            |\n",
      "|covar_samp           |\n",
      "|crc32                |\n",
      "|cube                 |\n",
      "|cume_dist            |\n",
      "|current_database     |\n",
      "|current_date         |\n",
      "|current_timestamp    |\n",
      "|date                 |\n",
      "|date_add             |\n",
      "|date_format          |\n",
      "|date_part            |\n",
      "|date_sub             |\n",
      "|date_trunc           |\n",
      "+---------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW FUNCTIONS\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|function             |\n",
      "+---------------------+\n",
      "|!                    |\n",
      "|!=                   |\n",
      "|%                    |\n",
      "|&                    |\n",
      "|*                    |\n",
      "|+                    |\n",
      "|-                    |\n",
      "|/                    |\n",
      "|<                    |\n",
      "|<=                   |\n",
      "|<=>                  |\n",
      "|<>                   |\n",
      "|=                    |\n",
      "|==                   |\n",
      "|>                    |\n",
      "|>=                   |\n",
      "|^                    |\n",
      "|abs                  |\n",
      "|acos                 |\n",
      "|acosh                |\n",
      "|add_months           |\n",
      "|aggregate            |\n",
      "|and                  |\n",
      "|any                  |\n",
      "|approx_count_distinct|\n",
      "|approx_percentile    |\n",
      "|array                |\n",
      "|array_contains       |\n",
      "|array_distinct       |\n",
      "|array_except         |\n",
      "+---------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW SYSTEM FUNCTIONS\").show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|function|\n",
      "+--------+\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW USER FUNCTIONS\").show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|function |\n",
      "+---------+\n",
      "|concat   |\n",
      "|concat_ws|\n",
      "|conv     |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW FUNCTIONS \\\"con*\\\"\").show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|function        |\n",
      "+----------------+\n",
      "|aggregate       |\n",
      "|array_repeat    |\n",
      "|atan            |\n",
      "|atan2           |\n",
      "|atanh           |\n",
      "|concat          |\n",
      "|concat_ws       |\n",
      "|current_database|\n",
      "|current_date    |\n",
      "|date            |\n",
      "|date_add        |\n",
      "|date_format     |\n",
      "|date_part       |\n",
      "|date_sub        |\n",
      "|date_trunc      |\n",
      "|datediff        |\n",
      "|element_at      |\n",
      "|flatten         |\n",
      "|float           |\n",
      "|format_number   |\n",
      "|format_string   |\n",
      "|greatest        |\n",
      "|locate          |\n",
      "|make_date       |\n",
      "|map_concat      |\n",
      "|negative        |\n",
      "|repeat          |\n",
      "|to_date         |\n",
      "|translate       |\n",
      "|xpath           |\n",
      "+----------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW FUNCTIONS LIKE \\\"*at*\\\"\").show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|function      |\n",
      "+--------------+\n",
      "|make_date     |\n",
      "|make_timestamp|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW FUNCTIONS LIKE \\\"^make*\\\"\").show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                            |\n",
      "+---------------------------------------------------------------------------------------------------------+\n",
      "|Function: concat_ws                                                                                      |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.ConcatWs                                                |\n",
      "|Usage: concat_ws(sep, [str | array(str)]+) - Returns the concatenation of the strings separated by `sep`.|\n",
      "+---------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION concat_ws\").show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                 |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|Function: array_contains                                                                      |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.ArrayContains                                |\n",
      "|Usage: array_contains(array, value) - Returns true if the array contains the value.           |\n",
      "|Extended Usage:\n",
      "    Examples:\n",
      "      > SELECT array_contains(array(1, 2, 3), 2);\n",
      "       true\n",
      "  |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION EXTENDED array_contains\").show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.20 User-Defined Functions\n",
    "We can use user-defined functions created in DataFrame in Spark SQL. We can also register functions through Hive using `CREATE TEMPORARY FUNCTION` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.4"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create increase_twice UDF\n",
    "def increase_twice(number):\n",
    "    \"\"\"\n",
    "    UDF for increasing value by twice.\n",
    "    :param : column name, float, int, double\n",
    "    >>> select value, increase_twice_udf(value) as twice; return twice the value\n",
    "    +----------+---------+\n",
    "    | value    | twice   |\n",
    "    | 10       | 20      |\n",
    "    +----------|---------+\n",
    "    \"\"\"\n",
    "    return float(number * 2)\n",
    "\n",
    "check = increase_twice(10.2)\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.increase_twice(number)>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register increase_twice_udf UDF\n",
    "spark.udf.register(\"increase_twice_udf\", increase_twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          function|\n",
      "+------------------+\n",
      "|increase_twice_udf|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW FUNCTIONS 'increase*'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+\n",
      "|function_desc                                                      |\n",
      "+-------------------------------------------------------------------+\n",
      "|Function: increase_twice_udf                                       |\n",
      "|Class: org.apache.spark.sql.UDFRegistration$$Lambda$3973/1557538885|\n",
      "|Usage: N/A.                                                        |\n",
      "|Extended Usage:\n",
      "    No example/argument for increase_twice_udf.\n",
      "   |\n",
      "+-------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION EXTENDED increase_twice_udf\").show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "\n",
      "+------------+---------+-----------+\n",
      "|    col_name|data_type|    comment|\n",
      "+------------+---------+-----------+\n",
      "|sepal_length|    float|       null|\n",
      "| sepal_width|    float|sepal width|\n",
      "|petal_length|    float|       null|\n",
      "| petal_width|    float|       null|\n",
      "|     species|   string|       null|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iris limit 10 \").show()\n",
    "\n",
    "spark.sql(\"DESCRIBE iris\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SELECT sepal_length, increase_twice_udf(cast(sepal_length as float)) double_sepal_length FROM iris`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------+------------------+-------------------+\n",
      "|sepal_length|(sepal_length * CAST(2 AS FLOAT))|    spl_twice_lgth|double_sepal_length|\n",
      "+------------+---------------------------------+------------------+-------------------+\n",
      "|         5.1|                             10.2|10.199999809265137| 10.199999809265137|\n",
      "|         4.9|                              9.8| 9.800000190734863|  9.800000190734863|\n",
      "|         4.7|                              9.4| 9.399999618530273|  9.399999618530273|\n",
      "|         4.6|                              9.2| 9.199999809265137|  9.199999809265137|\n",
      "|         5.0|                             10.0|              10.0|               10.0|\n",
      "|         5.4|                             10.8|10.800000190734863| 10.800000190734863|\n",
      "|         4.6|                              9.2| 9.199999809265137|  9.199999809265137|\n",
      "|         5.0|                             10.0|              10.0|               10.0|\n",
      "|         4.4|                              8.8| 8.800000190734863|  8.800000190734863|\n",
      "|         4.9|                              9.8| 9.800000190734863|  9.800000190734863|\n",
      "+------------+---------------------------------+------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT sepal_length, sepal_length * 2, increase_twice_udf(sepal_length) spl_twice_lgth, increase_twice_udf(cast(sepal_length as float)) as double_sepal_length FROM iris\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.21 Subqueries\n",
    "Spark SQL support both correlated and uncorrelated subqueries. Refer to previous SQL chapter for subqueries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.22 Interoperate SQL and DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: float (nullable = true)\n",
      " |-- sepal_width: float (nullable = true)\n",
      " |-- petal_length: float (nullable = true)\n",
      " |-- petal_width: float (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n",
      "+------------+-----------+------------+-----------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|   species|\n",
      "+------------+-----------+------------+-----------+----------+\n",
      "|         5.0|        3.6|         1.4|        0.2|    setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2|    setosa|\n",
      "|         5.0|        3.0|         1.6|        0.2|    setosa|\n",
      "|         5.0|        3.4|         1.6|        0.4|    setosa|\n",
      "|         5.0|        3.2|         1.2|        0.2|    setosa|\n",
      "|         5.0|        3.5|         1.3|        0.3|    setosa|\n",
      "|         5.0|        3.5|         1.6|        0.6|    setosa|\n",
      "|         5.0|        3.3|         1.4|        0.2|    setosa|\n",
      "|         5.0|        2.0|         3.5|        1.0|versicolor|\n",
      "|         5.0|        2.3|         3.3|        1.0|versicolor|\n",
      "+------------+-----------+------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.sql(\"SELECT * from iris\")\n",
    "df_1.printSchema()\n",
    "df_1.where('sepal_length == 5.0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.23 Catalog API\n",
    "Metadata can also be accessed through Spark SQL Catalog API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='analytics_tensor', description='', locationUri='file:/Users/kcmahesh/company/training/Spark/Chapter_6_Spark_SQL/spark-warehouse/analytics_tensor.db'),\n",
       " Database(name='default', description='Default Hive database', locationUri='file:/Users/kcmahesh/company/training/Spark/Chapter_6_Spark_SQL/spark-warehouse')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='iris', database='analytics_tensor', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='iris_dataset_20200628', database='analytics_tensor', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='list_example_vw', database='analytics_tensor', description=None, tableType='VIEW', isTemporary=False),\n",
       " Table(name='map_example_vw', database='analytics_tensor', description=None, tableType='VIEW', isTemporary=False),\n",
       " Table(name='partition_student', database='analytics_tensor', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='sample_iris', database='analytics_tensor', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='struct_example_vw', database='analytics_tensor', description=None, tableType='VIEW', isTemporary=False),\n",
       " Table(name='student', database='analytics_tensor', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='student_name', database='analytics_tensor', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='student_score', database='analytics_tensor', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='user_profile', database='analytics_tensor', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='iris_setosa_tmp_vw', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='sepal_length', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='sepal_width', description='sepal width', dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='petal_length', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='petal_width', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='species', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab_3 Hints\n",
    "\n",
    "In lab_1 we have created external file by performing some basic ETL on employees database. We'll utilize the same file for creating external table and partitioning the data. \n",
    "\n",
    "**Note**: Use this hints in lab_3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS datalake_raw\")\n",
    "\n",
    "spark.sql(\"USE datalake_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE employees_employees (\n",
    "        emp_no integer,\n",
    "        birth_date date,\n",
    "        first_name string,\n",
    "        last_name string,\n",
    "        gender string,\n",
    "        hire_date date)\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY ','\n",
    "    LOCATION '/dataset/incoming/employees/employees'\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
