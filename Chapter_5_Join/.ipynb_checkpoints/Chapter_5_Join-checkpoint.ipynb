{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**5.1 Join**](#5.1-Join)   \n",
    "[**5.2 Join Types**](#5.2-Join-Types)   \n",
    "[**5.2.1 Inner Join**](#5.2.1-Inner-Join)   \n",
    "[**5.2.2 Left Outer Join**](#5.2.2-Left-Outer-Join)   \n",
    "[**5.2.3 Left Semi Join**](#5.2.3-Left-Semi-Join)   \n",
    "[**5.2.4 Left Anti Join**](#5.2.4-Left-Anti-Join)   \n",
    "[**5.2.5 Right Outer Join**](#5.2.5-Right-Outer-Join)   \n",
    "[**5.2.6 Outer Join**](#5.2.6-Outer-Join)   \n",
    "[**5.2.7 Natural Join**](#5.2.7-Natural-Join)   \n",
    "[**5.2.8 Cross Join**](#5.2.8-Cross-Join)   \n",
    "[**5.3 Complex Data Types Join**](#5.3-Complex-Data-Types-Join)   \n",
    "[**5.4 Duplicate Columns in Join**](#5.4-Duplicate-Columns-in-Join)     \n",
    "[**5.5 Optimization and Performance Tuning**](#5.5-Optimization-and-Performance-Tuning)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Join\n",
    "`Join` is used to combine/merge two or more datasets based on common key(s) between those datasets. Relational database are designed based on normalization $^1$. Join are heavily used in data transformation during ETL/ELT process.  If you are not familiar with join, check out [join in SQL chapter](https://github.com/analyticstensor/sql/blob/master/Chapter_4_Filtering_Join_Subquery/Course_Materials/sql-chapter_4.pdf). \n",
    "\n",
    "In Spark, `join` operator is used to join two DataFrames. The join operator and it's parameter is described below:\n",
    "\n",
    "Method:   \n",
    "**join**(other, on=None, how=None)   \n",
    "Parameter:   \n",
    "**other**: DataFrame to be joined.      \n",
    "**on**: column name or list of column names for join condition. All the column names must exits on both DataFrame. It also accepts join expression which determines whether two rows should join or not.     \n",
    "**how**: type of join which determines what records will be in resultset. Must be `inner, left, left_outer, left_semi, left_anti, right, right_outer, outer, full, and cross`. The default value is `inner`.\n",
    "\n",
    "For Example:  \n",
    "\n",
    "`employees.join(dept_emp, employees.emp_no == dept_emp.emp_no, 'left')`\n",
    "\n",
    "`employees.join(dept_emp, 'emp_no', 'left')`\n",
    "\n",
    "`employees.join(dept_emp, 'emp_no')`\n",
    "\n",
    "`employees.join(dept_emp, ['emp_no', 'dept_id'])`\n",
    "\n",
    "`joinCond = [employees.emp_no == dept_emp.emp_no, employees.dept_id == dept.dept_id]   \n",
    "employees.join(dept_emp, joinCond, 'inner')`\n",
    "\n",
    "\n",
    "**Test**   \n",
    "@todo: with an optional `join condition`. Join condition can either be part of join operators or filter operators i.e (`where` or `filter`).   \n",
    "For example:   \n",
    "`employees.join(dep_emp, employees.emp_no == dep_emp.emp_no, 'left')   \n",
    "employees.join(dep_emp).where(\"employees.emp_no\" == \"dep_emp.emp_no\")   \n",
    "employees.join(dep_emp).filter(\"employees.emp_no\" == \"dep_emp.emp_no\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Join Types\n",
    "* **Inner Join**: Hold rows from both left and right DataFrames that has matching keys between them.\n",
    "* **Left Outer Join**: Hold rows from left DataFrame that have either matching keys or doesn't have matching keys from right DataFrame as well as matching records from right DataFrame for the join keys/condition.\n",
    "* **Left Semi Join**: Hold rows only from left DataFrame that has matching keys in right DataFrame. Similiar to `in`.\n",
    "* **Left Anti Join**: Hold rows only from left DataFrame that doesn't have matching keys in right DataFrame. Similar to `not in`. \n",
    "* **Right Outer Join**: Hold rows from right DataFrame that have either matching keys or doesn't have matching keys from right DataFrame as well as matching records from left DataFrame for the join keys/condition.\n",
    "* **Full Outer Join**: Combination of Left Outer and Right Outer Join. \n",
    "* **Natural Join**: Used to perform join by implicitly matching columns between two DataFrames that has same names.\n",
    "* **Cross Join**: Used to perform Cartesian joins where all the row from left DataFrame matches with all the rows from right DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 5.2: Join Types\n",
    "\n",
    "| Join Types | Spark SQL | DataFrame |\n",
    "| --------- | ----------- | --------- |\n",
    "| Inner Join | INNER | inner |\n",
    "| Left Outer Join | LEFT OUTER | left, leftouter |\n",
    "| Left Semi Join | LEFT SEMI | leftsemi |\n",
    "| Left Anti Join | LEFT ANTI | leftanti |\n",
    "| Right Outer Join | RIGHT OUTER | right, rightouter |\n",
    "| Outer Join | FULL OUTER | outer, full, fullouter \n",
    "| Natural Join | NATURAL | Inner Join, Left Outer Join, Right Outer Join, Full Outer Join can also be specified |\n",
    "| Cross Join | CROSS | cross |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Inner Join\n",
    "`Inner Join` checks the join keys between both (i.e. left and right) DataFrames, if the key matches between those DataFrames then it includes only matching records from both DataFrame. For unmatched keys, the records will be ignored in the resulting DataFrame. `inner` is the default join, which means if we don't specify join type then it will set value to `inner`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read Config File*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script import conf\n",
    "\n",
    "config_file = 'db_properties.ini'\n",
    "config_section = 'mysql'\n",
    "read_prop = conf.ReadProperties(config_file, config_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Spark Session*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Chapter 5 Join\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read MySQL employees table into Spark DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read employees table\n",
    "employees = spark.read.jdbc(url = read_prop.get_properties()['url'], table = 'employees', properties = read_prop.get_properties())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read MySQL dept_emp table into Spark DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dept_emp table\n",
    "dept_emp = spark.read.jdbc(url = read_prop.get_properties()['url'], table = 'dept_emp', properties = read_prop.get_properties())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Print Schema for employees and dept_emp DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.printSchema()\n",
    "dept_emp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inner Join with single join key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|dept_no| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|   d005|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24|   d003|1990-11-02|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d005|1992-01-15|1996-02-11|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d008|1996-02-11|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d007|1990-12-26|2000-01-24|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d009|2000-01-24|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|   d005|1991-03-14|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28|   d005|1993-02-12|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d005|1998-07-04|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d008|1995-09-08|1998-07-04|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join employees and dept_emp in emp_no\n",
    "\n",
    "employees.join(dept_emp, 'emp_no').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|dept_no| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|   d005|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24|   d003|1990-11-02|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d005|1992-01-15|1996-02-11|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d008|1996-02-11|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d007|1990-12-26|2000-01-24|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d009|2000-01-24|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|   d005|1991-03-14|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28|   d005|1993-02-12|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d005|1998-07-04|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d008|1995-09-08|1998-07-04|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join employees and dept_emp in emp_no\n",
    "\n",
    "employees.join(dept_emp, 'emp_no', 'inner').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|dept_no| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206|   d005|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362|   d003|1990-11-02|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d005|1992-01-15|1996-02-11|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d008|1996-02-11|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d007|1990-12-26|2000-01-24|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d009|2000-01-24|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| 11033|   d005|1991-03-14|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| 11141|   d005|1993-02-12|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d005|1998-07-04|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d008|1995-09-08|1998-07-04|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join employees and dept_emp in emp_no\n",
    "\n",
    "employees.join(dept_emp, employees.emp_no == dept_emp.emp_no, 'inner').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|dept_no| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206|   d005|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362|   d003|1990-11-02|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d005|1992-01-15|1996-02-11|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d008|1996-02-11|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d007|1990-12-26|2000-01-24|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d009|2000-01-24|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| 11033|   d005|1991-03-14|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| 11141|   d005|1993-02-12|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d005|1998-07-04|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d008|1995-09-08|1998-07-04|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join employees and dept_emp in emp_no\n",
    "\n",
    "joinExpr = employees[\"emp_no\"] == dept_emp[\"emp_no\"]\n",
    "employees.join(dept_emp, joinExpr, 'inner').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|dept_no| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206|   d005|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362|   d003|1990-11-02|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d005|1992-01-15|1996-02-11|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d008|1996-02-11|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d007|1990-12-26|2000-01-24|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d009|2000-01-24|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| 11033|   d005|1991-03-14|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| 11141|   d005|1993-02-12|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d005|1998-07-04|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d008|1995-09-08|1998-07-04|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join employees and dept_emp in emp_no\n",
    "\n",
    "joinExpr = employees.emp_no == dept_emp.emp_no\n",
    "employees.join(dept_emp, joinExpr, 'inner').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|dept_no| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206|   d005|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362|   d003|1990-11-02|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d005|1992-01-15|1996-02-11|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d008|1996-02-11|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d007|1990-12-26|2000-01-24|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d009|2000-01-24|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| 11033|   d005|1991-03-14|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| 11141|   d005|1993-02-12|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d005|1998-07-04|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d008|1995-09-08|1998-07-04|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join employees and dept_emp in emp_no\n",
    "\n",
    "joinExpr = employees[\"emp_no\"] == dept_emp[\"emp_no\"]\n",
    "joinType = \"inner\"\n",
    "employees.join(dept_emp, joinExpr, joinType).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inner Join with multiple join keys having same column name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date| from_date|   to_date|\n",
      "+------+-------+----------+----------+----------+----------+\n",
      "|111692|   d009|1985-01-01|9999-01-01|1985-01-01|1988-10-17|\n",
      "|110022|   d001|1985-01-01|9999-01-01|1985-01-01|1991-10-01|\n",
      "|110039|   d001|1986-04-12|9999-01-01|1991-10-01|9999-01-01|\n",
      "|111133|   d007|1986-12-30|9999-01-01|1991-03-07|9999-01-01|\n",
      "|110567|   d005|1986-10-21|9999-01-01|1992-04-25|9999-01-01|\n",
      "|110344|   d004|1985-11-22|9999-01-01|1988-09-09|1992-08-02|\n",
      "|110228|   d003|1985-08-04|9999-01-01|1992-03-21|9999-01-01|\n",
      "|110420|   d004|1992-02-05|9999-01-01|1996-08-30|9999-01-01|\n",
      "|110800|   d006|1986-08-12|9999-01-01|1991-09-12|1994-06-28|\n",
      "|110854|   d006|1989-06-09|9999-01-01|1994-06-28|9999-01-01|\n",
      "+------+-------+----------+----------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join dept_emp and dept_manager in emp_no and dept_no\n",
    "\n",
    "# Read dept_emp table\n",
    "dept_manager = spark.read.jdbc(url = read_prop.get_properties()['url'], table = 'dept_manager', properties = read_prop.get_properties())\n",
    "\n",
    "dept_emp.join(dept_manager, ['emp_no', 'dept_no']).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inner Join with multiple join keys having different column name**\n",
    "\n",
    "* By creating the list of join condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+-----------------+\n",
      "|emp_no|dept_no| from_date|   to_date|department_number|\n",
      "+------+-------+----------+----------+-----------------+\n",
      "| 10001|   d005|1986-06-26|9999-01-01|             d005|\n",
      "| 10002|   d007|1996-08-03|9999-01-01|             d007|\n",
      "| 10003|   d004|1995-12-03|9999-01-01|             d004|\n",
      "| 10004|   d004|1986-12-01|9999-01-01|             d004|\n",
      "| 10005|   d003|1989-09-12|9999-01-01|             d003|\n",
      "| 10006|   d005|1990-08-05|9999-01-01|             d005|\n",
      "| 10007|   d008|1989-02-10|9999-01-01|             d008|\n",
      "| 10008|   d005|1998-03-11|2000-07-31|             d005|\n",
      "| 10009|   d006|1985-02-18|9999-01-01|             d006|\n",
      "| 10010|   d004|1996-11-24|2000-06-26|             d004|\n",
      "+------+-------+----------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|department_number|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+\n",
      "|111692|   d009|1985-01-01|9999-01-01|             d009|111692|   d009|1985-01-01|1988-10-17|\n",
      "|110022|   d001|1985-01-01|9999-01-01|             d001|110022|   d001|1985-01-01|1991-10-01|\n",
      "|110039|   d001|1986-04-12|9999-01-01|             d001|110039|   d001|1991-10-01|9999-01-01|\n",
      "|111133|   d007|1986-12-30|9999-01-01|             d007|111133|   d007|1991-03-07|9999-01-01|\n",
      "|110567|   d005|1986-10-21|9999-01-01|             d005|110567|   d005|1992-04-25|9999-01-01|\n",
      "|110344|   d004|1985-11-22|9999-01-01|             d004|110344|   d004|1988-09-09|1992-08-02|\n",
      "|110228|   d003|1985-08-04|9999-01-01|             d003|110228|   d003|1992-03-21|9999-01-01|\n",
      "|110420|   d004|1992-02-05|9999-01-01|             d004|110420|   d004|1996-08-30|9999-01-01|\n",
      "|110800|   d006|1986-08-12|9999-01-01|             d006|110800|   d006|1991-09-12|1994-06-28|\n",
      "|110854|   d006|1989-06-09|9999-01-01|             d006|110854|   d006|1994-06-28|9999-01-01|\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join dept_emp and dept_manager in emp_no and department_number from dept_emp DataFrame\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "# create new column department_number in dept_emp DataFrame\n",
    "dept_emp = dept_emp.withColumn(\"department_number\", col(\"dept_no\"))\n",
    "\n",
    "# display top 5 records\n",
    "dept_emp.show(10)\n",
    "\n",
    "joinExpr = [dept_emp.emp_no == dept_manager.emp_no, dept_emp.department_number == dept_manager.dept_no]\n",
    "joinType = \"inner\"\n",
    "\n",
    "dept_emp.join(dept_manager, joinExpr, joinType).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|department_number|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+\n",
      "|111692|   d009|1985-01-01|9999-01-01|             d009|111692|   d009|1985-01-01|1988-10-17|\n",
      "|110022|   d001|1985-01-01|9999-01-01|             d001|110022|   d001|1985-01-01|1991-10-01|\n",
      "|110039|   d001|1986-04-12|9999-01-01|             d001|110039|   d001|1991-10-01|9999-01-01|\n",
      "|111133|   d007|1986-12-30|9999-01-01|             d007|111133|   d007|1991-03-07|9999-01-01|\n",
      "|110567|   d005|1986-10-21|9999-01-01|             d005|110567|   d005|1992-04-25|9999-01-01|\n",
      "|110344|   d004|1985-11-22|9999-01-01|             d004|110344|   d004|1988-09-09|1992-08-02|\n",
      "|110228|   d003|1985-08-04|9999-01-01|             d003|110228|   d003|1992-03-21|9999-01-01|\n",
      "|110420|   d004|1992-02-05|9999-01-01|             d004|110420|   d004|1996-08-30|9999-01-01|\n",
      "|110800|   d006|1986-08-12|9999-01-01|             d006|110800|   d006|1991-09-12|1994-06-28|\n",
      "|110854|   d006|1989-06-09|9999-01-01|             d006|110854|   d006|1994-06-28|9999-01-01|\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinExpr = [dept_emp[\"emp_no\"] == dept_manager[\"emp_no\"], dept_emp[\"department_number\"] == dept_manager[\"dept_no\"]]\n",
    "joinType = \"inner\"\n",
    "\n",
    "dept_emp.join(dept_manager, joinExpr, joinType).show(10)\n",
    "\n",
    "join1 = dept_emp.join(dept_manager, joinExpr, joinType)\n",
    "join2 = join1.join(employees, emp_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|department_number|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+\n",
      "|111692|   d009|1985-01-01|9999-01-01|             d009|111692|   d009|1985-01-01|1988-10-17|\n",
      "|110022|   d001|1985-01-01|9999-01-01|             d001|110022|   d001|1985-01-01|1991-10-01|\n",
      "|110039|   d001|1986-04-12|9999-01-01|             d001|110039|   d001|1991-10-01|9999-01-01|\n",
      "|111133|   d007|1986-12-30|9999-01-01|             d007|111133|   d007|1991-03-07|9999-01-01|\n",
      "|110567|   d005|1986-10-21|9999-01-01|             d005|110567|   d005|1992-04-25|9999-01-01|\n",
      "|110344|   d004|1985-11-22|9999-01-01|             d004|110344|   d004|1988-09-09|1992-08-02|\n",
      "|110228|   d003|1985-08-04|9999-01-01|             d003|110228|   d003|1992-03-21|9999-01-01|\n",
      "|110420|   d004|1992-02-05|9999-01-01|             d004|110420|   d004|1996-08-30|9999-01-01|\n",
      "|110800|   d006|1986-08-12|9999-01-01|             d006|110800|   d006|1991-09-12|1994-06-28|\n",
      "|110854|   d006|1989-06-09|9999-01-01|             d006|110854|   d006|1994-06-28|9999-01-01|\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+------+----------+-----------+-----------+------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|department_number|emp_no|dept_no| from_date|   to_date|emp_no|birth_date| first_name|  last_name|gender| hire_date|\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+------+----------+-----------+-----------+------+----------+\n",
      "|111035|   d007|1985-01-01|9999-01-01|             d007|111035|   d007|1985-01-01|1991-03-07|111035|1962-02-24|Przemyslawa|  Kaelbling|     M|1985-01-01|\n",
      "|110228|   d003|1985-08-04|9999-01-01|             d003|110228|   d003|1992-03-21|9999-01-01|110228|1958-12-02|    Karsten|    Sigstam|     F|1985-08-04|\n",
      "|111692|   d009|1985-01-01|9999-01-01|             d009|111692|   d009|1985-01-01|1988-10-17|111692|1954-10-05|      Tonny|Butterworth|     F|1985-01-01|\n",
      "|111400|   d008|1985-01-01|9999-01-01|             d008|111400|   d008|1985-01-01|1991-04-08|111400|1959-11-09|       Arie|    Staelin|     M|1985-01-01|\n",
      "|110854|   d006|1989-06-09|9999-01-01|             d006|110854|   d006|1994-06-28|9999-01-01|110854|1960-08-19|       Dung|      Pesch|     M|1989-06-09|\n",
      "|110085|   d002|1985-01-01|9999-01-01|             d002|110085|   d002|1985-01-01|1989-12-17|110085|1959-10-28|       Ebru|      Alpin|     M|1985-01-01|\n",
      "|110039|   d001|1986-04-12|9999-01-01|             d001|110039|   d001|1991-10-01|9999-01-01|110039|1963-06-21|   Vishwani|   Minakawa|     M|1986-04-12|\n",
      "|110420|   d004|1992-02-05|9999-01-01|             d004|110420|   d004|1996-08-30|9999-01-01|110420|1963-07-27|      Oscar|   Ghazalie|     M|1992-02-05|\n",
      "|110765|   d006|1989-01-07|9999-01-01|             d006|110765|   d006|1989-05-06|1991-09-12|110765|1954-05-22|     Rutger|    Hofmeyr|     F|1989-01-07|\n",
      "|110386|   d004|1988-10-14|9999-01-01|             d004|110386|   d004|1992-08-02|1996-08-30|110386|1953-10-04|       Shem|     Kieras|     M|1988-10-14|\n",
      "+------+-------+----------+----------+-----------------+------+-------+----------+----------+------+----------+-----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinExpr = [dept_emp[\"emp_no\"] == dept_manager[\"emp_no\"],\n",
    "            dept_emp[\"department_number\"] == dept_manager[\"dept_no\"]\n",
    "           ]\n",
    "joinExpr2 = [employees[\"emp_no\"] == dept_emp[\"emp_no\"]]\n",
    "joinType = \"inner\"\n",
    "\n",
    "dept_emp.join(dept_manager, joinExpr, joinType).show(10)\n",
    "dfAll = dept_emp.join(dept_manager, joinExpr, joinType).join(employees, joinExpr2, joinType)\n",
    "#join2 = join1.join(employees, emp_no\n",
    "dfAll.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inner Join and Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+-------+----------+----------+-----------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|dept_no| from_date|   to_date|department_number|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+-----------------+\n",
      "|285977|1960-09-26|    Georgi|     Gide|     M|1986-10-17|   d004|1986-10-17|9999-01-01|             d004|\n",
      "|205890|1954-09-28|    Georgi|Chenoweth|     F|1987-06-27|   d007|1987-06-27|9999-01-01|             d007|\n",
      "|497592|1959-04-22|    Georgi|   Ariola|     M|1986-03-27|   d007|1986-03-27|1995-10-05|             d007|\n",
      "| 85237|1957-04-05|    Georgi| Chartres|     M|1985-08-30|   d005|1985-08-30|9999-01-01|             d005|\n",
      "|420295|1952-09-13|    Georgi|Kuhnemann|     M|1989-11-07|   d002|1994-12-12|9999-01-01|             d002|\n",
      "|493075|1953-07-19|    Georgi| Kaminger|     M|1987-01-22|   d009|1987-01-22|1995-07-13|             d009|\n",
      "| 22787|1953-02-10|    Georgi|   Hebert|     F|1993-05-13|   d005|1993-05-13|9999-01-01|             d005|\n",
      "|213744|1962-02-25|    Georgi|  Bolsens|     F|1986-05-04|   d002|2000-07-15|2002-04-30|             d002|\n",
      "|213744|1962-02-25|    Georgi|  Bolsens|     F|1986-05-04|   d003|1992-03-16|2000-07-15|             d003|\n",
      "|229388|1953-12-09|    Georgi|    Silva|     M|1988-11-29|   d004|1988-11-29|2001-08-10|             d004|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "employees.join(dept_emp, 'emp_no','inner').where(col(\"first_name\") == 'Georgi').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inner Join, Filter and Distinct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+-------+----------+----------+-----------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|dept_no| from_date|   to_date|department_number|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+-----------------+\n",
      "|285977|1960-09-26|    Georgi|     Gide|     M|1986-10-17|   d004|1986-10-17|9999-01-01|             d004|\n",
      "|205890|1954-09-28|    Georgi|Chenoweth|     F|1987-06-27|   d007|1987-06-27|9999-01-01|             d007|\n",
      "|497592|1959-04-22|    Georgi|   Ariola|     M|1986-03-27|   d007|1986-03-27|1995-10-05|             d007|\n",
      "| 85237|1957-04-05|    Georgi| Chartres|     M|1985-08-30|   d005|1985-08-30|9999-01-01|             d005|\n",
      "|420295|1952-09-13|    Georgi|Kuhnemann|     M|1989-11-07|   d002|1994-12-12|9999-01-01|             d002|\n",
      "|493075|1953-07-19|    Georgi| Kaminger|     M|1987-01-22|   d009|1987-01-22|1995-07-13|             d009|\n",
      "| 22787|1953-02-10|    Georgi|   Hebert|     F|1993-05-13|   d005|1993-05-13|9999-01-01|             d005|\n",
      "|213744|1962-02-25|    Georgi|  Bolsens|     F|1986-05-04|   d002|2000-07-15|2002-04-30|             d002|\n",
      "|213744|1962-02-25|    Georgi|  Bolsens|     F|1986-05-04|   d003|1992-03-16|2000-07-15|             d003|\n",
      "|229388|1953-12-09|    Georgi|    Silva|     M|1988-11-29|   d004|1988-11-29|2001-08-10|             d004|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(dept_emp, 'emp_no','inner')\\\n",
    "        .where(col(\"first_name\") == 'Georgi')\\\n",
    "        .distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|first_name|gender|\n",
      "+----------+------+\n",
      "|    Georgi|     F|\n",
      "|    Georgi|     M|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example with select statement\n",
    "employees.join(dept_emp, 'emp_no','inner')\\\n",
    "        .where(col(\"first_name\") == 'Georgi')\\\n",
    "        .select(\"first_name\", \"gender\")\\\n",
    "        .distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inner Join, Filter, Distinct, and Sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+-------+----------+----------+-----------------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|dept_no| from_date|   to_date|department_number|\n",
      "+------+----------+----------+-----------+------+----------+-------+----------+----------+-----------------+\n",
      "| 10001|1953-09-02|    Georgi|    Facello|     M|1986-06-26|   d005|1986-06-26|9999-01-01|             d005|\n",
      "| 10909|1954-11-11|    Georgi|    Atchley|     M|1985-04-21|   d005|1993-12-26|9999-01-01|             d005|\n",
      "| 11029|1962-07-12|    Georgi|   Itzfeldt|     M|1992-12-27|   d007|1992-12-27|9999-01-01|             d007|\n",
      "| 11430|1957-01-23|    Georgi|    Klassen|     M|1996-02-27|   d007|1997-07-06|9999-01-01|             d007|\n",
      "| 12157|1960-03-30|    Georgi|    Barinka|     M|1985-06-04|   d003|1985-06-04|9999-01-01|             d003|\n",
      "| 15220|1957-08-03|    Georgi|  Panienski|     F|1995-07-23|   d004|1995-07-23|9999-01-01|             d004|\n",
      "| 15660|1956-01-13|    Georgi| Hartvigsen|     M|1994-10-13|   d005|1996-12-27|9999-01-01|             d005|\n",
      "| 15689|1962-09-14|    Georgi|Capobianchi|     M|1995-03-11|   d004|1995-07-06|1997-01-10|             d004|\n",
      "| 15689|1962-09-14|    Georgi|Capobianchi|     M|1995-03-11|   d005|1995-03-11|1995-07-06|             d005|\n",
      "| 15843|1958-07-15|    Georgi|     Varley|     M|1987-04-14|   d009|1989-06-08|9999-01-01|             d009|\n",
      "+------+----------+----------+-----------+------+----------+-------+----------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(dept_emp, 'emp_no','inner')\\\n",
    "    .where(col(\"first_name\") == 'Georgi')\\\n",
    "    .distinct()\\\n",
    "    .orderBy(\"emp_no\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inner Join, Filter, and Grouping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|department_number|count|\n",
      "+-----------------+-----+\n",
      "|             d005|   67|\n",
      "|             d009|   22|\n",
      "|             d003|   18|\n",
      "|             d001|   14|\n",
      "|             d007|   45|\n",
      "|             d004|   64|\n",
      "|             d002|   14|\n",
      "|             d006|   19|\n",
      "|             d008|   15|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(dept_emp, 'emp_no','inner')\\\n",
    "    .where(col(\"first_name\") == 'Georgi')\\\n",
    "    .groupBy(\"department_number\").count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Left Outer Join\n",
    "`Left Outer Join` checks the join keys between both (i.e. left and right) DataFrames, it includes all the records from left DataFrames as well as matching records from right DataFrame for the given keys. If the records doesn't exists i.e. unmatched keys in right DataFrame, `null` value will be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|dept_no| from_date|   to_date|department_number|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206|   d005|1988-04-19|9999-01-01|             d005|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362|   d003|1990-11-02|1997-07-16|             d003|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d005|1992-01-15|1996-02-11|             d005|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d008|1996-02-11|9999-01-01|             d008|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d007|1990-12-26|2000-01-24|             d007|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d009|2000-01-24|9999-01-01|             d009|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| 11033|   d005|1991-03-14|9999-01-01|             d005|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| 11141|   d005|1993-02-12|9999-01-01|             d005|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d005|1998-07-04|9999-01-01|             d005|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d008|1995-09-08|1998-07-04|             d008|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left outer join employees and dept_emp in emp_no\n",
    "\n",
    "joinExpr = employees[\"emp_no\"] == dept_emp[\"emp_no\"]\n",
    "joinType = \"left_outer\"\n",
    "employees.join(dept_emp, joinExpr, joinType).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 Left Semi Join\n",
    "`Left Semi Join` checks the join keys between both (i.e. left and right) DataFrames, it only includes records from left DataFrames that have matching keys in right DataFrame. It doesn't include any records from right DataFrame. It is similar to `IN` and `EXISTS` in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|\n",
      "| 11458|1958-08-09|     Stevo|Chenoweth|     F|1985-10-06|\n",
      "| 11748|1953-03-07|    Lihong| Massonet|     M|1992-12-20|\n",
      "| 11858|1962-11-21|   Slavian|     Baik|     M|1988-11-12|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  left semi join employees and dept_emp in emp_no\n",
    "\n",
    "joinExpr = employees[\"emp_no\"] == dept_emp[\"emp_no\"]\n",
    "joinType = \"left_semi\"\n",
    "employees.join(dept_emp, joinExpr, joinType).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 Left Anti Join\n",
    "`Left Anti Join` checks the join keys between both (i.e. left and right) DataFrames, it only includes records from left DataFrames that doesn't have matching keys in right DataFrame. Basically, it is reverse of left semi joins. It doesn't include any records from right DataFrame. It is similar to `NOT IN` and `NOT EXISTS` in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+---------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date|\n",
      "+------+----------+----------+---------+------+---------+\n",
      "+------+----------+----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  left anti join employees and dept_emp in emp_no\n",
    "\n",
    "joinExpr = employees[\"emp_no\"] == dept_emp[\"emp_no\"]\n",
    "joinType = \"left_anti\"\n",
    "employees.join(dept_emp, joinExpr, joinType).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.5 Right Outer Join\n",
    "`Right Outer Join` checks the join keys between both (i.e. left and right) DataFrames, it includes all the records from right DataFrames as well as matching records from left DataFrame for the given keys. If the records doesn't exists i.e. unmatched keys in left DataFrame, `null` value will be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|dept_no| from_date|   to_date|department_number|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206|   d005|1988-04-19|9999-01-01|             d005|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362|   d003|1990-11-02|1997-07-16|             d003|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d005|1992-01-15|1996-02-11|             d005|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d008|1996-02-11|9999-01-01|             d008|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d007|1990-12-26|2000-01-24|             d007|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d009|2000-01-24|9999-01-01|             d009|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| 11033|   d005|1991-03-14|9999-01-01|             d005|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| 11141|   d005|1993-02-12|9999-01-01|             d005|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d005|1998-07-04|9999-01-01|             d005|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d008|1995-09-08|1998-07-04|             d008|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  right outer join employees and dept_emp in emp_no\n",
    "\n",
    "joinExpr = employees[\"emp_no\"] == dept_emp[\"emp_no\"]\n",
    "joinType = \"right_outer\"\n",
    "employees.join(dept_emp, joinExpr, joinType).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.6 Outer Join\n",
    "`Outer Join` checks the join keys between both (i.e. left and right) DataFrames, it includes all the records from both left and right DataFrames for matched and unmatched keys between those DataFrames. `null` value will be inserted for all the records that doesn't have matching keys.  It is combination of left and right outer join. It is sometime referred as `Full Join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|dept_no| from_date|   to_date|department_number|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206|   d005|1988-04-19|9999-01-01|             d005|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362|   d003|1990-11-02|1997-07-16|             d003|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d005|1992-01-15|1996-02-11|             d005|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d008|1996-02-11|9999-01-01|             d008|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d007|1990-12-26|2000-01-24|             d007|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d009|2000-01-24|9999-01-01|             d009|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| 11033|   d005|1991-03-14|9999-01-01|             d005|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| 11141|   d005|1993-02-12|9999-01-01|             d005|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d005|1998-07-04|9999-01-01|             d005|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d008|1995-09-08|1998-07-04|             d008|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# outer join employees and dept_emp in emp_no\n",
    "\n",
    "joinExpr = employees[\"emp_no\"] == dept_emp[\"emp_no\"]\n",
    "joinType = \"outer\"\n",
    "employees.join(dept_emp, joinExpr, joinType).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.7 Natural Join\n",
    "`Natural Join` doesn't need any join keys but it will implicitly choose the columns between two DataFrame by matching column names. We can also specify join types like left, right and outer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural join employees and dept_emp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.8 Cross Join\n",
    "`Cross Join` doesn't need any join keys. It matches with each records in left DataFrame to each records in right DataFrame and generate the resultset. It is a cartesian products of left and right DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|dept_no| from_date|   to_date|department_number|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10001|   d005|1986-06-26|9999-01-01|             d005|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10002|   d007|1996-08-03|9999-01-01|             d007|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10003|   d004|1995-12-03|9999-01-01|             d004|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10004|   d004|1986-12-01|9999-01-01|             d004|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10005|   d003|1989-09-12|9999-01-01|             d003|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10006|   d005|1990-08-05|9999-01-01|             d005|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10007|   d008|1989-02-10|9999-01-01|             d008|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10008|   d005|1998-03-11|2000-07-31|             d005|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10009|   d006|1985-02-18|9999-01-01|             d006|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10010|   d004|1996-11-24|2000-06-26|             d004|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10010|   d006|2000-06-26|9999-01-01|             d006|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10011|   d009|1990-01-22|1996-11-09|             d009|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10012|   d005|1992-12-18|9999-01-01|             d005|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10013|   d003|1985-10-20|9999-01-01|             d003|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10014|   d005|1993-12-29|9999-01-01|             d005|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10015|   d008|1992-09-19|1993-08-22|             d008|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10016|   d007|1998-02-11|9999-01-01|             d007|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10017|   d001|1993-08-03|9999-01-01|             d001|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10018|   d004|1992-07-29|9999-01-01|             d004|\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 10018|   d005|1987-04-03|1992-07-29|             d005|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross join employees and dept_emp\n",
    "\n",
    "employees.crossJoin(dept_emp).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Complex Data Types Join\n",
    "While performing join with complex data type it isn't too difficult. All the expression will be a valid join expression until and unless it returns a Boolean type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "None\n",
      "+---+-------+--------+------------------------+\n",
      "|id |first  |last    |skills                  |\n",
      "+---+-------+--------+------------------------+\n",
      "|1  |John   |Warthorn|[10, 20, 30]            |\n",
      "|2  |Harry  |Joy     |[10]                    |\n",
      "|3  |Patrick|Roy     |[40, 60, 80]            |\n",
      "|4  |Bicky  |Boss    |[20]                    |\n",
      "|5  |Micheal|Todd    |[0]                     |\n",
      "|6  |Bobby  |Home    |[10, 20, 30, 40, 50, 60]|\n",
      "+---+-------+--------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_user = \"id int, first string, last string, skills array<int>\"\n",
    "\n",
    "dataset_user = [[1, \"John\", \"Warthorn\", [10, 20 ,30]],\n",
    "       [2, \"Harry\",\"Joy\", [10]],\n",
    "       [3, \"Patrick\", \"Roy\", [40, 60, 80]],\n",
    "       [4, \"Bicky\", \"Boss\", [20]],\n",
    "       [5, \"Micheal\",\"Todd\", [0]],\n",
    "       [6, \"Bobby\", \"Home\", [10, 20, 30, 40, 50, 60]]\n",
    "      ]\n",
    "# create a DataFrame using the schema defined above\n",
    "user = spark.createDataFrame(dataset_user, schema_user)\n",
    "print(user.printSchema())\n",
    "user.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- skill_id: integer (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      "\n",
      "None\n",
      "+--------+------+\n",
      "|skill_id|course|\n",
      "+--------+------+\n",
      "|10      |Java  |\n",
      "|20      |Python|\n",
      "|30      |Scala |\n",
      "|40      |Spark |\n",
      "|50      |Bash  |\n",
      "|60      |Cloud |\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_skill = \"skill_id int, course string\"\n",
    "\n",
    "dataset_skill = [[10, \"Java\", ],\n",
    "       [20, \"Python\" ],\n",
    "       [30, \"Scala\"],\n",
    "       [40, \"Spark\"],\n",
    "       [50, \"Bash\"],\n",
    "       [60, \"Cloud\"]\n",
    "      ]\n",
    "# create a DataFrame using the schema defined above\n",
    "skill = spark.createDataFrame(dataset_skill, schema_skill)\n",
    "print(skill.printSchema())\n",
    "skill.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------------------------+--------+------+\n",
      "|id |first  |last    |skills                  |skill_id|course|\n",
      "+---+-------+--------+------------------------+--------+------+\n",
      "|1  |John   |Warthorn|[10, 20, 30]            |10      |Java  |\n",
      "|1  |John   |Warthorn|[10, 20, 30]            |20      |Python|\n",
      "|1  |John   |Warthorn|[10, 20, 30]            |30      |Scala |\n",
      "|2  |Harry  |Joy     |[10]                    |10      |Java  |\n",
      "|3  |Patrick|Roy     |[40, 60, 80]            |40      |Spark |\n",
      "|3  |Patrick|Roy     |[40, 60, 80]            |60      |Cloud |\n",
      "|4  |Bicky  |Boss    |[20]                    |20      |Python|\n",
      "|6  |Bobby  |Home    |[10, 20, 30, 40, 50, 60]|10      |Java  |\n",
      "|6  |Bobby  |Home    |[10, 20, 30, 40, 50, 60]|20      |Python|\n",
      "|6  |Bobby  |Home    |[10, 20, 30, 40, 50, 60]|30      |Scala |\n",
      "|6  |Bobby  |Home    |[10, 20, 30, 40, 50, 60]|40      |Spark |\n",
      "|6  |Bobby  |Home    |[10, 20, 30, 40, 50, 60]|50      |Bash  |\n",
      "|6  |Bobby  |Home    |[10, 20, 30, 40, 50, 60]|60      |Cloud |\n",
      "+---+-------+--------+------------------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "user.join(skill, expr(\"array_contains(skills, skill_id)\"),\"inner\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Duplicate Columns in Join\n",
    "\n",
    "While joining DataFrames, there might be same column names in both DataFrame. Duplicate column names only occurs:    \n",
    "1. If two DataFrames have same column name that is specified on join expression, and after join it don't remove key from one DataFrame.\n",
    "2.  If two DataFrames have same column name but they are not specified on join expression.\n",
    "\n",
    "[Additional Resources](https://kb.databricks.com/data/join-two-dataframes-duplicated-columns.html)$^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods for handling duplicate column names in join**  \n",
    "\n",
    "\n",
    "**Method I**  \n",
    "If the join key has same column name then instead of using Boolean expression in join, use string or sequence. This will automatically remove one columns during join.  \n",
    "`Join with Boolean Expression`  \n",
    "employees.join(dept_emp, employees.emp_no == dept_emp.emp_no, 'inner').show(10)  \n",
    "`Join with String`  \n",
    "employees.join(dept_emp, \"emp_no\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|dept_no| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206|   d005|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362|   d003|1990-11-02|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d005|1992-01-15|1996-02-11|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623|   d008|1996-02-11|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d007|1990-12-26|2000-01-24|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817|   d009|2000-01-24|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| 11033|   d005|1991-03-14|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| 11141|   d005|1993-02-12|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d005|1998-07-04|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317|   d008|1995-09-08|1998-07-04|\n",
      "+------+----------+----------+---------+------+----------+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join with Boolean Expression\n",
    "employees.join(dept_emp, employees.emp_no == dept_emp.emp_no, 'inner').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|dept_no| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|   d005|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24|   d003|1990-11-02|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d005|1992-01-15|1996-02-11|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d008|1996-02-11|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d007|1990-12-26|2000-01-24|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d009|2000-01-24|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|   d005|1991-03-14|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28|   d005|1993-02-12|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d005|1998-07-04|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d008|1995-09-08|1998-07-04|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join with String\n",
    "employees.join(dept_emp, \"emp_no\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method II**   \n",
    "Drop the duplicate column name after join. By specifying Dataframe and column name OR only column name. In example-1 below, When DataFrame name is specified with its column name then only the column from particular DataFrame will be dropped. In example-2 below, When column name doesn't contains DataFrame name then both column will be dropped.\n",
    "employees.join(dept_emp, employees.emp_no == dept_emp.emp_no, 'inner').drop((\"emp_no\")).show(10)  \n",
    "employees.join(dept_emp, employees.emp_no == dept_emp.emp_no, 'inner').drop((dept_emp.emp_no)).show(10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|dept_no| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|   d005|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24|   d003|1990-11-02|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d005|1992-01-15|1996-02-11|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d008|1996-02-11|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d007|1990-12-26|2000-01-24|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d009|2000-01-24|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|   d005|1991-03-14|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28|   d005|1993-02-12|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d005|1998-07-04|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d008|1995-09-08|1998-07-04|\n",
      "+------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example-1: when we specify the column name it will drop both columns\n",
    "employees.join(dept_emp, employees.emp_no == dept_emp.emp_no, 'inner').drop((\"emp_no\")).show(10)\n",
    "\n",
    "# example-2: when we specify the DataFrame and column name it will drop the columns only from the specific DF.\n",
    "employees.join(dept_emp, employees.emp_no == dept_emp.emp_no, 'inner').drop((dept_emp.emp_no)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method III**   \n",
    "Rename a columns before join.  \n",
    "employees = employees.withColumnRenamed(\"emp_no\", \"employees_no\")  \n",
    "employees.join(dept_emp, employees.employee_no == dept_emp.emp_no, 'inner').drop(\"emp_no\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employees_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+------------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "|employees_no|birth_date|first_name|last_name|gender| hire_date|dept_no| from_date|   to_date|\n",
      "+------------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "|       10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|   d005|1988-04-19|9999-01-01|\n",
      "|       10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24|   d003|1990-11-02|1997-07-16|\n",
      "|       10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d005|1992-01-15|1996-02-11|\n",
      "|       10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|   d008|1996-02-11|9999-01-01|\n",
      "|       10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d007|1990-12-26|2000-01-24|\n",
      "|       10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|   d009|2000-01-24|9999-01-01|\n",
      "|       11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|   d005|1991-03-14|9999-01-01|\n",
      "|       11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28|   d005|1993-02-12|9999-01-01|\n",
      "|       11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d005|1998-07-04|9999-01-01|\n",
      "|       11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|   d008|1995-09-08|1998-07-04|\n",
      "+------------+----------+----------+---------+------+----------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename column and drop emp_no columns\n",
    "\n",
    "employees = employees.withColumnRenamed(\"emp_no\", \"employees_no\")\n",
    "employees.printSchema()\n",
    "employees.join(dept_emp, employees.employees_no == dept_emp.emp_no, 'inner').drop(\"emp_no\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Optimization and Performance Tuning\n",
    "\n",
    "Optimization and performance tuning is very important while joining big DataFrames. There are several ways to optimize during join operation. Before that we need to understand how Spark performs join operation internally. Check out the link below:   \n",
    "* Node-to-Node Communication\n",
    "* Per node Communcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution Plan**  \n",
    "Execution Plan helps to give picture about the Physical Plan of join. We can display execution plan using `explain()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employees_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [employees_no#1369], [emp_no#12], Inner\n",
      ":- *(2) Sort [employees_no#1369 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(employees_no#1369, 200), true, [id=#842]\n",
      ":     +- *(1) Project [emp_no#0 AS employees_no#1369, birth_date#1, first_name#2, last_name#3, gender#4, hire_date#5]\n",
      ":        +- *(1) Scan JDBCRelation(employees) [numPartitions=1] [gender#4,emp_no#0,hire_date#5,birth_date#1,first_name#2,last_name#3] PushedFilters: [*IsNotNull(emp_no)], ReadSchema: struct<gender:string,emp_no:int,hire_date:date,birth_date:date,first_name:string,last_name:string>\n",
      "+- *(4) Sort [emp_no#12 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(emp_no#12, 200), true, [id=#848]\n",
      "      +- *(3) Scan JDBCRelation(dept_emp) [numPartitions=1] [emp_no#12,dept_no#13,from_date#14,to_date#15] PushedFilters: [*IsNotNull(emp_no)], ReadSchema: struct<emp_no:int,dept_no:string,from_date:date,to_date:date>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [emp_no#12], [employees_no#1369], Inner\n",
      ":- *(2) Sort [emp_no#12 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(emp_no#12, 200), true, [id=#875]\n",
      ":     +- *(1) Scan JDBCRelation(dept_emp) [numPartitions=1] [emp_no#12,dept_no#13,from_date#14,to_date#15] PushedFilters: [*IsNotNull(emp_no)], ReadSchema: struct<emp_no:int,dept_no:string,from_date:date,to_date:date>\n",
      "+- *(4) Sort [employees_no#1369 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(employees_no#1369, 200), true, [id=#881]\n",
      "      +- *(3) Project [emp_no#0 AS employees_no#1369, birth_date#1, first_name#2, last_name#3, gender#4, hire_date#5]\n",
      "         +- *(3) Scan JDBCRelation(employees) [numPartitions=1] [gender#4,emp_no#0,hire_date#5,birth_date#1,first_name#2,last_name#3] PushedFilters: [*IsNotNull(emp_no)], ReadSchema: struct<gender:string,emp_no:int,hire_date:date,birth_date:date,first_name:string,last_name:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.printSchema()\n",
    "employees.join(dept_emp, employees.employees_no == dept_emp.emp_no, 'inner').explain()\n",
    "dept_emp.join(employees, employees.employees_no == dept_emp.emp_no, 'inner').explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster Configuration**  \n",
    "We can tune the Spark cluster by configuring default parameter. Refer the link below:  \n",
    "[Spark Configuration](https://spark.apache.org/docs/latest/configuration.html)$^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partitioning DataFrame**  \n",
    "Partitioning DataFrame is important when the size of dataset is large. It helps to:  \n",
    "* Split the data into multiple splitable.\n",
    "* Spread data across multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning job**  \n",
    "Tuning help to optimize the query and make code run faster. To optimize:  \n",
    "* Change the join order\n",
    "* Use broadcast join hints. \n",
    "\n",
    "[Spark Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)  \n",
    "\n",
    "**Spark SQL Configurations**\n",
    "For optimizing or debugging the SQL code, we can configure spark configuration properties. The table below shows some configuration for tuning the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table: 5.1 Spark SQL configurations\n",
    "\n",
    "| Property | Default | Description |\n",
    "|:---------------:|:---------------:|:---------------:|\n",
    "| spark.sql.inMemoryColumnarStorage.compressed | true | When set to true, Spark SQL automatically selects a compression codec for each column based on statistics of the data. |\n",
    "| spark.sql.inMemoryColumnarStorage.batchSize | 10000 | Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OutOfMemoryErrors (OOMs) when caching data. |\n",
    "| spark.sql.files.maxPartitionBytes | 134217728 (128 MB) | The maximum number of bytes to pack into a single partition when reading files. |  \n",
    "| spark.sql.broadcastTimeout | 300 | Timeout in seconds for the broadcast wait time in broadcast joins. |\n",
    "| spark.sql.files.openCostInBytes | 10485760 (10 MB) | The estimated cost to open a file, measured by the number of bytes that could be scanned in the same time. This is used when putting multiple files into a partition. It is better to overestimate; that way the partitions with small files will be faster than partitions with bigger files (which is scheduled first). |\n",
    "| spark.sql.autoBroadcastJoinThreshold | 10485760 (10 MB) | Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. You can disable broadcasting by setting this value to -1. Note that currently statistics are supported only for Hive Metastore tables for which the command ANALYZE TABLE COMPUTE STATISTICS noscan has been run |\n",
    "| spark.sql.shuffle.partitions | 200 | Configures the number of partitions to use when shuffling data for joins or aggregations. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "$^{1}$ https://en.wikipedia.org/wiki/Database_normalization#Normal_forms   \n",
    "$^{2}$ https://kb.databricks.com/data/join-two-dataframes-duplicated-columns.html   \n",
    "$^{3}$ https://spark.apache.org/docs/latest/configuration.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
