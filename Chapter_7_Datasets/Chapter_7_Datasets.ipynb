{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**7.1 Datasets**](#7.1-Datasets)   \n",
    "[**7.2 Creating Datasets**](#7.2-Creating-Datasets)   \n",
    "[**7.2.1 Scala**](#7.2.1-Scala)   \n",
    "[**7.2.2 Java**](#7.2.2-Java)   \n",
    "[**7.3 DataFrame and Dataset**](#7.3-DataFrame-and-Dataset)   \n",
    "[**7.4 Encoder**](#7.4-Encoder)   \n",
    "[**7.5 Actions**](#7.5-Actions)   \n",
    "[**7.6 Transformations**](#7.6-Transformations)   \n",
    "[**7.7 Joins**](#7.7-Joins)   \n",
    "[**7.8 Grouping and Aggregations**](#7.8-Grouping-and-Aggregations)     \n",
    "[**7.9 Write Output to File**](#7.9-Write-Output-to-File)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Datasets\n",
    "We have already learned two structured API i.e. DataFrame and SparkSQL. Datasets are the foundational type for all the structured APIs. It is Java Virtual Machine (JVM) language feature that works only with Scala and Java. Datasets are language-native type classes and objects in Scala and Java, where DataFrame doesn't have those characteristic.\n",
    "\n",
    "`Datasets` is a strongly-typed collection of domain-specific objects that can be tranformed in parallel using functional or relational operations. Datasets supports both `strongly-typed` and `untyped` API. As said earlier, Datasets only exists in Scala and Java. DataFrames exists in Python and R. Comparing with DataFrame, DataFrames are Datasets of type `Row`. Row is a generic typed JVM object that holds different types of fields. Spark will internally convert the Row objects into Spark types. For e..g Int Row will be convered to IntegerType and IntegerType() in Scala/Java and Python respectively. \n",
    "\n",
    "**Needs of Datasets**  \n",
    "* If the functionality are not supported in DataFrame.\n",
    "* If type-safety is the major issue.\n",
    "\n",
    "If we want to convert a large business logic then instead of using DataFrame and SparkSQL we can use Datasets. Something if we are working on calculation and we want the output be precise then we can use Dataset. Datasets are used if we want to reuse multiple transformations of entire rows between single-node workloads and Spark workloads. i.e. During ETL, if we want to collect data by driver and manipulate in single-node libaries at the beginning of transformation.  \n",
    "\n",
    "\n",
    "Let walk through the concept of Dataset described in Databricks [articles](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html). \n",
    "\n",
    "**_Self Reading_**: Read the above article and this [article](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html) and identify the needs and usage of using Datasets.\n",
    "\n",
    "\n",
    "**Dataset APIs characteristics**  \n",
    "* Strongly-typed API\n",
    "* Untype API\n",
    "\n",
    "According to Databrick's blog: If we consider, DataFrame as an alias for a collection of generic objects `Dataset[Row]`, `Row` is a generic `untyped` JVM object. Dataset is a collection of `strongly-typed` JVM objects.\n",
    "\n",
    "![Databrick Dataset APIs](https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Creating Datasets\n",
    "\n",
    "While creating Datasets we need to define the schemas at the beginning. We can define the object for all the row in Dataset using Scala or Java.  \n",
    "* `case class` object is used in Scala for defining a schema.\n",
    "* `JavaBean` is used in Java for defining a schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2.1 Scala**  \n",
    "In Scala, we need to create `case class`. `case class` is a regular class which are used for modeling immutable data and pattern matching.  \n",
    "[Check out for case class](https://docs.scala-lang.org/tour/case-classes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo on spark-shell**  \n",
    "\n",
    "Step 1: Run spark-shell in terminal  \n",
    "Step 2: Create dummy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filename: /tmp/dataset.csv  \n",
    "1,Bob,A  \n",
    "2,Harry,A  \n",
    "3,John,A  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create case class in Scala for employee_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// define a case class that represent employee data\n",
    "case class Employee(\n",
    "    emp_id: Int,\n",
    "    name: String,\n",
    "    grade: String  //pay grade\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Read the employee data and create Dataset[Row] from `case class` Employee_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Encoders\n",
    "\n",
    "val emp = spark.read.schema(Encoders.product[Employee].schema).csv(\"/tmp/dataset.csv\").as[Employee]\n",
    "\n",
    "// It will return:\n",
    "emp: org.apache.spark.sql.Dataset[Employee] = [student_id: int, name: string ... 1 more field]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_display employee record_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.first.emp_id // it will return first emp_id as well as its type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.first.name // it will return first name as well as its type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Print top 10 rows_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.take(10).foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will return:  \n",
    "Employee(1,Bob,A)  \n",
    "Employee(2,Harry,A)  \n",
    "Employee(3,John,A)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2.2 Java**  \n",
    "In Java, we need to create our class and encode the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo**  \n",
    "\n",
    "https://databricks.com/spark/getting-started-with-apache-spark/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Encoders;\n",
    "import java.io.Serializable;\n",
    "\n",
    "public class Employee implements Serializable{\n",
    " int id ;\n",
    " String firstname;\n",
    " String lastname;\n",
    " Datetime dob;\n",
    "    \n",
    " //getter and setters\n",
    " int getId(){\n",
    "     return id;\n",
    " }\n",
    " \n",
    " String getFirstName{\n",
    "     return firstname;\n",
    " }\n",
    " @todo setter\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset<Employee> emp = spark.read\n",
    "  .csv(\"/tmp/employees.csv\")\n",
    "  .as(Encoders.bean(Employee.class));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 DataFrame and Dataset\n",
    "\n",
    "There are some reason of using DataFrame and Dataset.  \n",
    "\n",
    "\n",
    "---------------------\n",
    "`DataFrame`\n",
    "* For processing relational transformation similar to SQL like queries.\n",
    "* For unification, code optimization, simplification of APIs across Spark Libraries.\n",
    "---------------------\n",
    "`Dataset` \n",
    "* For strict compile type safety, where multiple case classes need to be created for specific Dataset[T].\n",
    "* For higher degree of type-safety at compile time.\n",
    "* For typed JVM objects by taking advantage of Catalyst optimization and benefiting from Tungstenâ€™s efficient code generation and serialization with Encoders.\n",
    "---------------------\n",
    "`DataFrame/Dataset`\n",
    "* For rich semantics, high-level abstractions, and domain specific language operators.\n",
    "* For processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of relational operators on semi-structured data.\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Encoder\n",
    "Encoder is the fundamental concept in the serialization and deserialization (SerDe) framework in Spark. It is also known as container of serde expressions in Dataset. Encoder is used to map the domain-specific type to Spark's internal type. It convert in-memory`off Java heap` data from Spark's Tungsten format to JVM Java objects. Basically, it serializes and deserializes Dataset objects from Spark's internal format to JVM objects.\n",
    "\n",
    "For example, In `Employee` class that has two field, `id` int and `name` string, Encoder will serialize the `Employee` object into binary structure. In Structure APIs, the binary structure is known as `Row`. While using Dataset API, all the row will be converted into object. \n",
    "\n",
    "Spark has built-in feature for generating `Encoders` for primitive types, Scalas case classes, and Java Beans. It's encoder is faster than Java and Kyro serde. [For more info](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html)  \n",
    "\n",
    "Spark uses [Tungsten](https://databricks.com/glossary/tungsten) for memory management. Tungsten stores objects off the Java heap memory which is compact and occupy less space compared to Java storage. Lets see the simple example in [Java Vs Spark Tungsten row-based format](https://spoddutur.github.io/spark-notes/deep_dive_into_storage_formats.html). Encoder will quickly serialize and deserialize using pointer arithmetic with memory address and offset. The figure below shows the comparision with Encoder, Java, and Kyro SerDes. ![Compare Encoder with Java and Kyro](https://databricks.com/wp-content/uploads/2016/01/Serialization-Deserialization-Performance-Chart-1024x364.png?noresize) I highly suggest to read the Spark storage format in this [link](https://spoddutur.github.io/spark-notes/deep_dive_into_storage_formats.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 Actions\n",
    "We can apply all the actions such as `collect, take, count` etc. in Datasets. [More info](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+----------+--------+-----+  \n",
    "|student_id|    name|grade|  \n",
    "+----------+--------+-----+  \n",
    "|         1|     Bob|    A|  \n",
    "|         2|   Harry|    A|  \n",
    "|         3|    John|    A|  \n",
    "+----------+--------+-----+  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accessing element from `case class`**  \n",
    "Specifying the attribute name will return the values as well as data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.first.name // it will return first element from the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6 Transformations \n",
    "All the transformations in DataFrame are supported by Datasets. We can also use complex and strongly type transformation in Datasets. We need to define the `generic function` for any new transformation. These function is not a UDF. All the function can be tested locally before executing to Spark.  [More info](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Filtering_  \n",
    "We can create filter by defining generic function. The function `gradeA` will check if the grade is A. It will return Boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradeA(emp_record: Employee): Boolean = {\n",
    "    return emp_record.grade == \"A\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.filter(emp_record => gradeA(emp_record)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will return  \n",
    "Employee = Employee(1,Bob,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.filter(emp_record => gradeA(emp_record)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Mapping_  \n",
    "Mapping is used when we want to map one value to other value. For e.g. extracting value, comparing values etc. The example below shows extracting one value from each row. It is similar to `select` in DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val full_name = emp.map(f => f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It will return\n",
    "+--------+\n",
    "|   value|\n",
    "+--------+\n",
    "|     Bob|\n",
    "|   Harry|\n",
    "|    John|\n",
    "+--------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7 Joins\n",
    "The joins used in DataFrame can also be applied to Datasets. Datasets has `joinWith` method which is similar to co-group in RDD. It will return two nested Datasets inside of one. `joinWith` creates a Dataset with two columns `_1` and `_2` that matches with the specified condition. It is used when we want to join and apply advance manipulation on result such as advance map or filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class EmployeeList(sequence_num: BigInt, id: Int)\n",
    "\n",
    "val employee_list = spark.range(10).map(x => (x, scala.util.Random.nextInt(50)))\n",
    ".withColumnRenamed(\"_1\", \"sequence_num\")\n",
    ".withColumnRenamed(\"_2\", \"id\").as[EmployeeList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_list\n",
    "// It returns\n",
    "employee_list: org.apache.spark.sql.Dataset[EmployeeList] = [sequence_num: bigint, id: int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val employee_info = emp.joinWith(employee_list, emp.col(\"emp_id\") === employee_list.col(\"sequence_num\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee.show()\n",
    "employee_list.show()\n",
    "employee_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**  \n",
    "[Self Reading](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins.html#joinWith) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Student(id: Long, name: String, state_id: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class State(s_id: Int, state_name: String, city: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val stu = Seq(\n",
    "    Student(1,\"John\",10),\n",
    "    Student(2,\"Harry\",20),\n",
    "    Student(3,\"Bob\",30),\n",
    "    Student(4,\"Michael\",10),\n",
    "    Student(5,\"Travis\",20)    \n",
    ").toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val state = Seq(\n",
    "    State(10,\"VA\",\"Glen Allen\"),\n",
    "    State(20,\"MA\",\"Harvard\"),\n",
    "    State(30,\"TX\",\"Irvine\"),\n",
    "    State(10,\"VA\",\"Henrico\"),\n",
    "    State(10,\"VA\",\"Herndon\")    \n",
    ").toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val join_stu_state = stu.joinWith(state, stu(\"state_id\") === state(\"s_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_stu_state.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu.show()\n",
    "state.show()\n",
    "join_stu_state.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explode struct type in Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.8 Grouping and Aggregations\n",
    "We can apply the grouping and aggreations similar to DataFrame in Datasets but it will return DataFrame type. So, the information on type will be lost. To keep the same type we can apply several methods. For e.g. `groupByKey` method will allow to perform group by a specific key in the Dataset and it will also return type Dataset.\n",
    "\n",
    "[More info](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.groupByKey(e => e.grade).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It return  \n",
    "org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.groupByKey(e => e.s_id).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.groupByKey(e => e.grade).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.groupByKey(e => e.grade).count().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.9 Write Output to File\n",
    "\n",
    "We can write the output to external storage using `write()` method. It uses DataFrameWriter interface to write a Dataset to external storage systems. We can save to multiple format such as csv, json, parquet, text etc as well as jdbc. [More information about saving output](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataFrameWriter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// write to json file\n",
    "emp.write.format(\"json\").save(\"/tmp/employees/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// write to csv file\n",
    "emp.write.format(\"csv\").save(\"/tmp/employees/dataset1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further reading**  \n",
    "[More information on Dataset API Operators](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-dataset-operators.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
