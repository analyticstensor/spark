{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Spark Structured API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**2.1 RDD**](#2.1-RDD)   \n",
    "[**2.2 DataFrames API**](#2.2-DataFrames-API)   \n",
    "[**2.3 Spark Data Types**](#2.3-Spark-Data-Types)  \n",
    "[**2.3.1 Data Types**](#2.3.1-Data-Types)   \n",
    "[**2.4 DataFrames**](#2.4-DataFrames)   \n",
    "[**2.4.1 Schemas**](#2.4.1-Schemas)     \n",
    "[**2.4.2 Columns and Expressions**](#2.4.2-Columns-and-Expressions)  \n",
    "[**2.4.3 Rows**](#2.4.3-Rows)  \n",
    "[**2.5 Spark Data Sources**](#2.5-Spark-Data-Sources)  \n",
    "[**2.5.1 DataFrameReader**](#2.5.1-DataFrameReader)   \n",
    "[**2.5.2 DataFrameWriter**](#2.5.2-DataFrameWriter)   \n",
    "[**2.6 DataFrame Operations**](#2.6-DataFrame-Operations)   \n",
    "[**2.6.1 Creating DataFrames**](#2.6.1-Creating-DataFrames)  \n",
    "[**2.6.2 Projections**](#2.6.2-Projections)   \n",
    "[**2.6.3 Renaming Columns**](#2.6.3-Renaming-Columns)   \n",
    "[**2.6.4 Dropping Columns**](#2.6.4-Dropping-Columns)   \n",
    "[**2.6.5 Adding Columns**](#2.6.5-Adding-Columns)   \n",
    "[**2.6.6 Changing Column Types**](#2.6.6-Changing-Column-Types)   \n",
    "[**2.6.7 Filtering Rows**](#2.6.7-Filtering-Rows)   \n",
    "[**2.6.8 Limit Rows**](#2.6.8-Limit-Rows)   \n",
    "[**2.6.9 Distinct Rows**](#2.6.9-Distinct-Rows)   \n",
    "[**2.6.10 Sorting Rows**](#2.6.10-Sorting-Rows)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 RDD\n",
    "**RDD**: Resilient Distributed Dataset (RDD) is a Spark's lower-level APIs. RDD are only used when the high level API for Dataset, DataFrame and SQL doesn't satisfy the business or logical requirement for solving the problems. Basically low-level API are used to manipulate RDD and manipulate shared variables i.e. broadcast variable and accumulators.  \n",
    "The main use case for using low-level API are:-  \n",
    "* When high-level APIs doesn't meet the business requirement for manipulation business logics.  \n",
    "* Utilizing and manipulating custom shared variables.  \n",
    "* Maintain existing codes written using RDDs.  \n",
    "* To obtain coarse granularity for controlling partitioning of data over cluster for performance and optimization.\n",
    "\n",
    "**Brief on RDD**  \n",
    "All the high level data structure (DataFrames, Datasets) in Spark are logically converted in RDD internally during job execution. RDDs is an immutable, partitioned collection of records. RDDs is an object of records. We can perform any operation on these objects. \n",
    "\n",
    "**Properties of RDD**. \n",
    "* A list of partitions.\n",
    "* A function for compution each split.\n",
    "* A list of dependencies on other RDDs.\n",
    "* A Partitioner for key-value RDDs.\n",
    "* A list of preferred locations to compute each split\n",
    "\n",
    "**Compare RDDs with DataFrame and Datasets**  \n",
    "RDDs has same operating Spark paradigms for executing jobs similar to DataFrame and Datasets. RDDs has both tranformations and actions. But there is no theorical concept of rows/records in RDDs. Each records are treated as raw Java/Scala/Python objects. RDDs also provides APIs for Scala, Java and Python. Some performance are lost while using Python APIs compared to Scala and Java for manipulation RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample: 3 simplified with avg function\\n\\nfrom pyspark.sql.functions import avg\\n\\n# create a DataFrame using SparkSession\\ndataDF = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\\n# Group the same names together, aggregate their age, and compute an average\\navgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))\\n# show the results of the final execution\\navgDF.show()\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open pySpark console and run the following codes:\n",
    "\"\"\"\n",
    "Example: 1\n",
    "dataset = range(1,1000)\n",
    "spark\n",
    "sc\n",
    "dataset\n",
    "dataset[5]\n",
    "datasetRDD = sc.parallelize(dataset,2)  # create RDD\n",
    "datasetRDD\n",
    "datasetRDD.take(5)  # display first 5 elements from RDD\n",
    "datasetRDD.count() # Count total elements in \n",
    "datasetRDD.first()  # Display first element in RDD\n",
    "datasetRDD.top(10) # Display top elements in RDD\n",
    "datasetRDD.filter(lambda x: x%2 == 0).collect() # Filter only even number\n",
    "\n",
    "Example: 2\n",
    "userComment = \"I like to visit New York. The public transportation are very reliable.\".split(\" \")\n",
    "allwords = sc.parallelize(userComment, 2)\n",
    "allwords.setName(\"Comments\")\n",
    "allwords.name()\n",
    "allwords.count()\n",
    "allwords.distinct().count()\n",
    "allwords.foreach(print)\n",
    "def likewords(words):\n",
    "    return words.startswith(\"like\")\n",
    "allwords.filter(lambda word: likewords(word)).collect()\n",
    "\"\"\"\n",
    "\n",
    "# Similar we can perform various transformation such as: distinct, filter, map, flatmap, sort\n",
    "# Various Actions such as: reduce, count, countapprox, countapproxdistinct, countbyvalue, first, max, min, take\n",
    "\n",
    "\"\"\"\n",
    "Saving output to file\n",
    "allwords.saveAsTextFile(\"file:/tmp/sparkwordSplit\")\n",
    "## Check the file on /tmp/sparkwordSplit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Example: 3\n",
    "# create an RDDs of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)])\n",
    "# use map and reduceByKey transformations with their \n",
    "# lambda expressions to aggregate and then compute average\n",
    "agesRDD = dataRDD.map(lambda x,y: (x, (y,1))) \\\n",
    "          \t.reduceByKey(lambda x,y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "           .map(lambda x, y, z: (x, y / z))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Example: 3 simplified with avg function\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# create a DataFrame using SparkSession\n",
    "dataDF = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "# Group the same names together, aggregate their age, and compute an average\n",
    "avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))\n",
    "# show the results of the final execution\n",
    "avgDF.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/apache-spark/spark-3.0.0\n",
      "1.txt\n",
      "\u001b[1m\u001b[32mals.py\u001b[m\u001b[m\n",
      "avro_inputformat.py\n",
      "\u001b[1m\u001b[32mkmeans.py\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[32mlogistic_regression.py\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[34mml\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[34mmllib\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[32mpagerank.py\u001b[m\u001b[m\n",
      "parquet_inputformat.py\n",
      "\u001b[1m\u001b[32mpi.py\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[32msort.py\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[34msql\u001b[m\u001b[m\n",
      "status_api_demo.py\n",
      "\u001b[1m\u001b[34mstreaming\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[32mtransitive_closure.py\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[32mwordcount.py\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# Executing Python Word Count Script in Spark\n",
    "!echo $SPARK_HOME   # print spark home\n",
    "!ls -1 $SPARK_HOME/examples/src/main/python # list all file on given location\n",
    "## Step 1: Let's try to understand the python code from wordcount.py script\n",
    "## Step 2: Copy the wordcount.py on your script location. i.e. under Chapter_2/script\n",
    "## Step 3: Create any file on same location based on your choice. Choose some interesting topic.\n",
    "## For example: Let's choose current business, entertainment, political, scientific, sport news. And copy and paste in file.\n",
    "## This example uses State of union of 2019 from the link below\n",
    "## https://www.cnn.com/2019/02/05/politics/donald-trump-state-of-the-union-2019-transcript/index.html\n",
    "\n",
    "# Step 4: copy the wordcount script\n",
    "## !mv $SPARK_HOME/examples/src/main/python/wordcount.py destination_directory\n",
    "## Step 5: Run the script using command below\n",
    "## spark-submit wordcount.py state_of_union_2019.txt \n",
    "## Try to understand the output and look the SparkUI on http://localhost:4040\n",
    "## Assignment: Order the top word on descending order, Remove any stopwords, Save the output to file and create wordcloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and paste the code below in new python file. Execute the code using command below:   \n",
    "`spark-submit script_name.py`   \n",
    "Assignment: Try to read the comment from file and store the output to file instead of console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Code Snippet 1: Creating RDD in PySpark\\n# import SparkContext and config\\nfrom pyspark import SparkContext\\nfrom pyspark import SparkConf\\n\\nSPARK_MASTER=\\'local\\'\\nSPARK_APP_NAME=\\'Word StartingwithS\\'\\n\\nconf = SparkConf().setMaster(SPARK_MASTER)         .setAppName(SPARK_APP_NAME)\\n    \\nsc = SparkContext(conf=conf)\\n\\nuserComment = \"I like to visit New York. The public transportation are very reliable. There are many shopping complex and sports actitivies. Shopping is always fun in Manhattan\"\\n\\nlistWords = userComment.split(\" \")\\nallwords = sc.parallelize(listWords, 2)\\n\\ndef likewords(words):\\n    return words.lower().startswith(\"s\")\\n    \\nfilter_words = allwords.filter(lambda word: likewords(word)).collect()\\n\\nfor w in filter_words:\\n    print(w)\\n\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Code Snippet 1: Creating RDD in PySpark\n",
    "# import SparkContext and config\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "SPARK_MASTER='local'\n",
    "SPARK_APP_NAME='Word StartingwithS'\n",
    "\n",
    "conf = SparkConf().setMaster(SPARK_MASTER) \\\n",
    "        .setAppName(SPARK_APP_NAME)\n",
    "    \n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "userComment = \"I like to visit New York. The public transportation are very reliable. There are many shopping complex and sports actitivies. Shopping is always fun in Manhattan\"\n",
    "\n",
    "listWords = userComment.split(\" \")\n",
    "allwords = sc.parallelize(listWords, 2)\n",
    "\n",
    "def likewords(words):\n",
    "    return words.lower().startswith(\"s\")\n",
    "    \n",
    "filter_words = allwords.filter(lambda word: likewords(word)).collect()\n",
    "\n",
    "for w in filter_words:\n",
    "    print(w)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparkContext and SparkSession**\n",
    "\n",
    "Prior to Spark 2.0, `SparkContext` was the entry point of all Spark application which is used to access all Spark features and Spark Configuration. After Spark 2.0, `SparkSession` is an unified entry point of all Spark Application. Spark Session interact with various Spark's functionality with less amount of constructs. In Spark 2.0, various context such as Spark context, hive context, SQL context are all encapsulated in a Spark Session. So Spark session combines all the different contexts.\n",
    "\n",
    "Figure 2.1: SparkSession\n",
    "![Figure 1](spark_session.png)\n",
    "\n",
    "**Spark Session**. \n",
    "Spark Session is created using a builder pattern. The spark session builder tries to get spark session if it already exists or create new session if it doesn't exits.\n",
    "\n",
    "Example below shows creating Spark session in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.67:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AppsName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe678fc7c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Spark session in Python\n",
    "from pyspark.sql import SparkSession\n",
    "SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"AppsName\") \\\n",
    "     .config(\"config.option\", \"value\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If there is an Py4JError like below.        \n",
    "`Py4JError: org.apache.spark.api.python.PythonUtils.getEncryptionEnabled does not exist in the JVM`  \n",
    "**Fixes**: Set environment variable for PYTHONPATH in bash_profile.     \n",
    "export SPARK_HOME=\"/usr/local/apache-spark/spark-3.0.0\"  \n",
    "export PYTHONPATH=\"$SPARK_HOME/python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 DataFrames API\n",
    "Spark DataFrame are distributed table-like collection of rows and named columns with a schema. Each columns has  a specific data types and must have same number of rows which also accepts `null` values. Like RDDs, DataFrames are immutable and keeps lineage of all transformation including the columns. It uses lazy evaluation to perform operation. An action in DataFrame performs transformation and return results.\n",
    "\n",
    "Figure 2.1(a): Spark Catalyst Optimizer  \n",
    "![Figure 2.1(a)](spark_catalyst_optimizer.png)\n",
    " \n",
    "\n",
    "Figure 2.1(b): Structured API Logical Plan$^1$  \n",
    "![Figure 2.1(b)](https://databricks.com/wp-content/uploads/2018/05/Catalyst-Optimizer-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Spark Data Types\n",
    "Spark has its own internal data type representation. Data types are define while creating schema for writing Spark application. The code snippet below show the data type definition for Scala, Java and Python.  \n",
    "\n",
    "**`Note: In this course we will concentrate on Python using PySpark APIs`**. \n",
    "\n",
    "\n",
    "**Scala types:**   \n",
    "`\n",
    "import org.apache.spark.sql.types._  \n",
    "val firstname = ByteType`  \n",
    "\n",
    "**Java types:**  \n",
    "`\n",
    "import org.apache.spark.sql.type.DataTypes;  \n",
    "ByteType firstname = DataTypes.ByteType  \n",
    "`\n",
    "\n",
    "**Python type:**  \n",
    "`\n",
    "from pyspark.sql.types import *  \n",
    "firstname = ByteType()\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Data Types\n",
    "\n",
    "Table 2.1: Python Basic Data Types in Spark$^2$    \n",
    "\n",
    "| Types | Data Type | Value Assigned in Python | APIs to instantiate |\n",
    "| --------- | --------- | ------------------------ | ------------------- |  \n",
    "| Numeric Types | ByteType | int or long. Number will be converted to 1-byte signed integer number during runtime. Range of number -128 to 127. | ByteType() |\n",
    "| Numeric Types | ShortType | int or long. Number will be converted to 2-byte signed integer number during runtime. Range of number -32768 to 32767. | ShortType() |\n",
    "| Numeric Types | IntegerType | int or long\t| IntegerType() |\n",
    "| Numeric Types | LongType | Long. Number will be converted to 8-byte signed integer number during runtime. Range of number -9223372036854775808 to 9223372036854775807 | LongType() |\n",
    "| Numeric Types | FloatType | float. Number will be conveted to 4-byte single-precision floating-point during runtime. | FloatType() |\n",
    "| Numeric Types | DoubleType | float | DoubleType() |\n",
    "| Numeric Types | DecimalType | decimal.Decimal | DecimalType() |\n",
    "| String Types | StringType | string | StringType() |\n",
    "| Binary Type | BinaryType | bytearray | BinaryType() |\n",
    "| Boolean Type | BooleanType | bool | BooleanType() |\n",
    "| Datetime Type | TimestampType | datetime.datetime | TimestampType() | \n",
    "| Datetime Type | DateType | datetime.date | DateType() |\n",
    "| Complex Type | ArrayType | list, tuple, or array | ArrayType(dataType, [containsNull]). The default value for containsNull is True.  |\n",
    "| Complex Type | MapType | dict | MapType(keyType, valueType, [valueContainsNull]). The default value for valueContainsNull is True. |\n",
    "| Complex Type | StructType | list or tuple. | StructType(fields). Fields is a list of StructFields. Fields with same name are not accepted. |\n",
    "| Complex Type | StructField | The value type in Python of the data type of this field. Like Int for StructField with data type IntegerType | StructField(name, dataType, [nullable]). The default value of nullabe is True. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 DataFrames\n",
    "DataFrame is composed of :-  \n",
    "* Columns\n",
    "* Rows\n",
    "\n",
    "DataFrame is similar to spreadSheet, table, pandas DataFrame or any other object that holds column definition and contains rows/record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Schemas\n",
    "Schema defines the column names and its associated data types for a DataFrame. Schema is very important while reading data from structured DataSources. The benefit of defining schema in Spark are:-  \n",
    "* Reduce Spark Engine to infer data types, where Spark need to read all data to identify types.\n",
    "* Early error detection when data doesn't match with the schema.\n",
    "\n",
    "Schema can be defined in two ways:-  \n",
    "1. automatically while reading the data which is also known as schema-on-read.\n",
    "2. manually by defining ourself explicitly before reading the data. \n",
    "\n",
    "Defining schema varies upon the use-case. For e.g. In ETL, defining schema explicitly is best practics. In adhoc reporting, schema-on-read is easier.\n",
    "\n",
    "\n",
    "* Schema is a **`StructType`** made from number of fields.\n",
    "* **`StructField`** has column name, data type, boolean flag (for defining missing or null values) and optional metadata for the column.\n",
    "* Schema can also contain complex types i.e. **`StructType`** of **`StructType`**\n",
    "\n",
    "We'll create simple schema in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = false)\n",
      " |-- birth_date: string (nullable = false)\n",
      " |-- FIRST_NAME: string (nullable = false)\n",
      " |-- LAST_NAME: string (nullable = false)\n",
      " |-- GENDER: string (nullable = false)\n",
      " |-- hire_date: string (nullable = false)\n",
      "\n",
      "None\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|FIRST_NAME|LAST_NAME|GENDER| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|     1|01/19/1950|     Jimmy|      Doe|     M|1975-01-15|\n",
      "|     2|08/15/1952|     Smith|   Butler|     F|1972-01-25|\n",
      "|     3|12/10/1953|     David|  Jackson|     M|1980-02-22|\n",
      "|     4|05/25/1960|      Jina|  Unknown|     F|1990-10-15|\n",
      "|     5|12/16/1945|     Smith|  Unknown|     F|1965-07-05|\n",
      "|     6|03/12/1980|       Jim|  Unknown|    NA|2005-12-20|\n",
      "|     7|08/23/1981|     Jimmy|  Unknown|     F|2015-11-23|\n",
      "|     8|07/1975/24|     Jimmy|  Unknown|    NA|2019-07-11|\n",
      "|     9|02/21/1990|     Jimmy|  Unknown|     F|2018-05-07|\n",
      "|    10|01/18/1991|     Jimmy|  Unknown|    NA|2014-12-01|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Python code to create schema and associate data with given schema\n",
    "\n",
    "# Load types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Schema\n",
    "empSchema = StructType([\n",
    "    StructField(\"emp_no\", IntegerType(), False),\n",
    "    StructField(\"birth_date\", StringType(), False),\n",
    "    StructField(\"FIRST_NAME\", StringType(), False),\n",
    "    StructField(\"LAST_NAME\", StringType(), False),\n",
    "    StructField(\"GENDER\", StringType(), False),\n",
    "    StructField(\"hire_date\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Create Data\n",
    "empData = [\n",
    "    [1, \"01/19/1950\", 'Jimmy', 'Doe', 'M', \"1975-01-15\"],\n",
    "    [2, \"08/15/1952\", 'Smith', 'Butler', 'F', \"1972-01-25\"],\n",
    "    [3, \"12/10/1953\", 'David', 'Jackson', 'M', \"1980-02-22\"],\n",
    "    [4, \"05/25/1960\", 'Jina', 'Unknown', 'F', \"1990-10-15\"],\n",
    "    [5, \"12/16/1945\", 'Smith', 'Unknown', 'F', \"1965-07-05\"],\n",
    "    [6, \"03/12/1980\", 'Jim', 'Unknown', 'NA', \"2005-12-20\"],\n",
    "    [7, \"08/23/1981\", 'Jimmy', 'Unknown', 'F', \"2015-11-23\"],\n",
    "    [8, \"07/1975/24\", 'Jimmy', 'Unknown', 'NA', \"2019-07-11\"],\n",
    "    [9, \"02/21/1990\", 'Jimmy', 'Unknown', 'F', \"2018-05-07\"],\n",
    "    [10, \"01/18/1991\", 'Jimmy', 'Unknown', 'NA', \"2014-12-01\"],  \n",
    "]\n",
    "\n",
    "# Create Sparksession\n",
    "spark = SparkSession.builder.appName(\"Employee\").getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "employee_df = spark.createDataFrame(empData,empSchema)\n",
    "print()\n",
    "# print schema\n",
    "print(employee_df.printSchema())\n",
    "# show dataframe\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|FIRST_NAME|LAST_NAME|GENDER| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|     1|01/19/1950|     Jimmy|      Doe|     M|1975-01-15|\n",
      "|     2|08/15/1952|     Smith|   Butler|     F|1972-01-25|\n",
      "|     3|12/10/1953|     David|  Jackson|     M|1980-02-22|\n",
      "|     4|05/25/1960|      Jina|  Unknown|     F|1990-10-15|\n",
      "|     5|12/16/1945|     Smith|  Unknown|     F|1965-07-05|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Columns and Expressions\n",
    "\n",
    "**Columns**: Columns defines the data types and holds values. Columns in Spark DataFrame are similar to columns in any database table, Pandas DataFrames, spreadsheet etc. Columns can be referred and constructed in multiple ways. The simplest method is using function below:     \n",
    "`col()`  \n",
    "`column()`  \n",
    "Both functions takes `column name` as argument. In Python, we can use either functions shown below to refer column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'FIRST_NAME'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "# We'll use the existing employee dataframe \n",
    "col(\"emp_no\")\n",
    "column(\"FIRST_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|emp_no|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(\"emp_no\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`emp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_no`' given input columns: [FIRST_NAME, GENDER, LAST_NAME, birth_date, emp_no, hire_date];;\n'Project ['emp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_no]\n+- LogicalRDD [emp_no#0, birth_date#1, FIRST_NAME#2, LAST_NAME#3, GENDER#4, hire_date#5], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o55.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`emp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_no`' given input columns: [FIRST_NAME, GENDER, LAST_NAME, birth_date, emp_no, hire_date];;\n'Project ['emp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_no]\n+- LogicalRDD [emp_no#0, birth_date#1, FIRST_NAME#2, LAST_NAME#3, GENDER#4, hire_date#5], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:122)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:125)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:66)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:87)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3436)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1413)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-70528b4a8911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memployee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"emp_no\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \"\"\"\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`emp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_no`' given input columns: [FIRST_NAME, GENDER, LAST_NAME, birth_date, emp_no, hire_date];;\n'Project ['emp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_noemp_no]\n+- LogicalRDD [emp_no#0, birth_date#1, FIRST_NAME#2, LAST_NAME#3, GENDER#4, hire_date#5], false\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(\"emp_no\" * 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(emp_no * 10)|\n",
      "+-------------+\n",
      "|           10|\n",
      "|           20|\n",
      "|           30|\n",
      "|           40|\n",
      "|           50|\n",
      "|           60|\n",
      "|           70|\n",
      "|           80|\n",
      "|           90|\n",
      "|          100|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(col(\"emp_no\") * 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expressions**: Expression is an operation and transformation performed in a columns. With expressions, the columns values can manipulated and modified. `expr` is a *pyspark.sql.functions* and *org.apache.spark.sql.functions* package. The expression created with `expr` functions always reference to DataFrame column. i.e  \n",
    "`expr(\"ColumnName\")` is similar to `col(\"ColumnName\")`  \n",
    "For example, `expr(\"columnName + 10\")`\n",
    "\n",
    "**Representing Columns as Expressions**   \n",
    "When we want to apply or perform transformation on column then we use column reference using `col()` and apply transformation as shown below:  \n",
    "`col(\"price\") - 5`   \n",
    "\n",
    "The same feature can be implemented using an expression with `expr()` function. This function will parse the transformations and column reference from string. The comparsion are shown below:\n",
    "\n",
    "`expr(\"price - 5\")` or `expr(\"price\") - 5` is similar to `col(\"price\") - 5`   \n",
    "\n",
    "So, Columns is a subset of expression functionality. In Python, we use expression as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+\n",
      "|emp_no|emp_no|(emp_no * 10)|\n",
      "+------+------+-------------+\n",
      "|     1|     1|           10|\n",
      "|     2|     2|           20|\n",
      "|     3|     3|           30|\n",
      "|     4|     4|           40|\n",
      "|     5|     5|           50|\n",
      "|     6|     6|           60|\n",
      "|     7|     7|           70|\n",
      "|     8|     8|           80|\n",
      "|     9|     9|           90|\n",
      "|    10|    10|          100|\n",
      "+------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "employee_df.select(expr(\"emp_no\"), col(\"emp_no\"),expr(\"emp_no * 10\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((emp_no * 100) - 30) + 200) < 50)'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr(\"(((emp_no * 100) - 30) + 200) < 50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Rows\n",
    "**Rows**: Rows is a single record that contain one or more columns. Spark represent record as a *Row* object which is stored as arrays of bytes. *Row* object are used to create rows manually. The example below shows creating row in Python using Row object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Analytics Tensor\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "record1 = Row(100, \"Analytics Tensor\", 1)  # Create row\n",
    "\n",
    "print(record1[0])  # Accessing field 1\n",
    "print(record1[1])  # Accessing field 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Spark Data Sources\n",
    "Spark supports variety of data sources both for reading and writing. As of Spark 2.4.4, the core data sources are:  \n",
    "* CSV\n",
    "* JSON\n",
    "* Parquet\n",
    "* ORC\n",
    "* JDBC/ODBC connections\n",
    "* Plain-text files\n",
    "\n",
    "Other supported data sources contributed by the community are Cassandra, HBase, MongoDB, AWSRedshift, XML etc.\n",
    "\n",
    "The high-level data source APIs for reading and writing are DataFrameReader and DataFrameWriter respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 DataFrameReader\n",
    "**DataFrameReader**: DataFrameReader is the core construct for reading data into a DataFrame. `DataFrameReader` is accessed through `SparkSession` with **`read`** attribute. i.e. `spark.read` or `spark.readStream` for reading static and streaming data source respectively.    \n",
    "\n",
    "The basic structure for reading data is shown below:   \n",
    "`DataFrameRead.format(....).option(\"key\", \"value\").schema(....).load()`\n",
    "\n",
    "\n",
    "[To learn more about DataFrameReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)$^3$\n",
    "\n",
    "\n",
    "After applying read method we'll specify other methods such as: `format`,`schema`,`mode`,`option`. Among these `option` method should be defined since other method can use default value. Each data source has it's own set of option for reading data. \n",
    "\n",
    "Code snippet below shows reading different files into DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filelocation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-7cd44622595a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FAILFAST\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilelocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefineSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filelocation' is not defined"
     ]
    }
   ],
   "source": [
    "# Read csv file\n",
    "df1 = spark.read.format(\"csv\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"path\", filelocation) \\\n",
    "    .schema(defineSchema) \\\n",
    "    .load()  \n",
    "    \n",
    "# Read parquet file\n",
    "df2 = spark.read.format(\"parquet\") \\\n",
    "    .option(\"path\", filelocation) \\\n",
    "    .load()\n",
    "    \n",
    "# Read file using default format    \n",
    "df3 = spark.read.option(\"path\", filelocation).load()\n",
    "    \n",
    "# Read JSON file\n",
    "df3 = spark.read.format(\"json\") \\\n",
    "    .option(\"path\", \"/tmp/datasets/employees/json/*\") \\\n",
    "    .load()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table: 2.4.1 DataFrameReader methods, arguments and options\n",
    "\n",
    "| Method | Arguments | Description |\n",
    "|:---------------:|:---------------:|:---------------:|\n",
    "| format() | parquet, csv, txt, json, jdbc, orc, avro | Parquet is default or whatever specified in spark.sql.sources.default |\n",
    "| option() | 1. ( mode, {permissive, dropmalformed, failfast} ) 2. ( inferSchema, {true, false} ) 3. ( path,  path_to_data_source) | A series of key/value pairs and options. |\n",
    "| schema() | DDL String or StructType | We can provide schema or specify to infer the schema in the option() | \n",
    "| load() | “path to a file” | Specifies the path to the data source. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2.4.1(a) Spark read modes\n",
    "\n",
    "| Read Mode | Description |\n",
    "| --------- | ----------- |\n",
    "| permissive | Sets all fields to **null** and store the corrupted records into a field `_corrupt_record`, configured by `columnNameofCorruptRecord`. This is default mode | \n",
    "| dropmalformed | Ignore the whole records when corrupted records is found. | \n",
    "| failfast | Throws an exception when the corrupted record is found. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 DataFrameWriter\n",
    "**DataFrameWriter**: DataFrameWriter is the core construct for writing data from a DataFrame into external data sources. `DataFrameWriter` is not accessed through `SparkSession` but from DataFrame we are saving into.\n",
    "\n",
    "The basic structure for writing data is shown below:   \n",
    "`DataFrameWriter.format(....).option(....).partitionBy(....).bucketBy(....).sortBy(....).save()`   \n",
    "\n",
    "\n",
    " [To learn more about DataFrameWriter](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter)$^4$ \n",
    " \n",
    " \n",
    "After applying write method we'll specify other methods such as: `format`,`option`,`save` etc. `format` is optional where Spark uses parquet as default file format. `option` specifies how to save our data and must be supplied.\n",
    "\n",
    "\n",
    "Code snippet below shows writing DataFrame into files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-c692db22c2e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write dataframe into csv file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OVERWRITE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"path_to_file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Write dataframe into csv file format\n",
    "df.write.format(\"csv\") \\\n",
    ".option(\"mode\", \"OVERWRITE\") \\\n",
    ".option(\"path\", \"path_to_file\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2.4.2(a) Spark save modes\n",
    "\n",
    "| Save Mode | Description |\n",
    "| --------- | ----------- |\n",
    "| append | Append the output file if file already exists in output location. | \n",
    "| overwrite | Overwrite the file if file already exists in output location.  Basically it is drop and re-create. | \n",
    "| errorIfExists | Throw an error and fail if files already exists in output location. This is default mode. |\n",
    "| ignore | Ignore and do nothing if file already exist in output location. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 DataFrame Operations\n",
    "DataFrame operations or transformations is the process of manipulating DataFrames which includes various tasks based on the business and logical functionality applied into DataFrames. Some of the common operation are adding new columns based on existing values, dropping columns while creating final output/report, aggregation, sorting, merge, convert data types etc. All of the requirement differs based on the needs but the basic concept is manipulating data to make it readable by applying transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1 Creating DataFrames\n",
    "Although we have seen syntax for reading data for multiple data sources but haven't loaded yet. In this tutorial, we'll use `Mysql Employee` database and some JSON and CSV files both from local and Cloud location.\n",
    "\n",
    "**Prerequisties**   \n",
    "1. [Spark running on local mode](https://spark.apache.org/downloads.html)$^5$\n",
    "2. [Jupyter Notebook](https://www.anaconda.com/distribution/#download-section)$^6$\n",
    "3. [Local Instances of MYSQL Database with Employee database](https://dev.mysql.com/downloads/mysql/)$^7$\n",
    "4. [AWS account with access to S3 bucket](https://portal.aws.amazon.com/billing/signup#/start)$^8$\n",
    "5. [Databricks account for community version](https://community.cloud.databricks.com/login.html)$^9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Links**:   \n",
    "* [PySpark SQL Module](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#)$^{10}$\n",
    "* [Spark Transformation and Actions](https://training.databricks.com/visualapi.pdf)$^{11}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.) Loading comma seperated value (csv) file from Local Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark session in Python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"Online Retail Apps\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "file_location = \"data/Online_Retail.csv\"\n",
    "\n",
    "online_retail = spark.read.format(\"csv\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"path\", file_location) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.) Loading employee table from MySQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If there is JDBC driver error then download driver and store under spark/lib location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Employee Apps\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "employeesDF = spark.read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", \"jdbc:mysql://localhost:3306/employees\") \\\n",
    "  .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "  .option(\"dbtable\", \"employees\") \\\n",
    "  .option(\"user\", \"root\") \\\n",
    "  .option(\"password\", \"Mysql123#\") \\\n",
    "  .option(\"serverTimezone\", \"EST\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display top 10 records from Employee DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.limit(10).show()\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT * from employees LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.) Loading Parquet file from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo load data from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.) Loading ORC file from Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo load data from HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5.) Loading JSON file from Databricks File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo load data from DBFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6.) Loading data built from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script is already there, copy and paste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 Projections\n",
    "Projection is the ability to select all or individual columns. Spark uses following two methods to select the columns.   \n",
    "* `select()`   \n",
    "* `selectExpr()`   \n",
    "We need to pass the column name as string for parameter. `selectExpr()` is the best method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|Description                        |\n",
      "+-----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE METAL LANTERN                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "|SET 7 BABUSHKA NESTING BOXES       |\n",
      "|GLASS STAR FROSTED T-LIGHT HOLDER  |\n",
      "|HAND WARMER UNION JACK             |\n",
      "|HAND WARMER RED POLKA DOT          |\n",
      "|ASSORTED COLOUR BIRD ORNAMENT      |\n",
      "|POPPY'S PLAYHOUSE BEDROOM          |\n",
      "|POPPY'S PLAYHOUSE KITCHEN          |\n",
      "|FELTCRAFT PRINCESS CHARLOTTE DOLL  |\n",
      "|IVORY KNITTED MUG COSY             |\n",
      "|BOX OF 6 ASSORTED COLOUR TEASPOONS |\n",
      "|BOX OF VINTAGE JIGSAW BLOCKS       |\n",
      "|BOX OF VINTAGE ALPHABET BLOCKS     |\n",
      "|HOME BUILDING BLOCK WORD           |\n",
      "|LOVE BUILDING BLOCK WORD           |\n",
      "|RECIPE BOX WITH METAL HEART        |\n",
      "+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select single column\n",
    "online_retail.select(\"Description\").show(20,False)\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT description from online_retail LIMIT 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+\n",
      "|InvoiceNo|StockCode|Quantity|\n",
      "+---------+---------+--------+\n",
      "|   536365|   85123A|       6|\n",
      "|   536365|    71053|       6|\n",
      "|   536365|   84406B|       8|\n",
      "|   536365|   84029G|       6|\n",
      "|   536365|   84029E|       6|\n",
      "|   536365|    22752|       2|\n",
      "|   536365|    21730|       6|\n",
      "|   536366|    22633|       6|\n",
      "|   536366|    22632|       6|\n",
      "|   536367|    84879|      32|\n",
      "+---------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select multiple columns in Python\n",
    "online_retail.select(\"InvoiceNo\", \"StockCode\", \"Quantity\").show(10)\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT InvoiceNo, StockCode, Quantity from online_retail LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/10 8:26|2.55     |17850     |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/10 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/10 8:26|2.75     |17850     |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/10 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/10 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |22752    |SET 7 BABUSHKA NESTING BOXES       |2       |12/1/10 8:26|7.65     |17850     |United Kingdom|\n",
      "|536365   |21730    |GLASS STAR FROSTED T-LIGHT HOLDER  |6       |12/1/10 8:26|4.25     |17850     |United Kingdom|\n",
      "|536366   |22633    |HAND WARMER UNION JACK             |6       |12/1/10 8:28|1.85     |17850     |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT          |6       |12/1/10 8:28|1.85     |17850     |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT      |32      |12/1/10 8:34|1.69     |13047     |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.select(\"*\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all columns\n",
    "online_retail.selectExpr(\"*\").show(10)\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT * from online_retail LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using expression to select columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|InvoiceNo|CustomerID|       Country|\n",
      "+---------+----------+--------------+\n",
      "|   536365|     17850|United Kingdom|\n",
      "|   536365|     17850|United Kingdom|\n",
      "|   536365|     17850|United Kingdom|\n",
      "|   536365|     17850|United Kingdom|\n",
      "|   536365|     17850|United Kingdom|\n",
      "|   536365|     17850|United Kingdom|\n",
      "|   536365|     17850|United Kingdom|\n",
      "|   536366|     17850|United Kingdom|\n",
      "|   536366|     17850|United Kingdom|\n",
      "|   536367|     13047|United Kingdom|\n",
      "+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select multiple columns in Python\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "online_retail.select( \n",
    "    expr(\"InvoiceNo\"), \n",
    "    col(\"CustomerID\"),\n",
    "    column(\"Country\"))\\\n",
    "    .show(10)\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT InvoiceNo, CustomerID, Country from online_retail LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|InvoiceNo|InvoiceNo|\n",
      "+---------+---------+\n",
      "|   536365|   536365|\n",
      "|   536365|   536365|\n",
      "|   536365|   536365|\n",
      "|   536365|   536365|\n",
      "|   536365|   536365|\n",
      "|   536365|   536365|\n",
      "|   536365|   536365|\n",
      "|   536366|   536366|\n",
      "|   536366|   536366|\n",
      "|   536367|   536367|\n",
      "+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.select(col(\"InvoiceNo\"), \"InvoiceNo\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   ID|\n",
      "+-----+\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|13047|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.selectExpr(\"CustomerID as ID\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alias Column Name using multiple methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   ID|\n",
      "+-----+\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|17850|\n",
      "|13047|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.select(expr(\"CustomerID as ID\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|cust_id|\n",
      "+-------+\n",
      "|  17850|\n",
      "|  17850|\n",
      "|  17850|\n",
      "|  17850|\n",
      "|  17850|\n",
      "|  17850|\n",
      "|  17850|\n",
      "|  17850|\n",
      "|  17850|\n",
      "|  13047|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.select(expr(\"CustomerID as ID\").alias(\"cust_id\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|   ID|UnitPrice|\n",
      "+-----+---------+\n",
      "|17850|     2.55|\n",
      "|17850|     3.39|\n",
      "|17850|     2.75|\n",
      "|17850|     3.39|\n",
      "|17850|     3.39|\n",
      "|17850|     7.65|\n",
      "|17850|     4.25|\n",
      "|17850|     1.85|\n",
      "|17850|     1.85|\n",
      "|13047|     1.69|\n",
      "+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.selectExpr(\"CustomerID as ID\", \"UnitPrice\").show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SelectExpr` is used to build complex expression for selecting columns as well as creating new columns. In the example below, we'll choose all the column and add new column based on validation rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|SameID|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom| false|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom| false|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom| false|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom| false|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom| false|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom| false|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom| false|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom| false|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom| false|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom| false|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.selectExpr(\"*\", # Choose all columns\n",
    "                     \"(InvoiceNo = CustomerID) as SameID\")\\\n",
    "                     .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SelectExpr()` is also very important while aggregating the data by applying multiple functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Distinct Stock|\n",
      "+--------------+\n",
      "|          1049|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "online_retail.selectExpr(\"count(distinct(StockCode)) as `Distinct Stock`\").show(10)\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT count(distinct(StockCode)) as 'Distinct Stock' from online_retail limit 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literal values are passing explicit values. If we need literal values for some validation then we use literal functions in expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+-----+---+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|valid|Ten|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+-----+---+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|Valid| 10|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|Valid| 10|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|Valid| 10|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|Valid| 10|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|Valid| 10|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|Valid| 10|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|Valid| 10|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|Valid| 10|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|Valid| 10|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|Valid| 10|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "online_retail.select(expr(\"*\"), lit(\"Valid\").alias(\"valid\"), lit(10).alias(\"Ten\")).show(10)\n",
    "# Equivalent SQL query\n",
    "# SELECT *, \"valid\" as valid, 10 as Ten from online_retail limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.3 Renaming Columns\n",
    "`withColumnRenamed()` method is used to rename a column. This method takes two argument. The first argument is the orginal column name to be renamed and second is the new column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_retail.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Invoice_No',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_retail.withColumnRenamed(\"InvoiceNo\", \"Invoice_No\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice_No: string (nullable = true)\n",
      " |-- Stock_Code: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_output = online_retail.withColumnRenamed(\"InvoiceNo\", \"Invoice_No\") \\\n",
    "                    .withColumnRenamed(\"StockCode\", \"Stock_Code\") \\\n",
    "                    .withColumnRenamed(\"CustomerID\", \"employee_id\")\n",
    "final_output.printSchema()\n",
    "final_output.write.format(\"csv\").option(\"path\", \"/tmp/online_retail_data12\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.4 Dropping Columns\n",
    "Column can be dropped or removed using `drop()` method. If we want to drop multiple columns then specify all the column name in delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_retail.drop(\"Country\").columns # drop single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice_No: string (nullable = true)\n",
      " |-- Stock_Code: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice_No: string (nullable = true)\n",
      " |-- Stock_Code: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Invoice_No: string (nullable = true)\n",
      " |-- Stock_Code: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_output = final_output.drop(\"Country\")\n",
    "final_output.printSchema()\n",
    "final_output.drop(\"emp_id\", \"CustomerID\", \"abc\").columns # drop multiple columns\n",
    "final_output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = \"Quantity, InvoiceDate, UnitPrice ,employee_id\"\n",
    "final_output = final_output.drop(\"Quantity\", \"InvoiceDate\", \"UnitPrice\" ,\"employee_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Invoice_No: string, Stock_Code: string, Description: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.drop(drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice_No: string (nullable = true)\n",
      " |-- Stock_Code: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_retail.drop(\"Country\", \"CustomerID\").columns # drop multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.5 Adding Columns\n",
    "`withColumn()` method is used to add new column to a DataFrame. `withColumn` take two arguments where first values is column name to be added and second is the expression to create value for the given row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+---------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|SeqNumber|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+---------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|        1|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|        1|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|        1|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|        1|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|        1|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|        1|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|        1|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|        1|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|        1|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|        1|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "online_retail.withColumn(\"SeqNumber\", lit(1)).show(10)\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT *, 1 as SeqNumber from online_retail limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|yearofPurchase|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|          null|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|         71053|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|          null|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|          null|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|          null|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|         22752|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|         21730|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|         22633|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|         22632|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|         84879|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- yearofPurchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "final_output = online_retail.withColumn(\"yearofPurchase\", substring(\"StockCode\",0,6).cast(\"integer\"))\n",
    "final_output.show(10)\n",
    "final_output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+--------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|isSameID|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+--------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|   false|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|   false|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|   false|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|   false|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|   false|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|   false|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|   false|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|   false|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|   false|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|   false|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.withColumn(\"isSameID\", expr(\"InvoiceNo == StockCode\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.6 Changing Column Types\n",
    "We can change column type into different type using `cast()` method. This is also known as type casting. The input for this method will be the string of data type name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+---------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|InoviceNo|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+---------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|   536365|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|   536365|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "online_retail.withColumn(\"InoviceNo\", col(\"InvoiceNo\").cast(\"string\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.7 Filtering Rows\n",
    "Filtering row is the process of the selecting the row/record that matches with the condition defined in an expression. The expression evaluates boolean value (i.e. true or false). We can filter either of the condition that matches the needs. In DataFrame we can filter row by:-   \n",
    "* Creating an expression of String\n",
    "* Building an expression using column manipulation\n",
    "\n",
    "`where()` and `filter()` methods are used to perform filter operation. The input parameter for both method are same. Since we are familiar with SQL syntax we'll use `where()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.where(\"InvoiceNo == 536365\").show(10)\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT * from employees where invoiceno = \"536365\" limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.filter(col(\"InvoiceNo\") == 536365).show(10)\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT * from online_retail where invoiceno = \"536365\" limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple filter can be applied by chaining each operation together. Spark internally handle all the ordering for optimization. We'll discuss more on next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple AND Operation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.where(\"UnitPrice > 3\").where(\"InvoiceNo == 536365\").show(10)\n",
    "                    \n",
    "# Equivalent SQL query\n",
    "# SELECT * from online_retail where unitprice > 3 and invoiceno = \"536365\" limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.8 Limit Rows\n",
    "When we want to display limited number of records for quick testing and validation. `limit()` method is used to select and display only certain amount of records. The parameter for this method is the number to display. It is similar to MySQL LIMIT, top in MSSQL, head etc. The default value is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.limit(8).show()\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT * from online_retail limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/10 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/10 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/10 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/10 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/10 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/10 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onlineretail_quick_validaition = online_retail.limit(20)#.show()\n",
    "onlineretail_quick_validaition.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.9 Distinct Rows\n",
    "Distinct is used to find the unique or deduplicate records in a DataFrame. `distinct()` method is used to get the unique records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|InvoiceNo|\n",
      "+---------+\n",
      "|   536365|\n",
      "|   536366|\n",
      "|   536367|\n",
      "|   536368|\n",
      "|   536369|\n",
      "|   536370|\n",
      "|   536371|\n",
      "|   536372|\n",
      "|   536373|\n",
      "|   536374|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.select(\"InvoiceNo\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_retail.select(\"InvoiceNo\").distinct().count()\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT count(distinct(invoiceno)) from online_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_retail.count()\n",
    "# Equivalent SQL query\n",
    "# SELECT count(invoiceno) from online_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_retail.select(\"InvoiceNo\", \"CustomerID\").distinct().count()\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT count(distinct(invoiceno, customerid)) from online_retail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.10 Sorting Rows\n",
    "Sorting is used to organize/arrange records based on ascending or desceding order. Sorting DataFrame can be done by using either methods:  \n",
    "* `sort()`\n",
    "* `orderBy()`\n",
    "\n",
    "Both the method accepts column expressions and strings. The default sorting is an ascending order. If we want to specify the sorting order explicitly for specified columns then we need to use `asc()` and `desc()` methods. Also, `asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last` can be applied to specify the null values location while sorting in DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|  InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------+---------+----------+--------------+\n",
      "|   536414|    22139|                null|      56|12/1/10 11:52|      0.0|      null|United Kingdom|\n",
      "|   536550|    85044|                null|       1|12/1/10 14:34|      0.0|      null|United Kingdom|\n",
      "|   536546|    22145|                null|       1|12/1/10 14:33|      0.0|      null|United Kingdom|\n",
      "|   536547|    37509|                null|       1|12/1/10 14:33|      0.0|      null|United Kingdom|\n",
      "|   536545|    21134|                null|       1|12/1/10 14:32|      0.0|      null|United Kingdom|\n",
      "|   536549|   85226A|                null|       1|12/1/10 14:34|      0.0|      null|United Kingdom|\n",
      "|   536390|    20668|DISCO BALL CHRIST...|     288|12/1/10 10:19|      0.1|     17511|United Kingdom|\n",
      "|   536409|    20668|DISCO BALL CHRIST...|      24|12/1/10 11:45|     0.12|     17908|United Kingdom|\n",
      "|   536409|    16238|PARTY TIME PENCIL...|      28|12/1/10 11:45|     0.21|     17908|United Kingdom|\n",
      "|   536408|    16237|SLEEPING CAT ERASERS|      30|12/1/10 11:41|     0.21|     14307|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.sort(\"UnitPrice\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/10 8:34|     1.65|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.orderBy(\"InvoiceNo\", \"UnitPrice\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/10 8:34|     1.65|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.orderBy(col(\"InvoiceNo\"), \"UnitPrice\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc, desc, expr\n",
    "online_retail.orderBy(expr(\"InvoiceNo desc\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|  InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+--------------------+--------+-------------+---------+----------+-------+\n",
      "|  C536548|    22245|HOOK, 1 HANGER ,M...|      -2|12/1/10 14:33|     0.85|     12472|Germany|\n",
      "|  C536548|    22892|SET OF SALT AND P...|      -7|12/1/10 14:33|     1.25|     12472|Germany|\n",
      "|  C536548|    20957|PORCELAIN HANGING...|      -1|12/1/10 14:33|     1.45|     12472|Germany|\n",
      "|  C536548|    22242|5 HOOK HANGER MAG...|      -5|12/1/10 14:33|     1.65|     12472|Germany|\n",
      "|  C536548|    22077|6 RIBBONS RUSTIC ...|      -6|12/1/10 14:33|     1.65|     12472|Germany|\n",
      "|  C536548|    22333|RETROSPOT PARTY B...|      -1|12/1/10 14:33|     1.65|     12472|Germany|\n",
      "|  C536548|    22631|CIRCUS PARADE LUN...|      -1|12/1/10 14:33|     1.95|     12472|Germany|\n",
      "|  C536548|    22244|3 HOOK HANGER MAG...|      -4|12/1/10 14:33|     1.95|     12472|Germany|\n",
      "|  C536548|    20914|SET/5 RED RETROSP...|      -1|12/1/10 14:33|     2.95|     12472|Germany|\n",
      "|  C536548|    21218|RED SPOTTY BISCUI...|      -3|12/1/10 14:33|     3.75|     12472|Germany|\n",
      "+---------+---------+--------------------+--------+-------------+---------+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail.orderBy(col(\"InvoiceNo\").desc(), col(\"UnitPrice\").asc()).show(10)\n",
    "\n",
    "# Equivalent SQL query\n",
    "# SELECT * from online_retail ORDER BY invoiceno desc, unitprice asc limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema for our data using DDL \n",
    "schema = \"`Id` INT,`First` STRING,`Last` STRING,`Url` STRING,`Published` STRING,`Hits` INT,`Campaigns` ARRAY<STRING>\"\n",
    "\n",
    "# create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "       [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "   # show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    "print()\n",
    "   # print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**  \n",
    "$^1$ https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html     \n",
    "$^2$ https://spark.apache.org/docs/latest/sql-reference.html#data-types   \n",
    "$^3$ https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader   \n",
    "$^4$ https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter  \n",
    "$^5$ https://spark.apache.org/downloads.html        \n",
    "$^6$ https://www.anaconda.com/distribution/#download-section)      \n",
    "$^7$ https://dev.mysql.com/downloads/mysql/            \n",
    "$^8$ https://portal.aws.amazon.com/billing/signup#/start    \n",
    "$^9$ https://community.cloud.databricks.com/login.html    \n",
    "$^{10}$ https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#    \n",
    "$^{11}$ https://training.databricks.com/visualapi.pdf      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
