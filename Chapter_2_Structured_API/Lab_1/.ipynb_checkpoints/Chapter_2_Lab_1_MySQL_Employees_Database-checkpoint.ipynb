{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Lab One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 2019-12-15   \n",
    "Author: Analytics Tensor   \n",
    "Description: The purpose is to become familiar with Spark's DataFrame data manipulation. In this lab, we will read data from MySQL database and load into Spark DataFrame using JDBC connection. The main objectives of this lab are:-     \n",
    "* Reading\n",
    "* Projection\n",
    "* Filtering\n",
    "* Sorting\n",
    "* Aggregation\n",
    "* Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading\n",
    "In Spark, reading the data from database can be through:-  \n",
    "* Load method\n",
    "* JDBC method\n",
    "\n",
    "Both method accomplished the same task. We will use both method but the efficient and secured way is to use JDBC method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Spark Lab One\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading data using load method.\n",
    "employees = spark.read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", \"jdbc:mysql://localhost:3306/employees\") \\\n",
    "  .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "  .option(\"dbtable\", \"employees\") \\\n",
    "  .option(\"user\", \"root\") \\\n",
    "  .option(\"password\", \"Mysql123#\") \\\n",
    "  .load()\n",
    "\n",
    "# Print Schema\n",
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display top 20 records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|    Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|    Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|    Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi|   Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|    Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|  Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya|   Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|       Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10011|1953-11-07|      Mary|      Sluis|     F|1990-01-22|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|\n",
      "| 10014|1956-02-12|     Berni|      Genin|     M|1987-03-11|\n",
      "| 10015|1959-08-19|  Guoxiang|  Nooteboom|     M|1987-07-02|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|\n",
      "| 10017|1958-07-06| Cristinel|  Bouloucos|     F|1993-08-03|\n",
      "| 10018|1954-06-19|  Kazuhide|       Peha|     F|1987-04-03|\n",
      "| 10019|1953-01-23|   Lillian|    Haddadi|     M|1999-04-30|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JDBC Method**   \n",
    "In JDBC method, we will pass the connection properties from config file. Python [configparser](https://docs.python.org/3/library/configparser.html) is used to read config file. While reading the SQL table, the connection properties is passed as dictionary through `properties` key/value in jdbc method. Using JDBC method, help to securely store the connection properties as well as repetatively use same connection properties multiple times for loading different table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read mysql database connection string from conf/db_properties.ini\n",
    "\n",
    "config_filename = 'conf/db_properties.ini'\n",
    "db_properties = {}\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_filename)\n",
    "db_prop = config['mysql']\n",
    "db_url = db_prop['url']\n",
    "db_properties['database'] = db_prop['database']\n",
    "db_properties['schema'] = db_prop['schema']\n",
    "db_properties['user'] = db_prop['user']\n",
    "db_properties['password'] = db_prop['password']\n",
    "db_properties['driver'] = db_prop['driver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Employee table using JDBC method\n",
    "\n",
    "employees = spark.read.jdbc(url = db_url, table = 'employees', properties = db_properties)\n",
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|    Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|    Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|    Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi|   Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|    Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|  Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya|   Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|       Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10011|1953-11-07|      Mary|      Sluis|     F|1990-01-22|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|\n",
      "| 10014|1956-02-12|     Berni|      Genin|     M|1987-03-11|\n",
      "| 10015|1959-08-19|  Guoxiang|  Nooteboom|     M|1987-07-02|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|\n",
      "| 10017|1958-07-06| Cristinel|  Bouloucos|     F|1993-08-03|\n",
      "| 10018|1954-06-19|  Kazuhide|       Peha|     F|1987-04-03|\n",
      "| 10019|1953-01-23|   Lillian|    Haddadi|     M|1999-04-30|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load all the remaining tables from employees databases**   \n",
    "List of tables: \n",
    "* current_dept_emp\n",
    "* departments\n",
    "* dept_emp\n",
    "* dept_emp_latest_date\n",
    "* dept_manager\n",
    "* employees\n",
    "* highest_salary_employee\n",
    "* salaries\n",
    "* titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load current_dept_emp\n",
    "current_dept_emp = spark.read.jdbc(url = db_url, table = 'current_dept_emp', properties = db_properties)\n",
    "\n",
    "# Load departments\n",
    "departments = spark.read.jdbc(url = db_url, table = 'departments', properties = db_properties)\n",
    "\n",
    "# Load dept_emp\n",
    "dept_emp = spark.read.jdbc(url = db_url, table = 'dept_emp', properties = db_properties)\n",
    "\n",
    "# Load dept_emp_latest_date\n",
    "dept_emp_latest_date = spark.read.jdbc(url = db_url, table = 'dept_emp_latest_date', properties = db_properties)\n",
    "\n",
    "# Load dept_manager\n",
    "dept_manager = spark.read.jdbc(url = db_url, table = 'dept_manager', properties = db_properties)\n",
    "\n",
    "# Load employees\n",
    "employees = spark.read.jdbc(url = db_url, table = 'employees', properties = db_properties)\n",
    "\n",
    "# Load highest_salary_employee\n",
    "highest_salary_employee = spark.read.jdbc(url = db_url, table = 'highest_salary_employee', properties = db_properties)\n",
    "\n",
    "# Load salaries\n",
    "salaries = spark.read.jdbc(url = db_url, table = 'salaries', properties = db_properties)\n",
    "\n",
    "# Load titles\n",
    "titles = spark.read.jdbc(url = db_url, table = 'titles', properties = db_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load table dynamically\n",
    "Let's assume we have more than 100 tables, in such case we can't load all the table manually. So, we programatically load all the table dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_dept_emp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c81c61846579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dynamically load all the table into respective dataframe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# List of table to be loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m table_list = [current_dept_emp, departments \\\n\u001b[0m\u001b[1;32m      4\u001b[0m                \u001b[0;34m,\u001b[0m\u001b[0mdept_emp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdept_emp_latest_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdept_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0memployees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighest_salary_employee\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'current_dept_emp' is not defined"
     ]
    }
   ],
   "source": [
    "# @todo\n",
    "\n",
    "# Dynamically load all the table into respective dataframe.\n",
    "# List of table to be loaded\n",
    "table_list = [current_dept_emp, departments \\\n",
    "               ,dept_emp, dept_emp_latest_date, dept_manager, \\\n",
    "               employees, highest_salary_employee, salaries, \\\n",
    "               titles]\n",
    "\n",
    "# create dictionary of employee_db\n",
    "#employees_db = {}\n",
    "#for table in table_list:\n",
    "    #print(table)\n",
    "    #print(\"\\nLoading Mysql {} table\".format(table))\n",
    "    #employees_db[table] = spark.read.jdbc(url = db_url, table = table, properties = db_properties)\n",
    "    #print(\"\\nSpark DataFrame created for {}\".format(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- emp_no: long (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- from_date_x: date (nullable = true)\n",
      " |-- to_date_x: date (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date_y: date (nullable = true)\n",
      " |-- to_date_y: date (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate on table list and print schema.\n",
    "table_list = [current_dept_emp,departments \\\n",
    "               ,dept_emp, dept_emp_latest_date, dept_manager, \\\n",
    "               employees, highest_salary_employee, salaries, \\\n",
    "               titles]\n",
    "for tables in table_list:\n",
    "    tables.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select top 10 records for all fields/attributes from employees DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "employees.select(col(\"*\")).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"*\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.selectExpr(\"*\").limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select top 5 first name, last name and gender from employees DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|gender|\n",
      "+----------+---------+------+\n",
      "|    Georgi|  Facello|     M|\n",
      "|   Bezalel|   Simmel|     F|\n",
      "|     Parto|  Bamford|     M|\n",
      "| Chirstian|  Koblick|     M|\n",
      "|   Kyoichi| Maliniak|     M|\n",
      "+----------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "employees.select(col(\"first_name\"), col(\"last_name\"), col(\"gender\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|gender|\n",
      "+----------+---------+------+\n",
      "|    Georgi|  Facello|     M|\n",
      "|   Bezalel|   Simmel|     F|\n",
      "|     Parto|  Bamford|     M|\n",
      "| Chirstian|  Koblick|     M|\n",
      "|   Kyoichi| Maliniak|     M|\n",
      "+----------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import column\n",
    "\n",
    "employees.select(column(\"first_name\"), column(\"last_name\"), column(\"gender\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|gender|\n",
      "+----------+---------+------+\n",
      "|    Georgi|  Facello|     M|\n",
      "|   Bezalel|   Simmel|     F|\n",
      "|     Parto|  Bamford|     M|\n",
      "| Chirstian|  Koblick|     M|\n",
      "|   Kyoichi| Maliniak|     M|\n",
      "+----------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "employees.select(expr(\"first_name\"), expr(\"last_name\"), column(\"gender\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|gender|\n",
      "+----------+---------+------+\n",
      "|    Georgi|  Facello|     M|\n",
      "|   Bezalel|   Simmel|     F|\n",
      "|     Parto|  Bamford|     M|\n",
      "| Chirstian|  Koblick|     M|\n",
      "|   Kyoichi| Maliniak|     M|\n",
      "+----------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"first_name\", \"last_name\", \"gender\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|gender|\n",
      "+----------+---------+------+\n",
      "|    Georgi|  Facello|     M|\n",
      "|   Bezalel|   Simmel|     F|\n",
      "|     Parto|  Bamford|     M|\n",
      "| Chirstian|  Koblick|     M|\n",
      "|   Kyoichi| Maliniak|     M|\n",
      "+----------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.selectExpr(\"first_name as first_name\", \"last_name as last_name\", \"gender\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|gender|\n",
      "+----------+---------+------+\n",
      "|    Georgi|  Facello|     M|\n",
      "|   Bezalel|   Simmel|     F|\n",
      "|     Parto|  Bamford|     M|\n",
      "| Chirstian|  Koblick|     M|\n",
      "|   Kyoichi| Maliniak|     M|\n",
      "+----------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(employees.first_name, employees.last_name, employees.gender).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Select all the fields and additional new field alias with \"Full Name\" from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+-----------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|        Full Name|\n",
      "+------+----------+----------+---------+------+----------+-----------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|   Georgi Facello|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|   Bezalel Simmel|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|    Parto Bamford|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|Chirstian Koblick|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| Kyoichi Maliniak|\n",
      "+------+----------+----------+---------+------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "employees.select(\"*\", concat_ws(\" \", \"first_name\",\"last_name\").alias(\"Full Name\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+-----------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|        Full Name|\n",
      "+------+----------+----------+---------+------+----------+-----------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|   Georgi Facello|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|   Bezalel Simmel|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|    Parto Bamford|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|Chirstian Koblick|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| Kyoichi Maliniak|\n",
      "+------+----------+----------+---------+------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "employees.withColumn(\"Full Name\", concat_ws(\" \", \"first_name\",\"last_name\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Select upper first name and lower last name of all employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|Upper FullName|Lower LastName|\n",
      "+--------------+--------------+\n",
      "|        GEORGI|       facello|\n",
      "|       BEZALEL|        simmel|\n",
      "|         PARTO|       bamford|\n",
      "|     CHIRSTIAN|       koblick|\n",
      "|       KYOICHI|      maliniak|\n",
      "+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "employees.select(upper(\"first_name\").alias(\"Upper FullName\"), lower(\"last_name\").alias(\"Lower LastName\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Drop gender from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_no: int, birth_date: date, first_name: string, last_name: string, hire_date: date]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.drop(\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame is immutable so the columns will not be dropped from original employees DataFrame. New DataFrame should be created or same DataFrame should be replaced with `drop` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = employees.drop(\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+----------+\n",
      "|emp_no|birth_date|first_name|last_name| hire_date|\n",
      "+------+----------+----------+---------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|1989-09-12|\n",
      "+------+----------+----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Drop first_name, last_name and gender from employee DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_1 = employees.drop(\"first_name\", \"last_name\", \"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n",
      "|emp_no|birth_date| hire_date|\n",
      "+------+----------+----------+\n",
      "| 10001|1953-09-02|1986-06-26|\n",
      "| 10002|1964-06-02|1985-11-21|\n",
      "| 10003|1959-12-03|1986-08-28|\n",
      "| 10004|1954-05-01|1986-12-01|\n",
      "| 10005|1955-01-21|1989-09-12|\n",
      "+------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Select new DF named \"emp_mod\" containing only 10000 employees information and adding new field emp_ID same as emp_no but with string type. Describe the schema to validate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "emp_mod = employees.withColumn(\"emp_ID\", col(\"emp_no\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- emp_ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_mod.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Select employee whose emp_no is 10001 from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.where(col(\"emp_no\") == 10001).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Select emp_no, first_name, last_name from employee whose emp_no is 10001 from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+\n",
      "|emp_no|first_name|last_name|\n",
      "+------+----------+---------+\n",
      "| 10001|    Georgi|  Facello|\n",
      "+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"emp_no\", \"first_name\", \"last_name\").where(col(\"emp_no\") == 10001).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+\n",
      "|emp_no|first_name|last_name|\n",
      "+------+----------+---------+\n",
      "| 10001|    Georgi|  Facello|\n",
      "+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.selectExpr(\"emp_no\", \"first_name\", \"last_name\").where(col(\"emp_no\") == 10001).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+\n",
      "|emp_no|first_name|last_name|\n",
      "+------+----------+---------+\n",
      "| 10001|    Georgi|  Facello|\n",
      "+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.where(col(\"emp_no\") == 10001).select(\"emp_no\",\"first_name\", \"last_name\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Select first_name, last_name, fullname from employee whose emp_no are 10001, 10020, 10050, 10070 from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n",
      "|emp_no|first_name| last_name|\n",
      "+------+----------+----------+\n",
      "| 10001|    Georgi|   Facello|\n",
      "| 10020|    Mayuko|   Warwick|\n",
      "| 10050|   Yinghua|    Dredge|\n",
      "| 10070|    Reuven|Garigliano|\n",
      "+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"emp_no\", \"first_name\", \"last_name\")\\\n",
    "    .where(col(\"emp_no\").isin(10001,10020,10050, 10070)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n",
      "|emp_no|first_name| last_name|\n",
      "+------+----------+----------+\n",
      "| 10001|    Georgi|   Facello|\n",
      "| 10020|    Mayuko|   Warwick|\n",
      "| 10050|   Yinghua|    Dredge|\n",
      "| 10070|    Reuven|Garigliano|\n",
      "+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"emp_no\", \"first_name\", \"last_name\")\\\n",
    "    .where(col(\"emp_no\").isin([10001,10020,10050, 10070])).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 11. Select all the employees whose first name start with 'S'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10022|1952-07-08|    Shahaf|   Famili|     M|1995-08-22|\n",
      "| 10024|1958-09-05|   Suzette|   Pettey|     F|1997-05-19|\n",
      "| 10053|1954-09-13|    Sanjiv| Zschoche|     F|1986-02-04|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"*\")\\\n",
    "    .where(col(\"first_name\").like('S%')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 11. Select all the employees whose first name starts with 'S' and ends with 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------+---------+------+----------+\n",
      "|emp_no|birth_date| first_name|last_name|gender| hire_date|\n",
      "+------+----------+-----------+---------+------+----------+\n",
      "| 10008|1958-02-19|     Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10093|1964-06-11|    Sailaja|  Desikan|     M|1996-11-05|\n",
      "| 10098|1961-09-23|Sreekrishna|Servieres|     F|1985-05-13|\n",
      "| 10235|1958-03-27|    Susanta| Roccetti|     F|1995-04-06|\n",
      "| 10259|1964-11-24|    Susanna|    Vesel|     M|1986-06-25|\n",
      "+------+----------+-----------+---------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"*\")\\\n",
    "    .where(col(\"first_name\").like('S%a')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 12. Select all the employees whose first name starts with 'S' and ends with 'a', and last name starts with 'K' and ends with 'a', and gender is Male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10328|1955-06-28| Serenella|Kawashima|     M|1994-01-16|\n",
      "| 20167|1960-05-10| Stamatina|   Kobara|     M|1985-04-04|\n",
      "| 22850|1960-05-06|   Shushma|  Kuzuoka|     M|1991-12-01|\n",
      "| 25313|1955-04-06|    Susuma|    Kroha|     M|1992-11-18|\n",
      "| 26339|1961-05-20|    Susuma|Kawashima|     M|1995-08-09|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"*\")\\\n",
    "    .where(col(\"first_name\").like('S%a'))\\\n",
    "    .where(col(\"last_name\").like('K%a'))\\\n",
    "    .where(col(\"gender\") == 'M').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 13. Select all the employees whose first name starts with 'J' or 'K'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10005|1955-01-21|   Kyoichi|   Maliniak|     M|1989-09-12|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|\n",
      "| 10018|1954-06-19|  Kazuhide|       Peha|     F|1987-04-03|\n",
      "| 10031|1959-01-27|   Karsten|     Joslin|     M|1991-09-01|\n",
      "| 10032|1960-08-09|     Jeong|    Reistad|     F|1990-06-20|\n",
      "| 10066|1952-11-13|      Kwee|   Schusler|     M|1986-02-26|\n",
      "| 10079|1961-10-05|   Kshitij|       Gils|     F|1986-03-27|\n",
      "| 10085|1962-11-07|   Kenroku|  Malabarba|     M|1994-04-09|\n",
      "| 10088|1954-02-25|  Jungsoon|   Syrzycki|     F|1988-09-02|\n",
      "| 10090|1961-05-30|    Kendra|    Hofting|     M|1986-03-14|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"*\")\\\n",
    "    .where(col(\"first_name\").like('J%') | (col(\"first_name\").like('K%')))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 14. Select all the employees whose first name starts with 'J' or last name starts with 'K'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------------+------+----------+\n",
      "|emp_no|birth_date|first_name|    last_name|gender| hire_date|\n",
      "+------+----------+----------+-------------+------+----------+\n",
      "| 10004|1954-05-01| Chirstian|      Koblick|     M|1986-12-01|\n",
      "| 10008|1958-02-19|    Saniya|     Kalloufi|     M|1994-09-15|\n",
      "| 10032|1960-08-09|     Jeong|      Reistad|     F|1990-06-20|\n",
      "| 10084|1960-05-25|     Tuval|     Kalloufi|     M|1995-12-15|\n",
      "| 10088|1954-02-25|  Jungsoon|     Syrzycki|     F|1988-09-02|\n",
      "| 10096|1954-09-16|    Jayson|      Mandell|     M|1990-01-14|\n",
      "| 10113|1963-11-13|    Jaewon|     Syrzycki|     M|1989-12-24|\n",
      "| 10152|1954-12-01|    Jaques|        Munro|     F|1986-01-27|\n",
      "| 10160|1953-10-18|  Debatosh|Khasidashvili|     M|1989-01-30|\n",
      "| 10164|1956-01-19|    Jagoda|    Braunmuhl|     M|1985-11-12|\n",
      "+------+----------+----------+-------------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"*\")\\\n",
    "    .where(col(\"first_name\").like('J%') | (col(\"last_name\").like('K%')))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 15. Select all the employees whose first name starts with 'J' and last name starts with 'K'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10213|1964-05-24|   Jackson|     Kakkad|     M|1992-11-06|\n",
      "| 10445|1957-01-10|   Junichi|   Kavanagh|     F|1987-11-04|\n",
      "| 10657|1958-03-09| Juichirou|Kitsuregawa|     M|1989-12-31|\n",
      "| 10660|1964-01-03|     Jouko|    Kolinko|     M|1988-08-12|\n",
      "| 11387|1954-07-18|  Jordanka|   Kalloufi|     M|1997-05-11|\n",
      "| 12581|1964-03-08|    Jaihie|    Kilgour|     M|1993-03-23|\n",
      "| 12622|1958-11-24|     Jiafu|     Kobara|     M|1993-11-05|\n",
      "| 12762|1964-04-27| Jaroslava|    Koblitz|     F|1992-09-25|\n",
      "| 12814|1952-06-10|    Jaques|    Kohling|     M|1995-04-29|\n",
      "| 13210|1953-06-19|   Jianhua|    Klassen|     M|1986-09-19|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"*\")\\\n",
    "    .where(col(\"first_name\").like('J%') & (col(\"last_name\").like('K%')))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Describe the summary of salaries DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            emp_no|            salary|\n",
      "+-------+------------------+------------------+\n",
      "|  count|           2844047|           2844047|\n",
      "|   mean|253057.44317657198|63810.744836143705|\n",
      "| stddev|161844.74133284207|16904.831259968036|\n",
      "|    min|             10001|             38623|\n",
      "|    max|            499999|            158220|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            emp_no|            salary|\n",
      "+-------+------------------+------------------+\n",
      "|  count|           2844047|           2844047|\n",
      "|   mean|253057.44317657198|63810.744836143705|\n",
      "| stddev|161844.74133284207|16904.831259968036|\n",
      "|    min|             10001|             38623|\n",
      "|    25%|             84857|             50510|\n",
      "|    50%|            249765|             61142|\n",
      "|    75%|            424894|             74189|\n",
      "|    max|            499999|            158220|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Print scheama of salaries DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Select top 10 records from salaries DF having lowest salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "|253406| 38623|2002-02-20|9999-01-01|\n",
      "| 49239| 38735|1996-09-17|1997-09-17|\n",
      "|281546| 38786|1996-11-13|1997-06-26|\n",
      "| 15830| 38812|2001-03-12|2002-03-12|\n",
      "| 64198| 38836|1989-10-20|1990-10-20|\n",
      "|475254| 38849|1993-06-04|1994-06-04|\n",
      "| 50419| 38850|1996-09-22|1997-09-22|\n",
      "| 34707| 38851|1990-10-03|1991-10-03|\n",
      "| 49239| 38859|1995-09-18|1996-09-17|\n",
      "|274049| 38864|1996-09-01|1997-09-01|\n",
      "+------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.sort(\"salary\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "|253406| 38623|2002-02-20|9999-01-01|\n",
      "| 49239| 38735|1996-09-17|1997-09-17|\n",
      "|281546| 38786|1996-11-13|1997-06-26|\n",
      "| 15830| 38812|2001-03-12|2002-03-12|\n",
      "| 64198| 38836|1989-10-20|1990-10-20|\n",
      "|475254| 38849|1993-06-04|1994-06-04|\n",
      "| 50419| 38850|1996-09-22|1997-09-22|\n",
      "| 34707| 38851|1990-10-03|1991-10-03|\n",
      "| 49239| 38859|1995-09-18|1996-09-17|\n",
      "|274049| 38864|1996-09-01|1997-09-01|\n",
      "+------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.orderBy(\"salary\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "|253406| 38623|2002-02-20|9999-01-01|\n",
      "| 49239| 38735|1996-09-17|1997-09-17|\n",
      "|281546| 38786|1996-11-13|1997-06-26|\n",
      "| 15830| 38812|2001-03-12|2002-03-12|\n",
      "| 64198| 38836|1989-10-20|1990-10-20|\n",
      "|475254| 38849|1993-06-04|1994-06-04|\n",
      "| 50419| 38850|1996-09-22|1997-09-22|\n",
      "| 34707| 38851|1990-10-03|1991-10-03|\n",
      "| 49239| 38859|1995-09-18|1996-09-17|\n",
      "|274049| 38864|1996-09-01|1997-09-01|\n",
      "+------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "\n",
    "salaries.orderBy(col(\"salary\").asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "|253406| 38623|2002-02-20|9999-01-01|\n",
      "| 49239| 38735|1996-09-17|1997-09-17|\n",
      "|281546| 38786|1996-11-13|1997-06-26|\n",
      "| 15830| 38812|2001-03-12|2002-03-12|\n",
      "| 64198| 38836|1989-10-20|1990-10-20|\n",
      "|475254| 38849|1993-06-04|1994-06-04|\n",
      "| 50419| 38850|1996-09-22|1997-09-22|\n",
      "| 34707| 38851|1990-10-03|1991-10-03|\n",
      "| 49239| 38859|1995-09-18|1996-09-17|\n",
      "|274049| 38864|1996-09-01|1997-09-01|\n",
      "+------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "\n",
    "salaries.orderBy(expr(\"salary as asc\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Select top 10 records from salaries DF having highest salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 43624|158220|2002-03-22|9999-01-01|\n",
      "| 43624|157821|2001-03-22|2002-03-22|\n",
      "|254466|156286|2001-08-04|9999-01-01|\n",
      "| 47978|155709|2002-07-14|9999-01-01|\n",
      "|253939|155513|2002-04-11|9999-01-01|\n",
      "|109334|155377|2000-02-12|2001-02-11|\n",
      "|109334|155190|2002-02-11|9999-01-01|\n",
      "|109334|154888|2001-02-11|2002-02-11|\n",
      "|109334|154885|1999-02-12|2000-02-12|\n",
      "| 80823|154459|2002-02-22|9999-01-01|\n",
      "+------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "salaries.orderBy(col(\"salary\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 43624|158220|2002-03-22|9999-01-01|\n",
      "| 43624|157821|2001-03-22|2002-03-22|\n",
      "|254466|156286|2001-08-04|9999-01-01|\n",
      "| 47978|155709|2002-07-14|9999-01-01|\n",
      "|253939|155513|2002-04-11|9999-01-01|\n",
      "|109334|155377|2000-02-12|2001-02-11|\n",
      "|109334|155190|2002-02-11|9999-01-01|\n",
      "|109334|154888|2001-02-11|2002-02-11|\n",
      "|109334|154885|1999-02-12|2000-02-12|\n",
      "| 80823|154459|2002-02-22|9999-01-01|\n",
      "+------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "salaries.orderBy(desc(\"salary\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Select top 10 emp_no and salary from salaries DF sorted by salary in ascending and  emp_no in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|emp_no|salary|\n",
      "+------+------+\n",
      "|253406| 38623|\n",
      "| 49239| 38735|\n",
      "|281546| 38786|\n",
      "| 15830| 38812|\n",
      "| 64198| 38836|\n",
      "|475254| 38849|\n",
      "| 50419| 38850|\n",
      "| 34707| 38851|\n",
      "| 49239| 38859|\n",
      "|274049| 38864|\n",
      "+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "salaries.orderBy(col(\"salary\").asc(), col(\"emp_no\").desc()).selectExpr(\"emp_no\", \"salary\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Select all records from departments sorted by dept_no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments.orderBy(\"dept_no\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Select all records from departments sorted by dept_name in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_no|         dept_name|\n",
      "+-------+------------------+\n",
      "|   d009|  Customer Service|\n",
      "|   d005|       Development|\n",
      "|   d002|           Finance|\n",
      "|   d003|   Human Resources|\n",
      "|   d001|         Marketing|\n",
      "|   d004|        Production|\n",
      "|   d006|Quality Management|\n",
      "|   d008|          Research|\n",
      "|   d007|             Sales|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments.orderBy(\"dept_name\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Select all records from departments sorted by dept_name in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_no|         dept_name|\n",
      "+-------+------------------+\n",
      "|   d007|             Sales|\n",
      "|   d008|          Research|\n",
      "|   d006|Quality Management|\n",
      "|   d004|        Production|\n",
      "|   d001|         Marketing|\n",
      "|   d003|   Human Resources|\n",
      "|   d002|           Finance|\n",
      "|   d005|       Development|\n",
      "|   d009|  Customer Service|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments.orderBy(desc(\"dept_name\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Count total employees from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  300024|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "employees.select(count(\"*\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In Spark `count(*)` will also count null values but specifying column name i.e. `count(first_name)` won't count null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(first_name)|\n",
      "+-----------------+\n",
      "|           300024|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "employees.select(count(\"first_name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. Count total distinct employees from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Distinct First Name|\n",
      "+-------------------+\n",
      "|               1275|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "employees.select(countDistinct(\"first_name\").alias(\"Distinct First Name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Distinct First Name|\n",
      "+-------------------+\n",
      "|               1275|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "employees.agg(countDistinct(\"first_name\").alias(\"Distinct First Name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While count the large dataset, an exact count might not be achieved. By using `approx_count_distinct()` method it gives approximate distinct count. The parameter for methods is column and rsd. RSD refers to realative standard error rate. The default is 0.05. If rsd is less than 0.01 then `countDistinct()` is efficient to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. Count approximate distinct count of emp_no from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Approx Count Distinct|\n",
      "+---------------------+\n",
      "|               276091|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "employees.select(approx_count_distinct(\"emp_no\", 0.1).alias(\"Approx Count Distinct\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. Find the first and last emp_no from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|First Record|Last Record|\n",
      "+------------+-----------+\n",
      "|       10001|     499999|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "employees.select(first(\"emp_no\").alias(\"First Record\"), last(\"emp_no\").alias(\"Last Record\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. Find minimum and maximum salary from salaries DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(salary)|max(salary)|\n",
      "+-----------+-----------+\n",
      "|      38623|     158220|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "salaries.select(min(\"salary\"), max(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. Find the sum of salary from salaries DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "| sum(salary)|\n",
      "+------------+\n",
      "|181480757419|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "salaries.select(sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. Find the distinct sum of salary from salaries DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|          7078688488|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "\n",
    "salaries.select(sumDistinct(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. Find the average salary from salaries DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           Average|\n",
      "+------------------+\n",
      "|63810.744836143705|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "salaries.select(avg(\"salary\").alias(\"Average\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. Find the mean salary from salaries DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       avg(salary)|\n",
      "+------------------+\n",
      "|63810.744836143705|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "salaries.select(mean(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. Find the count, sum and average salary from salaries DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+------------+\n",
      "|           Average|  Count|         Sum|\n",
      "+------------------+-------+------------+\n",
      "|63810.744836143705|2844047|181480757419|\n",
      "+------------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count, sum\n",
    "\n",
    "salaries.select(\\\n",
    "            avg(\"salary\").alias(\"Average\"),\\\n",
    "            count(\"salary\").alias(\"Count\"),\\\n",
    "            sum(\"salary\").alias(\"Sum\")\\\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33. Find the correlation, sample covariance and population covariance between id and emp_no from highest_salary_employee DF. Not a good column choice, but just for example. To get detail about correlation and covariance check out the link. https://towardsdatascience.com/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+---------------------+\n",
      "|   corr(id, emp_no)|covar_samp(id, emp_no)|covar_pop(id, emp_no)|\n",
      "+-------------------+----------------------+---------------------+\n",
      "|-0.4156756782643663|   -132.73809523809305|  -126.70454545454336|\n",
      "+-------------------+----------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "highest_salary_employee.select(corr(\"id\", \"emp_no\"), covar_samp(\"id\", \"emp_no\"),\\\n",
    "                         covar_pop(\"id\", \"emp_no\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34. Find total count of employee based on gender from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|gender| count|\n",
      "+------+------+\n",
      "|     F|120051|\n",
      "|     M|179973|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.groupBy(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. Find the total sum of salary based on gender from highest_salary_employee DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|gender|sum(salary)|\n",
      "+------+-----------+\n",
      "|     F|    1092209|\n",
      "|     M|    1112980|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highest_salary_employee.groupBy(\"gender\")\\\n",
    "        .agg(sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "36. Find the total count of salary based on gender from highest_salary_employee DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|gender|count(salary)|\n",
      "+------+-------------+\n",
      "|     F|           11|\n",
      "|     M|           11|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highest_salary_employee.groupBy(\"gender\")\\\n",
    "        .agg(count(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|gender|CountOfSalary|\n",
      "+------+-------------+\n",
      "|     F|           11|\n",
      "|     M|           11|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "highest_salary_employee.groupBy(\"gender\")\\\n",
    "        .agg(expr(\"count(salary) as CountOfSalary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Window Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing DataFrame into External Sources**\n",
    "\n",
    "Spark uses DataFrameWriter to write file into external sources. Once the dataset has been processed and transformed, the output of the DataFrame is stored in external filesytem, database or streaming application for reporting, analysis or machine learning consumption. We'll use several output source for writing final DataFrame result. [For more information](https://spark.apache.org/docs/latest/sql-data-sources.html).\n",
    "\n",
    "Let assume our final DataFrame is empDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|         full_name|\n",
      "+------+----------+----------+---------+------+----------+------------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|    Georgi Facello|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|    Bezalel Simmel|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|     Parto Bamford|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| Chirstian Koblick|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|  Kyoichi Maliniak|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|    Anneke Preusig|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10| Tzvetan Zielinski|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|   Saniya Kalloufi|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|       Sumant Peac|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|Duangkaew Piveteau|\n",
      "+------+----------+----------+---------+------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "empDF = employees.withColumn(\"full_name\", concat_ws(\" \", \"first_name\",\"last_name\"))\n",
    "empDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing DataFrame to CSV File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing DF to CSV File\n",
    "\n",
    "file_path = '/tmp/employee/spark_csv'\n",
    "\n",
    "empDF.write.format(\"csv\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"path\", file_path)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing DataFrame to Avro File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o89.save.\n: org.apache.spark.sql.AnalysisException: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:723)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:833)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:252)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5e7434a5a70d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mempDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/apache-spark/spark-3.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;"
     ]
    }
   ],
   "source": [
    "# Writing DF to Avro File\n",
    "\n",
    "file_path = '/tmp/employee/spark_avro'\n",
    "\n",
    "empDF.write.format(\"avro\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"path\", file_path)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing DataFrame to ORC File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing DF to ORC File\n",
    "\n",
    "file_path = '/tmp/employee/spark_orc'\n",
    "\n",
    "empDF.write.format(\"orc\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"path\", file_path)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing DataFrame to Parquet File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing DF to Parquet File\n",
    "\n",
    "file_path = '/tmp/employee/spark_parquet'\n",
    "\n",
    "empDF.write.format(\"parquet\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"path\", file_path)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameReader' object has no attribute 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-98c4978f700d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/tmp/employee/spark_parquet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0memp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameReader' object has no attribute 'mode'"
     ]
    }
   ],
   "source": [
    "# Writing DF to Parquet File\n",
    "\n",
    "file_path = '/tmp/employee/spark_parquet'\n",
    "\n",
    "emp = spark.read.format(\"parquet\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"path\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing DataFrame to JSON File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing DF to JSON File\n",
    "\n",
    "file_path = '/tmp/employee/spark_json'\n",
    "\n",
    "empDF.write.format(\"json\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"path\", file_path)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing DataFrame to Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+\n",
      "|emp_no|birth_date|         full_name|\n",
      "+------+----------+------------------+\n",
      "| 10001|1953-09-02|    Georgi Facello|\n",
      "| 10002|1964-06-02|    Bezalel Simmel|\n",
      "| 10003|1959-12-03|     Parto Bamford|\n",
      "| 10004|1954-05-01| Chirstian Koblick|\n",
      "| 10005|1955-01-21|  Kyoichi Maliniak|\n",
      "| 10006|1953-04-20|    Anneke Preusig|\n",
      "| 10007|1957-05-23| Tzvetan Zielinski|\n",
      "| 10008|1958-02-19|   Saniya Kalloufi|\n",
      "| 10009|1952-04-19|       Sumant Peac|\n",
      "| 10010|1963-06-01|Duangkaew Piveteau|\n",
      "+------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  datetime import datetime\n",
    "#from pyspark.sql.functions import cast\n",
    "\n",
    "# get current date\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "table_name = \"analytics_tensor.spark_employees_etl\"  # + current_date\n",
    "\n",
    "\n",
    "# Convert date type for birth_date and hire_date to string.\n",
    "empDF_final = empDF.selectExpr(\"emp_no\", \"cast(birth_date as string)\",\\\n",
    "                \"full_name\")\n",
    "\n",
    "empDF_final.show(10)\n",
    "\n",
    "# write to MySQL using existing connections \n",
    "empDF_final.write.jdbc(url = db_url, table = table_name, properties = db_properties).mode('append').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usecase-1**  \n",
    "\n",
    "Reading: Load employees table from MySQL database into Spark. \n",
    "\n",
    "Requirement: The report must include following information:   \n",
    "1. Calculate the employees current age.\n",
    "2. Calculate total number of years worked by employee.\n",
    "3. Find the age of employee when they are hired at the company.\n",
    "4. Show employee birth year.\n",
    "5. Create employee abbreviated name that contains 2 first character from last name and all character from first in lower case.\n",
    "6. Reverse employee number.\n",
    "\n",
    "Ordering: Sort the data by employee abbreviated name in ascending order.\n",
    "\n",
    "The ordinality, attributes name and type is defined below:\n",
    "6. id (type: integer)\n",
    "5. user (type: string)\n",
    "1. age (type: integer)\n",
    "4. birth_year (type: integer)\n",
    "3. start_age (type: integer)\n",
    "2. year_worked (type: integer)\n",
    "\n",
    "Output 1: The output file must be written in each file type shown below. The directory structure is defined below:   \n",
    "BASE_DIR = `/opt/spark_processing/data/employee`   \n",
    "FILE_TYPE = { `csv | parquet | orc | avro | json` }   \n",
    "DATA = { `employee` }   \n",
    "CURRENT_DATE = `now()`   \n",
    "FILE_NAME = `spark_$DATA_$CURRENT_DATE`   \n",
    "LOCATION = `$BASE_DIR/$FILE_NAME/$DATA/$FILE_NAME`     \n",
    " \n",
    "Output 2: Mysql table ( database name: analytics_tensor, table name: spark_employees_current_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
