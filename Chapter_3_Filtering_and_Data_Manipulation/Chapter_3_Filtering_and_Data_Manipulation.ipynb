{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Filtering and Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**3.1 Filtering**](#3.1-Filtering)   \n",
    "[**3.1.1 Filtering Column**](#3.1.1-Filtering-Column)   \n",
    "[**3.1.2 Filtering Row**](#3.1.2-Filtering-Row)   \n",
    "[**3.1.3 Filter single column**](#3.1.3-Filter-single-column)   \n",
    "[**3.1.4 Filter multiple column with AND operator**](#3.1.4-Filter-multiple-column-with-AND-operator)   \n",
    "[**3.1.5 Filter multiple column with OR operator**](#3.1.5-Filter-multiple-column-with-OR-operator)   \n",
    "[**3.1.6 Filter with Boolean expression**](#3.1.6-Filter-with-Boolean-expression)   \n",
    "[**3.2 PySpark SQL Module**](#3.2-PySpark-SQL-Module)   \n",
    "[**3.3 Numeric Type Manipulation**](#3.3-Numeric-Type-Manipulation)   \n",
    "[**3.4 String Type Manipulation**](#3.4-String-Type-Manipulation)   \n",
    "[**3.5 Date and Timestamp Type Manipulation**](#3.5-Date-and-Timestamp-Type-Manipulation)   \n",
    "[**3.6 Complex Type Manipulation**](#3.6-Complex-Type-Manipulation)   \n",
    "[**3.6.1 Arrays Type**](#3.6.1-Arrays-Type)   \n",
    "[**3.6.2 Maps Type**](#3.6.2-Maps-Type)   \n",
    "[**3.6.3 Structs Type**](#3.6.3-Structs-Type)   \n",
    "[**3.7 Handling Nulls**](#3.7-Handling-Nulls)   \n",
    "[**3.7.1 Dropping Null Values**](#3.7.1-Dropping-Null-Values)   \n",
    "[**3.7.2 Filling Null Values**](#3.7.2-Filling-Null-Values)   \n",
    "[**3.7.3 Filtering Null Values**](#3.7.3-Filtering-Null-Values)   \n",
    "[**3.9 User Defined Functions**](#3.8-User-Defined-Functions)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Filtering\n",
    "**Filtering**: Filtering is the process of subsetting data for analysis and reporting. Filter can be applied both on `rows` and `columns`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Filtering Column\n",
    "Filtering column is the process of reducing i.e. dropping or removing columns/attributes from originial DataFrame. Filtering column are used to:-      \n",
    "* remove sensitive fields from data.\n",
    "* remove less important fields.\n",
    "* remove temporary fields added during data transformation or validation.\n",
    "* reduce data size for fast processing and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read mysql database connection string from conf/db_properties.ini\n",
    "\n",
    "config_filename = '../Chapter_2_Structured_API/Lab_1/conf/db_properties.ini'\n",
    "db_properties = {}\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_filename)\n",
    "db_prop = config['mysql']\n",
    "db_url = db_prop['url']\n",
    "db_properties['database'] = db_prop['database']\n",
    "db_properties['schema'] = db_prop['schema']\n",
    "db_properties['user'] = db_prop['user']\n",
    "db_properties['password'] = db_prop['password']\n",
    "db_properties['serverTimezone'] = db_prop['serverTimezone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Chapter 3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load current_dept_emp\n",
    "current_dept_emp = spark.read.jdbc(url = db_url, table = 'current_dept_emp', properties = db_properties)\n",
    "\n",
    "# Load departments\n",
    "departments = spark.read.jdbc(url = db_url, table = 'departments', properties = db_properties)\n",
    "\n",
    "# Load dept_emp\n",
    "dept_emp = spark.read.jdbc(url = db_url, table = 'dept_emp', properties = db_properties)\n",
    "\n",
    "# Load dept_emp_latest_date\n",
    "dept_emp_latest_date = spark.read.jdbc(url = db_url, table = 'dept_emp_latest_date', properties = db_properties)\n",
    "\n",
    "# Load dept_manager\n",
    "dept_manager = spark.read.jdbc(url = db_url, table = 'dept_manager', properties = db_properties)\n",
    "\n",
    "# Load employees\n",
    "employees = spark.read.jdbc(url = db_url, table = 'employees', properties = db_properties)\n",
    "\n",
    "# Load highest_salary_employee\n",
    "highest_salary_employee = spark.read.jdbc(url = db_url, table = 'highest_salary_employee', properties = db_properties)\n",
    "\n",
    "# Load salaries\n",
    "salaries = spark.read.jdbc(url = db_url, table = 'salaries', properties = db_properties)\n",
    "\n",
    "# Load titles\n",
    "titles = spark.read.jdbc(url = db_url, table = 'titles', properties = db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Single Column**\n",
    "\n",
    "`drop()` method is used to drop the columns from DataFrame.\n",
    "\n",
    "To drop single column pass the column name to `drop()` method. The example below shows dropping emp_no from employees DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+----------+----------+---------+------+----------+\n",
      "|birth_date|first_name|last_name|gender|hire_date |\n",
      "+----------+----------+---------+------+----------+\n",
      "|1953-09-02|Georgi    |Facello  |M     |1986-06-26|\n",
      "|1964-06-02|Bezalel   |Simmel   |F     |1985-11-21|\n",
      "|1959-12-03|Parto     |Bamford  |M     |1986-08-28|\n",
      "|1954-05-01|Chirstian |Koblick  |M     |1986-12-01|\n",
      "|1955-01-21|Kyoichi   |Maliniak |M     |1989-09-12|\n",
      "|1953-04-20|Anneke    |Preusig  |F     |1989-06-02|\n",
      "|1957-05-23|Tzvetan   |Zielinski|F     |1989-02-10|\n",
      "|1958-02-19|Saniya    |Kalloufi |M     |1994-09-15|\n",
      "|1952-04-19|Sumant    |Peac     |F     |1985-02-18|\n",
      "|1963-06-01|Duangkaew |Piveteau |F     |1989-08-24|\n",
      "+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop emp_no from employees DF and store new value into emp_tmpDF.\n",
    "\n",
    "emp_tmpDF = employees.drop(\"emp_no\")\n",
    "emp_tmpDF.printSchema()\n",
    "emp_tmpDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Multiple Column**\n",
    "\n",
    "To drop multipe column pass the column names to `drop()` method. The example below shows dropping emp_no, birth_date, gender and hire_date from employees DF. The argument to drop() method can be either string of column names or list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n",
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|Georgi    |Facello  |\n",
      "|Bezalel   |Simmel   |\n",
      "|Parto     |Bamford  |\n",
      "|Chirstian |Koblick  |\n",
      "|Kyoichi   |Maliniak |\n",
      "|Anneke    |Preusig  |\n",
      "|Tzvetan   |Zielinski|\n",
      "|Saniya    |Kalloufi |\n",
      "|Sumant    |Peac     |\n",
      "|Duangkaew |Piveteau |\n",
      "+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Drop emp_no, birth_date, gender and hire_date from employees DF and store new value into emp_tmpDF.\n",
    "\n",
    "emp_tmpDF = employees.drop(\"emp_no\", \"birth_date\", \"gender\" ,\"hire_date\")\n",
    "emp_tmpDF.printSchema()\n",
    "emp_tmpDF.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+----------+---------+----------+\n",
      "|first_name|last_name|hire_date |\n",
      "+----------+---------+----------+\n",
      "|Georgi    |Facello  |1986-06-26|\n",
      "|Bezalel   |Simmel   |1985-11-21|\n",
      "|Parto     |Bamford  |1986-08-28|\n",
      "|Chirstian |Koblick  |1986-12-01|\n",
      "|Kyoichi   |Maliniak |1989-09-12|\n",
      "|Anneke    |Preusig  |1989-06-02|\n",
      "|Tzvetan   |Zielinski|1989-02-10|\n",
      "|Saniya    |Kalloufi |1994-09-15|\n",
      "|Sumant    |Peac     |1985-02-18|\n",
      "|Duangkaew |Piveteau |1989-08-24|\n",
      "+----------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Drop emp_no, birth_date, gender and hire_date from employees DF and store new value into emp_tmpDF.\n",
    "\n",
    "column_list = ['emp_no', 'birth_date', 'gender']\n",
    "emp_tmpDF = employees.drop(*column_list)\n",
    "emp_tmpDF.printSchema()\n",
    "emp_tmpDF.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emp_no', 'birth_date', 'gender']\n"
     ]
    }
   ],
   "source": [
    "print(column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Filtering Row\n",
    "Filtering row or record is the process of filtering unwanted record(s) from DataFrame. Filtering row are used to:-   \n",
    "* eliminate errorneous record.\n",
    "* eliminate duplicate record.\n",
    "* eliminate null or empty record.\n",
    "* select record based on certain business rule or logic.\n",
    "* select record for particular groups of interest.\n",
    "* select record for particular period of time.\n",
    "\n",
    "Records are always filtered based on the boolean `true` or `false` value evaluated either from the single `column` or `column expressions` value. The column expression can be constructed from multiple columns containing assignment, arithmetic, bitwise, logical, comparison etc. operators combined with various list of function available in `pyspark.sql.functions`. The logical statements built from these expression will result boolean value where the record are filtered based on it.   \n",
    "\n",
    "`where()` and `filter()` method are used to filter records. The argument to these method are `column name`, `column expression` or `boolean statements`. Both method performs similiar task. We'll use `where()` in our entire session since it is easier to remember and similar to SQL clause.   \n",
    "\n",
    "Boolean expressions uses combination of boolean operation as shown in table below. We can also use Boolean column to filter the values from DataFrame. Boolean column is constructed using Boolean expression result in DataFrame. The example shown below in *Filter with Boolean column* section.  \n",
    "\n",
    "Table 3.1.2 (a) Boolean Operation\n",
    "\n",
    "| Operator | Description |\n",
    "| --------- | ----------- |\n",
    "| `&` | And operation |\n",
    "| `\\|` | Or operation |\n",
    "| `!` | Not operation |\n",
    "\n",
    "\n",
    "**Note**: `and` filter is always chained together sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Filter single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name    |gender|hire_date |\n",
      "+------+----------+----------+-------------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello      |M     |1986-06-26|\n",
      "|10909 |1954-11-11|Georgi    |Atchley      |M     |1985-04-21|\n",
      "|11029 |1962-07-12|Georgi    |Itzfeldt     |M     |1992-12-27|\n",
      "|11430 |1957-01-23|Georgi    |Klassen      |M     |1996-02-27|\n",
      "|12157 |1960-03-30|Georgi    |Barinka      |M     |1985-06-04|\n",
      "|15220 |1957-08-03|Georgi    |Panienski    |F     |1995-07-23|\n",
      "|15660 |1956-01-13|Georgi    |Hartvigsen   |M     |1994-10-13|\n",
      "|15689 |1962-09-14|Georgi    |Capobianchi  |M     |1995-03-11|\n",
      "|15843 |1958-07-15|Georgi    |Varley       |M     |1987-04-14|\n",
      "|16672 |1955-04-25|Georgi    |Peris        |M     |1986-03-13|\n",
      "|16939 |1956-02-10|Georgi    |Ranon        |F     |1988-05-31|\n",
      "|18453 |1959-01-10|Georgi    |Maksimenko   |M     |1990-02-21|\n",
      "|19579 |1953-02-09|Georgi    |Impagliazzo  |F     |1993-01-17|\n",
      "|19903 |1960-10-11|Georgi    |Delgrossi    |M     |1987-08-13|\n",
      "|20095 |1962-11-30|Georgi    |Highland     |M     |1988-09-11|\n",
      "|20822 |1955-10-10|Georgi    |Rullman      |F     |1990-01-23|\n",
      "|21369 |1962-09-08|Georgi    |Kadhim       |M     |1988-08-10|\n",
      "|22787 |1953-02-10|Georgi    |Hebert       |F     |1993-05-13|\n",
      "|29258 |1964-01-23|Georgi    |Radhakrishnan|F     |1987-11-10|\n",
      "|29833 |1952-06-24|Georgi    |Kawashima    |M     |1994-03-05|\n",
      "+------+----------+----------+-------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter based on single column\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "employees.where(col(\"first_name\") == \"Georgi\")\\\n",
    "         .select(\"*\")\\\n",
    "         .show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------+\n",
      "|first_name|  last_name|emp_no|\n",
      "+----------+-----------+------+\n",
      "|    Georgi|    Facello| 10001|\n",
      "|    Georgi|    Atchley| 10909|\n",
      "|    Georgi|   Itzfeldt| 11029|\n",
      "|    Georgi|    Klassen| 11430|\n",
      "|    Georgi|    Barinka| 12157|\n",
      "|    Georgi|  Panienski| 15220|\n",
      "|    Georgi| Hartvigsen| 15660|\n",
      "|    Georgi|Capobianchi| 15689|\n",
      "|    Georgi|     Varley| 15843|\n",
      "|    Georgi|      Peris| 16672|\n",
      "+----------+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterGeorgi = employees.select(\"first_name\", \"last_name\", \"emp_no\")\\\n",
    "    .where(col(\"first_name\") == \"Georgi\")\n",
    "filterGeorgi.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name  |gender|hire_date |\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello    |M     |1986-06-26|\n",
      "|10909 |1954-11-11|Georgi    |Atchley    |M     |1985-04-21|\n",
      "|11029 |1962-07-12|Georgi    |Itzfeldt   |M     |1992-12-27|\n",
      "|11430 |1957-01-23|Georgi    |Klassen    |M     |1996-02-27|\n",
      "|12157 |1960-03-30|Georgi    |Barinka    |M     |1985-06-04|\n",
      "|15220 |1957-08-03|Georgi    |Panienski  |F     |1995-07-23|\n",
      "|15660 |1956-01-13|Georgi    |Hartvigsen |M     |1994-10-13|\n",
      "|15689 |1962-09-14|Georgi    |Capobianchi|M     |1995-03-11|\n",
      "|15843 |1958-07-15|Georgi    |Varley     |M     |1987-04-14|\n",
      "|16672 |1955-04-25|Georgi    |Peris      |M     |1986-03-13|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter based on single column\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "employees.where(\"first_name == 'Georgi'\")\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Filter multiple column with AND operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+----------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |year(birth_date)|\n",
      "+------+----------+----------+---------+------+----------+----------------+\n",
      "|10001 |1953-09-02|Georgi    |Facello  |M     |1986-06-26|1953            |\n",
      "|55649 |1956-01-23|Georgi    |Facello  |M     |1988-05-04|1956            |\n",
      "+------+----------+----------+---------+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "employees.where(\"first_name == 'Georgi'\")\\\n",
    "         .where(\"last_name == 'Facello'\") \\\n",
    "         .where(year(col(\"birth_date\")) > 1952)\\\n",
    "         .select(\"*\",year(col(\"birth_date\")))\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello  |M     |1986-06-26|\n",
      "|55649 |1956-01-23|Georgi    |Facello  |M     |1988-05-04|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#F.when(col(\"col-1\")>0.0) & (col(\"col-2\")>0.0), 1).otherwise(0)\n",
    "\n",
    "employees.where((col(\"first_name\") == 'Georgi') & (col(\"last_name\") == 'Facello'))\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello  |M     |1986-06-26|\n",
      "|55649 |1956-01-23|Georgi    |Facello  |M     |1988-05-04|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstName = col(\"first_name\") == \"Georgi\"\n",
    "lastName = col(\"last_name\") == \"Facello\"\n",
    "\n",
    "employees.where(firstName & lastName)\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5 Filter multiple column with OR operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+----------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name |gender|hire_date |\n",
      "+------+----------+----------+----------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello   |M     |1986-06-26|\n",
      "|10327 |1954-04-01|Roded     |Facello   |M     |1987-09-18|\n",
      "|10909 |1954-11-11|Georgi    |Atchley   |M     |1985-04-21|\n",
      "|11029 |1962-07-12|Georgi    |Itzfeldt  |M     |1992-12-27|\n",
      "|11430 |1957-01-23|Georgi    |Klassen   |M     |1996-02-27|\n",
      "|12157 |1960-03-30|Georgi    |Barinka   |M     |1985-06-04|\n",
      "|12751 |1964-07-06|Nahum     |Facello   |M     |1995-01-09|\n",
      "|15220 |1957-08-03|Georgi    |Panienski |F     |1995-07-23|\n",
      "|15346 |1959-09-26|Kirk      |Facello   |F     |1991-12-07|\n",
      "|15660 |1956-01-13|Georgi    |Hartvigsen|M     |1994-10-13|\n",
      "+------+----------+----------+----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.where((col(\"first_name\") == 'Georgi') | (col(\"last_name\") == 'Facello'))\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10327|1954-04-01|     Roded|  Facello|     M|1987-09-18|\n",
      "| 12751|1964-07-06|     Nahum|  Facello|     M|1995-01-09|\n",
      "| 15346|1959-09-26|      Kirk|  Facello|     F|1991-12-07|\n",
      "| 15685|1958-07-12|   Kasturi|  Facello|     M|1992-03-13|\n",
      "| 18686|1962-02-23| Kwangyoen|  Facello|     F|1985-05-02|\n",
      "| 19041|1957-05-29|    Billur|  Facello|     F|1992-08-03|\n",
      "| 21947|1954-06-18|   Taisook|  Facello|     F|1991-07-30|\n",
      "| 23938|1955-07-11|     Nahum|  Facello|     M|1985-09-15|\n",
      "| 24774|1956-09-23|       Uno|  Facello|     F|1989-11-09|\n",
      "| 24806|1959-09-30|  Charmane|  Facello|     F|1989-03-17|\n",
      "| 25955|1962-10-09| Christoph|  Facello|     M|1989-03-24|\n",
      "| 27732|1955-06-04|  Girolamo|  Facello|     M|1986-06-30|\n",
      "| 30320|1953-12-21|  Kristine|  Facello|     F|1990-06-10|\n",
      "| 31107|1962-10-13|       Fai|  Facello|     M|1994-07-11|\n",
      "| 32138|1955-04-13|    Anyuan|  Facello|     F|1988-10-04|\n",
      "| 32580|1953-04-25|      Kirk|  Facello|     M|1988-05-31|\n",
      "| 36371|1961-05-29| Cristinel|  Facello|     M|1986-10-14|\n",
      "| 37261|1957-11-06|    Zijian|  Facello|     F|1986-06-30|\n",
      "| 41667|1953-12-23|     Barna|  Facello|     F|1985-09-13|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "genderFilter = col(\"gender\") == 'M'\n",
    "ageFilter = year(employees.birth_date) <= 1969\n",
    "\n",
    "employees.where(employees.last_name.isin(\"Facello\"))\\\n",
    "         .where(genderFilter | ageFilter)\\\n",
    "         .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.6 Filter with Boolean expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------+------+----------+-----------+\n",
      "|first_name|  last_name|gender|emp_no|birth_date|male50above|\n",
      "+----------+-----------+------+------+----------+-----------+\n",
      "|    Georgi|    Facello|     M| 10001|1953-09-02|       true|\n",
      "|     Parto|    Bamford|     M| 10003|1959-12-03|       true|\n",
      "| Chirstian|    Koblick|     M| 10004|1954-05-01|       true|\n",
      "|   Kyoichi|   Maliniak|     M| 10005|1955-01-21|       true|\n",
      "|    Saniya|   Kalloufi|     M| 10008|1958-02-19|       true|\n",
      "|  Patricio|  Bridgland|     M| 10012|1960-10-04|       true|\n",
      "| Eberhardt|     Terkki|     M| 10013|1963-06-07|       true|\n",
      "|     Berni|      Genin|     M| 10014|1956-02-12|       true|\n",
      "|  Guoxiang|  Nooteboom|     M| 10015|1959-08-19|       true|\n",
      "|  Kazuhito|Cappelletti|     M| 10016|1961-05-02|       true|\n",
      "+----------+-----------+------+------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get male employee's first_name, last_name, gender, emp_no above 50 year old from employees DF\n",
    "\n",
    "from pyspark.sql.functions import datediff, current_date, col\n",
    "\n",
    "\n",
    "genderFilter = col(\"gender\") == 'M'\n",
    "ageFilter = datediff(current_date(), col(\"birth_date\"))/365 > 50\n",
    "\n",
    "employees.withColumn(\"male50above\", genderFilter & ageFilter)\\\n",
    "         .where(\"male50above\")\\\n",
    "         .select(\"first_name\", \"last_name\", \"gender\", \"emp_no\", \"birth_date\", \"male50above\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 PySpark SQL Module\n",
    "\n",
    "Always keep the image below on your memory and link in your browser bookmark. This helps you to solve problem faster by finding all the resources easily which is one-stop shop for all [Spark Python API Documents](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html).  \n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "\n",
    "\n",
    "![Pysark SQL Module Image](spark_sql_module.png)\n",
    "\n",
    "\n",
    "\n",
    "If you want to deep dive into Scala then use the link below (Optional for this Course). Copy and paste if link doesn't work.\n",
    "\n",
    "* **DataSet Functions**: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset    \n",
    "* **DataFrame and SQL Functions**: http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html\n",
    "* **DataFrameStatFunctions**: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions\n",
    "* **DataFrameNaFunctions**: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions     \n",
    "* **Column Methods**: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Numeric Type Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|\n",
      "+------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+-----------------------+\n",
      "|emp_no|salary| from_date|   to_date|salary_with_5%_increase|\n",
      "+------+------+----------+----------+-----------------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|               63122.85|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|                65207.1|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|                69377.7|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|                69925.8|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|               70309.05|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|                74598.3|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|               78049.65|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|                79050.3|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|                79793.7|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|                80728.2|\n",
      "+------+------+----------+----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.select(\"*\", ((col(\"salary\") * 0.05) + col(\"salary\")).alias(\"salary_with_5%_increase\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+------------+\n",
      "|emp_no|salary| from_date|   to_date|bonus_salary|\n",
      "+------+------+----------+----------+------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|    63622.85|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|    65707.10|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|    69877.70|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|    70425.80|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|    70809.05|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|    75098.30|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|    78549.65|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|    79550.30|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|    80293.70|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|    81228.20|\n",
      "+------+------+----------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate new salary by increasing 5% and adding $500 commission with new fields 'bonus_salary' in salaries DF.\n",
    "\n",
    "salaries.selectExpr(\"*\", \"salary * 0.05 + 500 + salary as bonus_salary\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+------------+\n",
      "|emp_no|salary| from_date|   to_date|bonus_salary|\n",
      "+------+------+----------+----------+------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|    63622.85|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|     65707.1|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|     69877.7|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|     70425.8|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|    70809.05|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|     75098.3|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|    78549.65|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|     79550.3|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|     80293.7|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|     81228.2|\n",
      "+------+------+----------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate new salary by increasing 5% and adding $500 commission with new fields 'bonus_salary' in salaries DF.\n",
    "\n",
    "bonusSalary = (col(\"salary\") * 0.05) + 500 + col(\"salary\")\n",
    "salaries.select(\"*\", bonusSalary.alias(\"bonus_salary\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|age_in_months|\n",
      "+-------------+\n",
      "|        806.0|\n",
      "|        677.0|\n",
      "|        731.0|\n",
      "|        798.0|\n",
      "|        789.0|\n",
      "|        810.0|\n",
      "|        761.0|\n",
      "|        752.0|\n",
      "|        822.0|\n",
      "|        689.0|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the rounded age of employees calculated in month.\n",
    "\n",
    "from pyspark.sql.functions import months_between, round\n",
    "\n",
    "employees.selectExpr(\"round(months_between(current_date(), birth_date)) as age_in_months\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+------------+\n",
      "|salary|bonus_salary|ceil_salary|floor_salary|\n",
      "+------+------------+-----------+------------+\n",
      "| 60117|    63122.85|      63123|       63122|\n",
      "| 62102|     65207.1|      65208|       65207|\n",
      "| 66074|     69377.7|      69378|       69377|\n",
      "| 66596|     69925.8|      69926|       69925|\n",
      "| 66961|    70309.05|      70310|       70309|\n",
      "| 71046|     74598.3|      74599|       74598|\n",
      "| 74333|    78049.65|      78050|       78049|\n",
      "| 75286|     79050.3|      79051|       79050|\n",
      "| 75994|     79793.7|      79794|       79793|\n",
      "| 76884|     80728.2|      80729|       80728|\n",
      "+------+------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcuate the floor and ceiling value for bonus_salary.\n",
    "\n",
    "from pyspark.sql.functions import ceil, floor\n",
    "\n",
    "bonus_salary = col(\"salary\") + col(\"salary\") * 0.05\n",
    "salaries.select(\"salary\", bonus_salary.alias(\"bonus_salary\"), ceil(bonus_salary).alias(\"ceil_salary\")\\\n",
    "                , floor(bonus_salary).alias(\"floor_salary\")).show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 String Type Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----------------+\n",
      "|first_name|last_name|user_name|length(last_name)|\n",
      "+----------+---------+---------+-----------------+\n",
      "|    Georgi|  Facello|     gefa|                7|\n",
      "|   Bezalel|   Simmel|     besi|                6|\n",
      "|     Parto|  Bamford|     paba|                7|\n",
      "| Chirstian|  Koblick|     chko|                7|\n",
      "|   Kyoichi| Maliniak|     kyma|                8|\n",
      "|    Anneke|  Preusig|     anpr|                7|\n",
      "|   Tzvetan|Zielinski|     tzzi|                9|\n",
      "|    Saniya| Kalloufi|     saka|                8|\n",
      "|    Sumant|     Peac|     supe|                4|\n",
      "| Duangkaew| Piveteau|     dupi|                8|\n",
      "+----------+---------+---------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first two character from lastname followed by all character from firstname.\n",
    "# Convert all the character in lower case with column name 'user_name' from employees DF.\n",
    "\n",
    "# The link below provide the solution for using last name length. Try and fix the problem.\n",
    "#https://stackoverflow.com/questions/51140470/using-a-column-value-as-a-parameter-to-a-spark-dataframe-function\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import length, substring, lower, concat, expr\n",
    "\n",
    "\n",
    "fname_max_length = 2\n",
    "lname_max_length = length(\"last_name\")\n",
    "substrFname = substring(\"first_name\", 0, fname_max_length)\n",
    "substrLname = substring(\"last_name\", 0, fname_max_length)  # Why can't we use lname_max_length? Try to find solution.\n",
    "concatNameLcase = lower(concat(substrFname, substrLname))\n",
    "employees.withColumn(\"user_name\", concatNameLcase).select(\"first_name\", \"last_name\", \"user_name\",\\\n",
    "                    lname_max_length).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "|first_name|last_name|  user_name|\n",
      "+----------+---------+-----------+\n",
      "|    Georgi|  Facello|  gefacello|\n",
      "|   Bezalel|   Simmel|   besimmel|\n",
      "|     Parto|  Bamford|  pabamford|\n",
      "| Chirstian|  Koblick|  chkoblick|\n",
      "|   Kyoichi| Maliniak| kymaliniak|\n",
      "|    Anneke|  Preusig|  anpreusig|\n",
      "|   Tzvetan|Zielinski|tzzielinski|\n",
      "|    Saniya| Kalloufi| sakalloufi|\n",
      "|    Sumant|     Peac|     supeac|\n",
      "| Duangkaew| Piveteau| dupiveteau|\n",
      "+----------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"first_name\", \"last_name\",\\\n",
    "          expr(\"lower(concat(substring(first_name, 0, 2), substring(last_name, 0, length(last_name)))) as user_name\")\\\n",
    "          ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|birth_date|   emp_dob|birth_mm_dd|\n",
      "+----------+----------+-----------+\n",
      "|1953-09-02|0000-00-00|      09-02|\n",
      "|1964-06-02|0000-00-00|      06-02|\n",
      "|1959-12-03|0000-00-00|      12-03|\n",
      "|1954-05-01|0000-00-00|      05-01|\n",
      "|1955-01-21|0000-00-00|      01-21|\n",
      "|1953-04-20|0000-00-00|      04-20|\n",
      "|1957-05-23|0000-00-00|      05-23|\n",
      "|1958-02-19|0000-00-00|      02-19|\n",
      "|1952-04-19|0000-00-00|      04-19|\n",
      "|1963-06-01|0000-00-00|      06-01|\n",
      "+----------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace employees birth_date with '0' but '-' alias with emp_dob.\n",
    "# Remove year from birth_date and rename column with birth_mm_dd\n",
    "# date_pattern = \"^\\d{4}-\\d{2}-\\d{2}$\"\n",
    "# Learn more about regular expression aka regex \n",
    "# https://www.rexegg.com/regex-quickstart.html\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "\n",
    "employees.select(\"birth_date\", regexp_replace(col(\"birth_date\"), '\\d', '0')\\\n",
    "                 .alias(\"emp_dob\"),\\\n",
    "                 regexp_replace(col(\"birth_date\"), \"^\\d{4}-\" , '')\\\n",
    "                 .alias(\"birth_mm_dd\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|first_name|isAm|\n",
      "+----------+----+\n",
      "|    Jaewon|   0|\n",
      "|  Takahira|   0|\n",
      "|    Chenxi|   0|\n",
      "|    Bikash|   0|\n",
      "|     Marjo|   0|\n",
      "|   Mingsen|   0|\n",
      "|  Dayanand|   0|\n",
      "|   Itzchak|   0|\n",
      "|    Sanjay|   0|\n",
      "|   Heather|   0|\n",
      "|      Adam|   0|\n",
      "|   Martien|   0|\n",
      "|    Marsha|   0|\n",
      "|      Ymte|   0|\n",
      "|       Utz|   0|\n",
      "|       Goo|   0|\n",
      "|     Nahum|   0|\n",
      "|     Mohan|   0|\n",
      "|Abdelghani|   0|\n",
      "|     Yucai|   0|\n",
      "+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.selectExpr(\"first_name\", \"instr(first_name, 'Am') as isAm\")\\\n",
    "            .distinct()\\\n",
    "            .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|first_name|isAm|\n",
      "+----------+----+\n",
      "|  Amalendu|   1|\n",
      "|   Amstein|   1|\n",
      "|   Amabile|   1|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show distinct records where employee first name contains 'Am'\n",
    "\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "employees.selectExpr(\"first_name\", \"instr(first_name, 'Am') as isAm\")\\\n",
    "            .where(\"isAm == 1\")\\\n",
    "            .distinct()\\\n",
    "            .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+------+----------+----------+---------+------+----------+\n",
      "|instr(first_name, Sri)|instr(first_name, Geo)|instr(first_name, Par)|instr(first_name, Ama)|instr(first_name, Nir)|instr(first_name, Sau)|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+------+----------+----------+---------+------+----------+\n",
      "|                     0|                     1|                     0|                     0|                     0|                     0| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "|                     0|                     0|                     1|                     0|                     0|                     0| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+------+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+------+----------+----------+---------+------+----------+\n",
      "|instr(first_name, Sri)|instr(first_name, Geo)|instr(first_name, Par)|instr(first_name, Ama)|instr(first_name, Nir)|instr(first_name, Sau)|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+------+----------+----------+---------+------+----------+\n",
      "|                     0|                     1|                     0|                     0|                     0|                     0| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "|                     0|                     0|                     1|                     0|                     0|                     0| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "|                     0|                     0|                     0|                     0|                     0|                     0| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+------+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+\n",
      "|first_name|\n",
      "+----------+\n",
      "|    Georgi|\n",
      "|   Bezalel|\n",
      "|     Parto|\n",
      "| Chirstian|\n",
      "|   Kyoichi|\n",
      "|    Anneke|\n",
      "|   Tzvetan|\n",
      "|    Saniya|\n",
      "|    Sumant|\n",
      "| Duangkaew|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display employee first name that contains 'Sri, Geo, Par, Ama, Nir, Sau'\n",
    "# @ todo validate the result it not working as expected.\n",
    "\n",
    "from pyspark.sql.functions import instr, expr\n",
    "\n",
    "nameLike = [\"Sri\", \"Geo\", \"Par\", \"Ama\", \"Nir\", \"Sau\"]\n",
    "def name_checker(name, nameLike):\n",
    "    return instr(name, nameLike)\n",
    "\n",
    "nameContains = [name_checker(employees.first_name, n) for n in nameLike]\n",
    "nameContains.append(expr(\"*\"))  # append column\n",
    "employees.select(nameContains).show(10)\n",
    "employees.select(*nameContains).show(10)\n",
    "employees.select(*nameContains).select(\"first_name\").show(10)\n",
    "empTMP = employees.select(*nameContains)\n",
    "#.where(\"instr(first_name, sri) == 1\").show(10)  # Apply the filter condition on the given column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|first_name|\n",
      "+----------+\n",
      "|    Georgi|\n",
      "|    Georgy|\n",
      "|    Georgi|\n",
      "|  Srinidhi|\n",
      "|    Georgi|\n",
      "|    Georgy|\n",
      "|    Georgi|\n",
      "|    Georgy|\n",
      "|    Georgy|\n",
      "|    Georgi|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empTMP = empTMP.withColumnRenamed(\"instr(first_name, sri)\", 'contains_sr')\\\n",
    "         .withColumnRenamed(\"instr(first_name, geo)\", 'contains_ge')\n",
    "empTMP.select(\"first_name\").where((col(\"contains_sr\") == 1) | (col(\"contains_ge\") == 1)).show(10) #selectExpr(\"first_name, contains_sr\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Date and Timestamp Type Manipulation\n",
    "\n",
    "Date and Timestamps plays crucial role during data modeling and consider as important attribute for tracking information. The format of date and timestamps  must be specified correctly for any database and programming languages. While reading the schema from file, usually date and timestamp types can be read as string and later converted to respective date type. Since all the application has its own date type formatting, treating string is better approach during schema-on-read.   \n",
    "* Date: stores only calendar date. Default format is `yyyy-mm-dd`.\n",
    "* Timestamps: stores date and time. Spark only supports seconds precision. While handling milliseconds and microseconds it need to treated as `longs`. Spark uses Java dates and timestamps formatting underneath. Default format is `yyyy-mm-dd hh:mm:ss`.\n",
    "\n",
    "To use own date formatting style refer to [Java SimpleDateFormat API](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------------+-----------------------+---------------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |current_date|current_timestamp      |current_date_str_type|\n",
      "+------+----------+----------+---------+------+----------+------------+-----------------------+---------------------+\n",
      "|10001 |1953-09-02|Georgi    |Facello  |M     |1986-06-26|2020-11-04  |2020-11-04 06:51:00.357|2020-11-04           |\n",
      "|10002 |1964-06-02|Bezalel   |Simmel   |F     |1985-11-21|2020-11-04  |2020-11-04 06:51:00.357|2020-11-04           |\n",
      "|10003 |1959-12-03|Parto     |Bamford  |M     |1986-08-28|2020-11-04  |2020-11-04 06:51:00.357|2020-11-04           |\n",
      "|10004 |1954-05-01|Chirstian |Koblick  |M     |1986-12-01|2020-11-04  |2020-11-04 06:51:00.357|2020-11-04           |\n",
      "|10005 |1955-01-21|Kyoichi   |Maliniak |M     |1989-09-12|2020-11-04  |2020-11-04 06:51:00.357|2020-11-04           |\n",
      "+------+----------+----------+---------+------+----------+------------+-----------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new tmp_emp DF by adding current date and timestamp from employees DF.\n",
    "\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "tmp_emp = employees.withColumn(\"current_date\", current_date())\\\n",
    "         .withColumn(\"current_timestamp\", current_timestamp())\\\n",
    "         .withColumn(\"current_date_str_type\", current_date().cast(\"string\"))\n",
    "tmp_emp.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- current_date: date (nullable = false)\n",
      " |-- current_timestamp: timestamp (nullable = false)\n",
      " |-- current_date_str_type: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_emp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_date()` is used to convert string date to date type. If the string format doesn't matches with specified date type format then it will return `null` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- str_curr_dt: date (nullable = true)\n",
      "\n",
      "+------+-----------+\n",
      "|emp_no|str_curr_dt|\n",
      "+------+-----------+\n",
      "| 10001| 2020-11-04|\n",
      "| 10002| 2020-11-04|\n",
      "| 10003| 2020-11-04|\n",
      "| 10004| 2020-11-04|\n",
      "| 10005| 2020-11-04|\n",
      "| 10006| 2020-11-04|\n",
      "| 10007| 2020-11-04|\n",
      "| 10008| 2020-11-04|\n",
      "| 10009| 2020-11-04|\n",
      "| 10010| 2020-11-04|\n",
      "+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert current_date_str_type column that was stored as string type into date type.\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "empid_hire_dt = tmp_emp.select(\"emp_no\", to_date(\"current_date_str_type\").alias(\"str_curr_dt\"))            \n",
    "empid_hire_dt.printSchema()\n",
    "empid_hire_dt.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2019-12-20')|to_date('2019-20-20')|\n",
      "+---------------------+---------------------+\n",
      "|           2019-12-20|                 null|\n",
      "|           2019-12-20|                 null|\n",
      "|           2019-12-20|                 null|\n",
      "|           2019-12-20|                 null|\n",
      "|           2019-12-20|                 null|\n",
      "+---------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert string type to date type when date format is wrong\n",
    "\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "#2019-20-20 is not a valid date so it will return null value.\n",
    "\n",
    "employees.select(to_date(lit(\"2019-12-20\")), to_date(lit(\"2019-20-20\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------------------+\n",
      "| hire_date|date_add(hire_date, 30)|date_sub(hire_date, 30)|\n",
      "+----------+-----------------------+-----------------------+\n",
      "|1986-06-26|             1986-07-26|             1986-05-27|\n",
      "|1985-11-21|             1985-12-21|             1985-10-22|\n",
      "|1986-08-28|             1986-09-27|             1986-07-29|\n",
      "|1986-12-01|             1986-12-31|             1986-11-01|\n",
      "|1989-09-12|             1989-10-12|             1989-08-13|\n",
      "|1989-06-02|             1989-07-02|             1989-05-03|\n",
      "|1989-02-10|             1989-03-12|             1989-01-11|\n",
      "|1994-09-15|             1994-10-15|             1994-08-16|\n",
      "|1985-02-18|             1985-03-20|             1985-01-19|\n",
      "|1989-08-24|             1989-09-23|             1989-07-25|\n",
      "+----------+-----------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add and subtract 30 days on hire date for all employees.\n",
    "\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "employees.select(\"hire_date\", date_add(col(\"hire_date\"), 30), date_sub(col(\"hire_date\"), 30)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "| hire_date|today_hire_day|\n",
      "+----------+--------------+\n",
      "|1986-06-26|         12550|\n",
      "|1985-11-21|         12767|\n",
      "|1986-08-28|         12487|\n",
      "|1986-12-01|         12392|\n",
      "|1989-09-12|         11376|\n",
      "+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate days difference from hire date till now.\n",
    "\n",
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "employees.select(\"hire_date\", datediff(current_date(), \"hire_date\").alias(\"today_hire_day\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|month_employed|\n",
      "+--------------+\n",
      "|  412.29032258|\n",
      "|   419.4516129|\n",
      "|  410.22580645|\n",
      "|  407.09677419|\n",
      "|  373.74193548|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+\n",
      "|month_employed|\n",
      "+--------------+\n",
      "| -412.29032258|\n",
      "|  -419.4516129|\n",
      "| -410.22580645|\n",
      "| -407.09677419|\n",
      "| -373.74193548|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total month of employees hire till now\n",
    "\n",
    "from pyspark.sql.functions import months_between\n",
    "\n",
    "employees.select(months_between(current_date(), \"hire_date\").alias(\"month_employed\")).show(5) # order always matter. today in param 1\n",
    "employees.select(months_between(\"hire_date\", current_date()).alias(\"month_employed\")).show(5) # order always matter. today in param 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying own date formatting style using SimpleDateFormat. Refer to previous link shown above for different formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|correct_date|incorrect_date|\n",
      "+------------+--------------+\n",
      "|  2019-12-20|          null|\n",
      "|  2019-12-20|          null|\n",
      "|  2019-12-20|          null|\n",
      "|  2019-12-20|          null|\n",
      "|  2019-12-20|          null|\n",
      "+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "ownDateFormat = \"yyyy-MM-dd\"\n",
    "employees.select(to_date(lit(\"2019-12-20\"), ownDateFormat).alias(\"correct_date\"), # correct date format\n",
    "                 to_date(lit(\"2019-20-20\"), ownDateFormat).alias(\"incorrect_date\"),  # incorrect date format, month 20 doesn't exist\n",
    "                ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|correct_date|incorrect_date|\n",
      "+------------+--------------+\n",
      "|        null|    2019-02-20|\n",
      "|        null|    2019-02-20|\n",
      "|        null|    2019-02-20|\n",
      "|        null|    2019-02-20|\n",
      "|        null|    2019-02-20|\n",
      "+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ownDateFormat = \"yyyy-dd-MM\"\n",
    "employees.select(to_date(lit(\"2019-12-20\"), ownDateFormat).alias(\"correct_date\"), # incorrect date format\n",
    "                 to_date(lit(\"2019-20-02\"), ownDateFormat).alias(\"incorrect_date\"),  # correct date format, month 20 doesn't exist\n",
    "                ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_timestamp()` is used to convert string date to timestamp type. If the string format doesn't matches with specified date type format then it will return `null` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "| hire_date|      hire_datetime|\n",
      "+----------+-------------------+\n",
      "|1986-06-26|1986-06-26 00:00:00|\n",
      "|1985-11-21|1985-11-21 00:00:00|\n",
      "|1986-08-28|1986-08-28 00:00:00|\n",
      "|1986-12-01|1986-12-01 00:00:00|\n",
      "|1989-09-12|1989-09-12 00:00:00|\n",
      "+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert hire date to timestamp type\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "ownDateFormat = \"yyyy-dd-MM\"\n",
    "employees.select(\"hire_date\",\n",
    "                 to_timestamp(col(\"hire_date\"), ownDateFormat).alias(\"hire_datetime\") # hours, mins and second is added.\n",
    "                ).show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison operators can be used to compare difference between two dates. String literal as date value can also be used while comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+---------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date|\n",
      "+------+----------+----------+---------+------+---------+\n",
      "+------+----------+----------+---------+------+---------+\n",
      "\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare between two dates using logical operator to filter the values.\n",
    "\n",
    "employees.where(col(\"hire_date\") > current_date()).show(10) # filter if hire_date is greater than today.\n",
    "employees.where(col(\"hire_date\") < current_date()).show(10) # filter if hire_date is less than today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+---------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date|\n",
      "+------+----------+----------+---------+------+---------+\n",
      "+------+----------+----------+---------+------+---------+\n",
      "\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|\n",
      "| 10012|1960-10-04|  Patricio|Bridgland|     M|1992-12-18|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare between two dates using string literal for date type with comparison operator to filter the values.\n",
    "\n",
    "# display employee hired in 2019-09-15\n",
    "employees.where(col(\"hire_date\") > \"2019-09-15\").show(10)\n",
    "\n",
    "# display employee hired from jan 1st 1990.\n",
    "employees.where(col(\"hire_date\") >= \"1990-01-01\").show(3) \n",
    "\n",
    "# display employee hired between 1980-01-01 to 1990-01-01\n",
    "employees.where(col(\"hire_date\") >= \"1980-01-01\")\\\n",
    "    .where(col(\"hire_date\") <= \"1990-01-01\")\\\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+----------+------+----------+\n",
      "|emp_no|birth_date|first_name| last_name|gender| hire_date|\n",
      "+------+----------+----------+----------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|   Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|    Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|   Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|   Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi|  Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|   Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan| Zielinski|     F|1989-02-10|\n",
      "| 10009|1952-04-19|    Sumant|      Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew|  Piveteau|     F|1989-08-24|\n",
      "| 10013|1963-06-07| Eberhardt|    Terkki|     M|1985-10-20|\n",
      "| 10014|1956-02-12|     Berni|     Genin|     M|1987-03-11|\n",
      "| 10015|1959-08-19|  Guoxiang| Nooteboom|     M|1987-07-02|\n",
      "| 10018|1954-06-19|  Kazuhide|      Peha|     F|1987-04-03|\n",
      "| 10021|1960-02-20|     Ramzi|      Erde|     M|1988-02-10|\n",
      "| 10023|1953-09-29|     Bojan|Montemayor|     F|1989-12-17|\n",
      "| 10025|1958-10-31| Prasadram|    Heyers|     M|1987-08-17|\n",
      "| 10027|1962-07-10|    Divier|   Reistad|     F|1989-07-07|\n",
      "| 10029|1956-12-13|     Otmar|    Herbst|     M|1985-11-20|\n",
      "| 10033|1956-11-14|      Arif|     Merlo|     M|1987-03-18|\n",
      "| 10034|1962-12-29|     Bader|      Swan|     M|1988-09-21|\n",
      "+------+----------+----------+----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display employee hired between 1980-01-01 to 1990-01-01 using between method\n",
    "\n",
    "employees.where(employees.hire_date.between(\"1980-01-01\", \"1990-01-01\"))\\\n",
    "               .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Complex Type Manipulation\n",
    "\n",
    "Complex types includes arrays, maps and structs. Comparing to Python type, it is list, dict, and list-of-list. Data can be stored in complex types as a column value in Spark. For example, we can store json representation in map that stores all the attributes in single column value. The most important aspect is retrieving the data for complex types. We'll use some examples below for each types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1 Arrays Type  \n",
    "\n",
    "Step 1: Data preparation: Let's create a new array type column in empDF from employees DF. We'll choose hire_date and split based on '-'. If we already have array type then ignore this step.   \n",
    "Step 2: We'll apply some functions to get result from array type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "| hire_date|   create_date|\n",
      "+----------+--------------+\n",
      "|1986-06-26|[1986, 06, 26]|\n",
      "|1985-11-21|[1985, 11, 21]|\n",
      "|1986-08-28|[1986, 08, 28]|\n",
      "|1986-12-01|[1986, 12, 01]|\n",
      "|1989-09-12|[1989, 09, 12]|\n",
      "|1989-06-02|[1989, 06, 02]|\n",
      "|1989-02-10|[1989, 02, 10]|\n",
      "|1994-09-15|[1994, 09, 15]|\n",
      "|1985-02-18|[1985, 02, 18]|\n",
      "|1989-08-24|[1989, 08, 24]|\n",
      "+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- create_date: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new Column with Array type \n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# create empDF DataFrame with create_date attribute\n",
    "empDF = employees.select(\"hire_date\", split(col(\"hire_date\"), \"-\").alias(\"create_date\"))\n",
    "empDF.show(10)\n",
    "empDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|create_date[0]|\n",
      "+--------------+\n",
      "|          1986|\n",
      "|          1985|\n",
      "|          1986|\n",
      "|          1986|\n",
      "|          1989|\n",
      "|          1989|\n",
      "|          1989|\n",
      "|          1994|\n",
      "|          1985|\n",
      "|          1989|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get first value from create_date\n",
    "\n",
    "empDF.selectExpr(\"create_date[0]\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|create_date[10]|\n",
      "+---------------+\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.selectExpr(\"create_date[10]\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|size(create_date)|\n",
      "+-----------------+\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the size of create_date\n",
    "\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "empDF.select(size(\"create_date\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+------+\n",
      "|   create_date|is2019|is1989|\n",
      "+--------------+------+------+\n",
      "|[1986, 06, 26]| false| false|\n",
      "|[1985, 11, 21]| false| false|\n",
      "|[1986, 08, 28]| false| false|\n",
      "|[1986, 12, 01]| false| false|\n",
      "|[1989, 09, 12]| false|  true|\n",
      "|[1989, 06, 02]| false|  true|\n",
      "|[1989, 02, 10]| false|  true|\n",
      "|[1994, 09, 15]| false| false|\n",
      "|[1985, 02, 18]| false| false|\n",
      "|[1989, 08, 24]| false|  true|\n",
      "|[1990, 01, 22]| false| false|\n",
      "|[1992, 12, 18]| false| false|\n",
      "|[1985, 10, 20]| false| false|\n",
      "|[1987, 03, 11]| false| false|\n",
      "|[1987, 07, 02]| false| false|\n",
      "|[1995, 01, 27]| false| false|\n",
      "|[1993, 08, 03]| false| false|\n",
      "|[1987, 04, 03]| false| false|\n",
      "|[1999, 04, 30]| false| false|\n",
      "|[1991, 01, 26]| false| false|\n",
      "+--------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if create_date contains certain value\n",
    "\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "empDF.select(\"create_date\",array_contains(\"create_date\", '2019').alias(\"is2019\"),\\\n",
    "      array_contains(\"create_date\", '1989').alias(\"is1989\"))\\\n",
    "     .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|   create_date|valueExploded|\n",
      "+--------------+-------------+\n",
      "|[1986, 06, 26]|         1986|\n",
      "|[1986, 06, 26]|           06|\n",
      "|[1986, 06, 26]|           26|\n",
      "|[1985, 11, 21]|         1985|\n",
      "|[1985, 11, 21]|           11|\n",
      "|[1985, 11, 21]|           21|\n",
      "|[1986, 08, 28]|         1986|\n",
      "|[1986, 08, 28]|           08|\n",
      "|[1986, 08, 28]|           28|\n",
      "|[1986, 12, 01]|         1986|\n",
      "|[1986, 12, 01]|           12|\n",
      "|[1986, 12, 01]|           01|\n",
      "+--------------+-------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split create_date into several record for each value contained in create_date column value.\n",
    "# To learn more about explode function check the link below used in hive function. \n",
    "# https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "empDF.select(\"create_date\", explode(\"create_date\").alias(\"valueExploded\")).show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 Maps Type\n",
    "\n",
    "Map is similar to dictionary in Python.\n",
    "\n",
    "Step 1: Data preparation: Let's create a new map type column in empDF from employees DF. We'll choose emp_no and first name as it's value. If we already have map type then ignore this step.   \n",
    "Step 2: We'll apply some functions to get result from map type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+\n",
      "|emp_no|first_name|              empMap|\n",
      "+------+----------+--------------------+\n",
      "| 10001|    Georgi|   [10001 -> Georgi]|\n",
      "| 10002|   Bezalel|  [10002 -> Bezalel]|\n",
      "| 10003|     Parto|    [10003 -> Parto]|\n",
      "| 10004| Chirstian|[10004 -> Chirstian]|\n",
      "| 10005|   Kyoichi|  [10005 -> Kyoichi]|\n",
      "| 10006|    Anneke|   [10006 -> Anneke]|\n",
      "| 10007|   Tzvetan|  [10007 -> Tzvetan]|\n",
      "| 10008|    Saniya|   [10008 -> Saniya]|\n",
      "| 10009|    Sumant|   [10009 -> Sumant]|\n",
      "| 10010| Duangkaew|[10010 -> Duangkaew]|\n",
      "+------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- empMap: map (nullable = false)\n",
      " |    |-- key: integer\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Map Column \n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# create empDF DataFrame with empMap attribute\n",
    "empDF = employees.select(\"emp_no\", \"first_name\", create_map(col(\"emp_no\"), col(\"first_name\")).alias(\"empMap\"))\n",
    "empDF.show(10)\n",
    "empDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------+\n",
      "|emp_no|              empMap|empMap[10001]|\n",
      "+------+--------------------+-------------+\n",
      "| 10001|   [10001 -> Georgi]|       Georgi|\n",
      "| 10002|  [10002 -> Bezalel]|         null|\n",
      "| 10003|    [10003 -> Parto]|         null|\n",
      "| 10004|[10004 -> Chirstian]|         null|\n",
      "| 10005|  [10005 -> Kyoichi]|         null|\n",
      "| 10006|   [10006 -> Anneke]|         null|\n",
      "| 10007|  [10007 -> Tzvetan]|         null|\n",
      "| 10008|   [10008 -> Saniya]|         null|\n",
      "| 10009|   [10009 -> Sumant]|         null|\n",
      "| 10010|[10010 -> Duangkaew]|         null|\n",
      "+------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access value from map column through its key.\n",
    "\n",
    "empDF.selectExpr(\"emp_no\", \"empMap\", \"empMap[10001]\").show(10) # since the key is integer we have used without quote but quote can also be used.\n",
    "\n",
    "# If the key is not present for record value then it will give null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+\n",
      "|emp_no|  key|    value|\n",
      "+------+-----+---------+\n",
      "| 10001|10001|   Georgi|\n",
      "| 10002|10002|  Bezalel|\n",
      "| 10003|10003|    Parto|\n",
      "| 10004|10004|Chirstian|\n",
      "| 10005|10005|  Kyoichi|\n",
      "| 10006|10006|   Anneke|\n",
      "| 10007|10007|  Tzvetan|\n",
      "| 10008|10008|   Saniya|\n",
      "| 10009|10009|   Sumant|\n",
      "| 10010|10010|Duangkaew|\n",
      "+------+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode map column with key and value column.\n",
    "\n",
    "empDF.selectExpr(\"emp_no\", \"explode(empMap)\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3 Structs Type   \n",
    "\n",
    "Struct can be consider as DataFrames of DataFrames. Struct type can be created by putting column names into parenthesis. i.e. like declaring tuples in Python. e.g. `struct(column_1, column_2)`\n",
    "\n",
    "Step 1: Data preparation: Let's create a new struct type column in empDF from employees DF. We'll choose first_name, and last_name as it's value. If we already have struct type then ignore this step.   \n",
    "Step 2: We'll apply some functions to get result from struct type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|empStruct            |\n",
      "+---------------------+\n",
      "|[Georgi, Facello]    |\n",
      "|[Bezalel, Simmel]    |\n",
      "|[Parto, Bamford]     |\n",
      "|[Chirstian, Koblick] |\n",
      "|[Kyoichi, Maliniak]  |\n",
      "|[Anneke, Preusig]    |\n",
      "|[Tzvetan, Zielinski] |\n",
      "|[Saniya, Kalloufi]   |\n",
      "|[Sumant, Peac]       |\n",
      "|[Duangkaew, Piveteau]|\n",
      "+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- empStruct: struct (nullable = false)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create empStruct Column \n",
    "\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "empDF = employees.select(struct(\"first_name\", \"last_name\").alias(\"empStruct\"))\n",
    "empDF.show(10,False)\n",
    "empDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+-----------------+\n",
      "|           empStruct|first_name|last_name|         FullName|\n",
      "+--------------------+----------+---------+-----------------+\n",
      "|   [Georgi, Facello]|    Georgi|  Facello|    GeorgiFacello|\n",
      "|   [Bezalel, Simmel]|   Bezalel|   Simmel|    BezalelSimmel|\n",
      "|    [Parto, Bamford]|     Parto|  Bamford|     PartoBamford|\n",
      "|[Chirstian, Koblick]| Chirstian|  Koblick| ChirstianKoblick|\n",
      "| [Kyoichi, Maliniak]|   Kyoichi| Maliniak|  KyoichiMaliniak|\n",
      "|   [Anneke, Preusig]|    Anneke|  Preusig|    AnnekePreusig|\n",
      "|[Tzvetan, Zielinski]|   Tzvetan|Zielinski| TzvetanZielinski|\n",
      "|  [Saniya, Kalloufi]|    Saniya| Kalloufi|   SaniyaKalloufi|\n",
      "|      [Sumant, Peac]|    Sumant|     Peac|       SumantPeac|\n",
      "|[Duangkaew, Pivet...| Duangkaew| Piveteau|DuangkaewPiveteau|\n",
      "+--------------------+----------+---------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get value from Struct through column name\n",
    "\n",
    "empDF.selectExpr(\"empStruct\",\\\n",
    "             \"empStruct.first_name\",\\\n",
    "             \"empStruct.last_name\",\\\n",
    "             \"concat(empStruct.first_name, empStruct.last_name) as FullName\")\\\n",
    "             .show(10) # show first_name, last_name, and fullname value from struct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|           empStruct|empStruct.first_name|empStruct.last_name|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|   [Georgi, Facello]|              Georgi|            Facello|\n",
      "|   [Bezalel, Simmel]|             Bezalel|             Simmel|\n",
      "|    [Parto, Bamford]|               Parto|            Bamford|\n",
      "|[Chirstian, Koblick]|           Chirstian|            Koblick|\n",
      "| [Kyoichi, Maliniak]|             Kyoichi|           Maliniak|\n",
      "|   [Anneke, Preusig]|              Anneke|            Preusig|\n",
      "|[Tzvetan, Zielinski]|             Tzvetan|          Zielinski|\n",
      "|  [Saniya, Kalloufi]|              Saniya|           Kalloufi|\n",
      "|      [Sumant, Peac]|              Sumant|               Peac|\n",
      "|[Duangkaew, Pivet...|           Duangkaew|           Piveteau|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get value from Struct through column name \n",
    "\n",
    "# It uses getField method\n",
    "\n",
    "empDF.select(\"empStruct\",\\\n",
    "            col(\"empStruct\").getField(\"first_name\"),\\\n",
    "            col(\"empStruct\").getField(\"last_name\"))\\\n",
    "            .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|           empStruct|first_name|last_name|\n",
      "+--------------------+----------+---------+\n",
      "|   [Georgi, Facello]|    Georgi|  Facello|\n",
      "|   [Bezalel, Simmel]|   Bezalel|   Simmel|\n",
      "|    [Parto, Bamford]|     Parto|  Bamford|\n",
      "|[Chirstian, Koblick]| Chirstian|  Koblick|\n",
      "| [Kyoichi, Maliniak]|   Kyoichi| Maliniak|\n",
      "|   [Anneke, Preusig]|    Anneke|  Preusig|\n",
      "|[Tzvetan, Zielinski]|   Tzvetan|Zielinski|\n",
      "|  [Saniya, Kalloufi]|    Saniya| Kalloufi|\n",
      "|      [Sumant, Peac]|    Sumant|     Peac|\n",
      "|[Duangkaew, Pivet...| Duangkaew| Piveteau|\n",
      "+--------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get get all Struct values using '*'\n",
    "\n",
    "empDF.selectExpr(\"empStruct\",\n",
    "    \"empStruct.*\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Handling Nulls\n",
    "\n",
    "Null values always plays vital role in all programming language. When loading the data, if data value doesn't matches with defined schema then those are always displayed as null values. Also, if some data value are not set then those can also be treated as null values. Empty value is not equivalent to null value.\n",
    "\n",
    "Null values in Spark can be:-   \n",
    "* dropped explicitly\n",
    "* filled with some values. It can be replace globally or column-wise.\n",
    "\n",
    "Several functions can be used for handling null values. Such as `coalese(), ifnull(), nullIf(), nvl(), nvl2()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+\n",
      "| 10001|1953-09-02|    Georgi|    Facello|     M|1986-06-26|             67|     true|         Yes|\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|             56|    false|        null|\n",
      "| 10003|1959-12-03|     Parto|    Bamford|     M|1986-08-28|             60|     true|         Yes|\n",
      "| 10004|1954-05-01| Chirstian|    Koblick|     M|1986-12-01|             66|     true|         Yes|\n",
      "| 10005|1955-01-21|   Kyoichi|   Maliniak|     M|1989-09-12|             65|     true|         Yes|\n",
      "| 10006|1953-04-20|    Anneke|    Preusig|     F|1989-06-02|             67|     true|         Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|  Zielinski|     F|1989-02-10|             63|     true|         Yes|\n",
      "| 10008|1958-02-19|    Saniya|   Kalloufi|     M|1994-09-15|             62|     true|         Yes|\n",
      "| 10009|1952-04-19|    Sumant|       Peac|     F|1985-02-18|             68|     true|         Yes|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|             57|    false|        null|\n",
      "| 10011|1953-11-07|      Mary|      Sluis|     F|1990-01-22|             66|     true|         Yes|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|             60|     true|         Yes|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|             57|    false|        null|\n",
      "| 10014|1956-02-12|     Berni|      Genin|     M|1987-03-11|             64|     true|         Yes|\n",
      "| 10015|1959-08-19|  Guoxiang|  Nooteboom|     M|1987-07-02|             61|     true|         Yes|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|             59|    false|        null|\n",
      "| 10017|1958-07-06| Cristinel|  Bouloucos|     F|1993-08-03|             62|     true|         Yes|\n",
      "| 10018|1954-06-19|  Kazuhide|       Peha|     F|1987-04-03|             66|     true|         Yes|\n",
      "| 10019|1953-01-23|   Lillian|    Haddadi|     M|1999-04-30|             67|     true|         Yes|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|             67|     true|         Yes|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             67|     true|         Yes|            Yes|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|             56|    false|        null|          false|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             66|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             65|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             67|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             63|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             62|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             68|     true|         Yes|            Yes|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|             57|    false|        null|          false|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             67|     true|         Yes|            Yes|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|             56|    false|        null|             No|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             66|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             65|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             67|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             63|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             62|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             68|     true|         Yes|            Yes|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|             57|    false|        null|             No|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if first columns value is empty or null, if it is empty then retrieve value from second column or put literal value.\n",
    "\n",
    "from pyspark.sql.functions import coalesce, months_between, floor, lit, current_date, col, expr\n",
    "\n",
    "# Create new DataFrame tmpDF that contain all the columns from employees and adding new columns \"age_above_60\"\n",
    "# column 'age_above_60' is calculated field that stores whether employees age is above 60 or not.\n",
    "employees.printSchema()\n",
    "\n",
    "# assign value\n",
    "total_month = 12\n",
    "filter_age = 60\n",
    "\n",
    "# emp_current_age stores employee current age\n",
    "# isabove50 stores boolean value whether or not age is 60\n",
    "# age_above_60 stores Yes for above age 60 else null\n",
    "\n",
    "tmpDF = employees.withColumn(\"emp_current_age\",\\\n",
    "                  floor(months_between(current_date(), \"birth_date\")/total_month))\\\n",
    "                  .withColumn(\"isabove60\", col(\"emp_current_age\") >= filter_age)\\\n",
    "                  .withColumn(\"age_above_60\", expr(\"case when isabove60 then 'Yes' else null end\"))                             \n",
    "tmpDF.show(20)\n",
    "\n",
    "# coalese to filter the null values with other column value\n",
    "tmpDF = tmpDF.withColumn(\"ready_to_retire\", coalesce(\"age_above_60\", col(\"isabove60\").cast(\"string\")))\n",
    "tmpDF.show(10)\n",
    "\n",
    "# coalese to filter the null values with literal 'No' value\n",
    "tmpDF = tmpDF.withColumn(\"ready_to_retire\", coalesce(\"age_above_60\", lit(\"No\")))\n",
    "tmpDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.1 Dropping Null Values\n",
    "\n",
    "`drop()` functions is used to remove rows that has null values. The default `drop()` method without parameter will drop records that has any null values. The parameter to the methods are:-   \n",
    "`any`: e.g. `drop(\"any\")`. `any` argument will drops row if row has any null values.        \n",
    "`all`: e.g. `drop(\"all\")`. `all` argument will drops row if row has all null values.     \n",
    "`any` or `all` followed by array of columns: e.g. `drop(\"all\", subset=[\"first_name\", \"last_name\"])`. Drops row only from the specified columns with `any` and `all` argument defined above.   \n",
    "\n",
    "Mostly, used for cleaning the final DataFrame after merging/joining multiple DataFrame which contains null during left, right, full join etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             67|     true|         Yes|            Yes|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             66|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             65|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             67|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             63|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             62|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             68|     true|         Yes|            Yes|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|             66|     true|         Yes|            Yes|\n",
      "| 10012|1960-10-04|  Patricio|Bridgland|     M|1992-12-18|             60|     true|         Yes|            Yes|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all records that has null values in tmpDF and store into emp_above60DF DF.\n",
    "\n",
    "emp_above60DF = tmpDF.na.drop()\n",
    "emp_above60DF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             67|     true|         Yes|            Yes|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             66|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             65|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             67|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             63|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             62|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             68|     true|         Yes|            Yes|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|             66|     true|         Yes|            Yes|\n",
      "| 10012|1960-10-04|  Patricio|Bridgland|     M|1992-12-18|             60|     true|         Yes|            Yes|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all records that has null values in any columns using 'any' parameter in tmpDF  and store in empNoNullDF DF.\n",
    "# 'any' parameter is to drop records that has any null values in the DataFrame. \n",
    "# Reason to use any: Sometime multiple columns might has null values which might not be useful during analysis.\n",
    "\n",
    "empNoNullDF = tmpDF.na.drop(\"any\")\n",
    "empNoNullDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             67|     true|         Yes|            Yes|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|             56|    false|        null|             No|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             66|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             65|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             67|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             63|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             62|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             68|     true|         Yes|            Yes|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|             57|    false|        null|             No|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all records that has null values across entire columns value using 'all' parameter in tmpDF DF. \n",
    "# For example: Due to data quality issue if incoming data is retrieve then entire record will be stored as null values\n",
    "# so we need to drop those bad records. In such case use 'all' in drop() method to delete records that has\n",
    "# null values in entire record.\n",
    "\n",
    "\n",
    "# Drop record where entire field value is null. None of the records has null values for all fields so it won't \n",
    "# drop any records.\n",
    "\n",
    "empInvalidDF = tmpDF.na.drop(\"all\")\n",
    "empInvalidDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             67|     true|         Yes|            Yes|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             66|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             65|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             67|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             63|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             62|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             68|     true|         Yes|            Yes|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|             66|     true|         Yes|            Yes|\n",
      "| 10012|1960-10-04|  Patricio|Bridgland|     M|1992-12-18|             60|     true|         Yes|            Yes|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all records that has null values with 'any' in tmpDF DataFrame only for first_name and age_above_60 column.\n",
    "\n",
    "empAbove60DF = tmpDF.na.drop(\"any\", subset=[\"first_name\", \"age_above_60\"])\n",
    "empAbove60DF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.2 Filling Null Values\n",
    "\n",
    "`fill()` method is used to fill records that contains null values for one or more columns with explicit user defined value. It works for all data types. `dict` can also used to fill multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------------+\n",
      "|isabove60|      age_above_60|ready_to_retire|\n",
      "+---------+------------------+---------------+\n",
      "|     true|               Yes|            Yes|\n",
      "|    false|Still Not Above 60|             No|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|    false|Still Not Above 60|             No|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|    false|Still Not Above 60|             No|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|    false|Still Not Above 60|             No|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "+---------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill null values with \"Still Not Above 60\" in employees that has null values.\n",
    "# The age_above_60 null values will be replace with \"Still Not Above 60\" literal value.\n",
    "\n",
    "empFillNullDF = tmpDF.na.fill(\"Still Not Above 60\")\n",
    "empFillNullDF.selectExpr(\"isabove60\", \"age_above_60\", \"ready_to_retire\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             67|     true|         Yes|            Yes|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|             56|    false|Wait till 60|             No|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             66|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             65|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             67|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             63|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             62|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             68|     true|         Yes|            Yes|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|             57|    false|Wait till 60|             No|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill \"XXXX\", \"0000-00-00\" and \"Wait till 60\"\n",
    "# for first_name, birth_date and age_above_60 columns respectively in tmpDF DF that has null values using\n",
    "# input from \"null_column_dict\" dict\n",
    "\n",
    "# Check the output for 'age_above_60' since only this column has null value\n",
    "\n",
    "null_column_dict = {\"first_name\": \"XXXX\", \"birth_date\": \"0000-00-00\", \"age_above_60\": 'Wait till 60'} \n",
    "empFillNullWithDictDF = tmpDF.na.fill(null_column_dict)\n",
    "empFillNullWithDictDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "20201104\n",
      "+------+------+----------+------------+\n",
      "|f_name|l_name| hire_date|hire_date_dt|\n",
      "+------+------+----------+------------+\n",
      "|  john|   doe|2020-04-01|  2020-04-01|\n",
      "|  null|   doe|2020-04-01|  2020-04-01|\n",
      "|  john|  null|2020-04-01|  2020-04-01|\n",
      "|  john|   doe|      null|        null|\n",
      "|  john|   doe|2020-04-01|  2020-04-01|\n",
      "+------+------+----------+------------+\n",
      "\n",
      "+------+------+----------+------------+\n",
      "|f_name|l_name| hire_date|hire_date_dt|\n",
      "+------+------+----------+------------+\n",
      "|  john|   doe|2020-04-01|  2020-04-01|\n",
      "|  XXXX|   doe|2020-04-01|  2020-04-01|\n",
      "|  john|  ZZZZ|2020-04-01|  2020-04-01|\n",
      "|  john|   doe|0000-00-00|  9999-12-31|\n",
      "|  john|   doe|2020-04-01|  2020-04-01|\n",
      "+------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today  = datetime.today().strftime('%Y-%m-%d')\n",
    "print(type(today))\n",
    "print(datetime.today().strftime('%Y%m%d'))\n",
    "exampleDF = spark.read.format(\"csv\").option(\"path\",\"/tmp/mytest.csv\").option(\"inferschema\",\"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load()\n",
    "#exampleDF.show(20)\n",
    "exampleDF = exampleDF.withColumn(\"hire_date_dt\", col(\"hire_date\").cast(\"date\"))\n",
    "#exampleDF.printSchema()\n",
    "exampleDF.show(20)\n",
    "newValue = {\"f_name\": \"XXXX\", \"l_name\": \"ZZZZ\", \"hire_date\": \"0000-00-00\", \"hire_date_dt\": \"9999-12-31\"} \n",
    "exampleFillNullWithDictDF = exampleDF.na.fill(newValue)\n",
    "exampleFillNullWithDictDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.3 Filtering Null Values\n",
    "\n",
    "Null values can be filtered by using `isNull()` method. If the column value is null then it return true and filter/select only records having null values.   \n",
    "\n",
    "Not null values can be filtered by using `isNotNull()` method. It perform opposite operation compared to `isNull()`. If the column value is not null then it return true and filter/select only records having not null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+----------------------+--------------------------+\n",
      "|emp_current_age|isabove60|age_above_60|(age_above_60 IS NULL)|(age_above_60 IS NOT NULL)|\n",
      "+---------------+---------+------------+----------------------+--------------------------+\n",
      "|             66|     true|         Yes|                 false|                      true|\n",
      "|             56|    false|        null|                  true|                     false|\n",
      "|             60|     true|         Yes|                 false|                      true|\n",
      "|             66|     true|         Yes|                 false|                      true|\n",
      "|             65|     true|         Yes|                 false|                      true|\n",
      "|             67|     true|         Yes|                 false|                      true|\n",
      "|             63|     true|         Yes|                 false|                      true|\n",
      "|             62|     true|         Yes|                 false|                      true|\n",
      "|             68|     true|         Yes|                 false|                      true|\n",
      "|             57|    false|        null|                  true|                     false|\n",
      "+---------------+---------+------------+----------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|             56|    false|        null|             No|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|             57|    false|        null|             No|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|             59|    false|        null|             No|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|             56|    false|        null|             No|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|             59|    false|        null|             No|\n",
      "| 10027|1962-07-10|    Divier|    Reistad|     F|1989-07-07|             57|    false|        null|             No|\n",
      "| 10028|1963-11-26|  Domenick|   Tempesti|     M|1991-10-22|             56|    false|        null|             No|\n",
      "| 10032|1960-08-09|     Jeong|    Reistad|     F|1990-06-20|             59|    false|        null|             No|\n",
      "| 10034|1962-12-29|     Bader|       Swan|     M|1988-09-21|             57|    false|        null|             No|\n",
      "| 10037|1963-07-22|   Pradeep|   Makrucki|     M|1990-12-05|             56|    false|        null|             No|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             66|     true|         Yes|            Yes|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             66|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             65|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             67|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             63|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             62|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             68|     true|         Yes|            Yes|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|             66|     true|         Yes|            Yes|\n",
      "| 10014|1956-02-12|     Berni|    Genin|     M|1987-03-11|             64|     true|         Yes|            Yes|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# show boolean value for isNull() and isNotNull() method\n",
    "tmpDF.select(\"emp_current_age\", \"isabove60\", \"age_above_60\", col(\"age_above_60\").isNull(),col(\"age_above_60\").isNotNull()).\\\n",
    "        show(10)\n",
    "\n",
    "# filter null values from age_above_60 column\n",
    "tmpDF.select(\"*\").where(col(\"age_above_60\").isNull()).show(10)\n",
    "\n",
    "# filter not null values from age_above_60 column\n",
    "tmpDF.select(\"*\").where(col(\"age_above_60\").isNotNull()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 User Defined Functions\n",
    "\n",
    "User Defined Functions (UDFs) are the custom function for manipulation and transforming the record values. If Spark doesn't provide specific function in its module to solve the business logic or problem then we need to create our own function known as UDF. Spark supports UDFs written on multiple languges such as Java, Python, Scala etc where Java and Scala has better performance compared to Python during data serialization. The best practice is to write UDF in Scala and call from Python. UDFs can have one or more columns as input parameters. These functions as simliar to other native functions. The functions need to registered before using it. By default, it is registered as temporary functions which is specific only for certain SparkSession. But it can also be permanently registered.\n",
    "\n",
    "For creating UDF, we use [`udf()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf) method. This method takes two parameter i.e. function name and returnType respectively. \n",
    "\n",
    "* function name is the python function name.\n",
    "* returnType is the return type of the user-defined function. The default return type is StringType. We need to import return type from [pyspark.sql.types](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module.\n",
    "\n",
    "In the code snippet below, we'll create increase_ten_percent UDF both in Python and Scala to add 10% in current salary. Then register the function and apply it in DataFrame to calculate new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0\n",
      "11.0\n",
      "+------+------+----------+----------+------+-------------------------+---------------+\n",
      "|emp_no|salary| from_date|   to_date|salary|(salary + (salary * 0.1))|increase_salary|\n",
      "+------+------+----------+----------+------+-------------------------+---------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26| 60117|                  66128.7|        66128.7|\n",
      "| 10001| 62102|1987-06-26|1988-06-25| 62102|                  68312.2|        68312.2|\n",
      "| 10001| 66074|1988-06-25|1989-06-25| 66074|                  72681.4|        72681.4|\n",
      "| 10001| 66596|1989-06-25|1990-06-25| 66596|                  73255.6|        73255.6|\n",
      "| 10001| 66961|1990-06-25|1991-06-25| 66961|                  73657.1|        73657.1|\n",
      "| 10001| 71046|1991-06-25|1992-06-24| 71046|                  78150.6|        78150.6|\n",
      "| 10001| 74333|1992-06-24|1993-06-24| 74333|                  81766.3|        81766.3|\n",
      "| 10001| 75286|1993-06-24|1994-06-24| 75286|                  82814.6|        82814.6|\n",
      "| 10001| 75994|1994-06-24|1995-06-24| 75994|                  83593.4|        83593.4|\n",
      "| 10001| 76884|1995-06-24|1996-06-23| 76884|                  84572.4|        84572.4|\n",
      "+------+------+----------+----------+------+-------------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- (salary + (salary * 0.1)): double (nullable = true)\n",
      " |-- increase_salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create, register, and call UDF in Python\n",
    "\n",
    "# Add 10% to current salary \n",
    "\n",
    "# Create increase_ten_percent function\n",
    "def increase_ten_percent(amount):\n",
    "    return float((amount * 0.10) + amount)\n",
    "\n",
    "# test function \n",
    "sal_1  = increase_ten_percent(20)\n",
    "print(sal_1)  # must return 22\n",
    "sal_2  = increase_ten_percent(10)\n",
    "print(sal_2)  # must return 10\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "# Create increase_ten_percent UDF\n",
    "increase_ten_percent_udf = udf(increase_ten_percent)\n",
    "\n",
    "# Call UDF\n",
    "salDF = salaries.select(\"*\", \"salary\",(col(\"salary\") + col(\"salary\")*0.10),increase_ten_percent_udf(col(\"salary\"))\\\n",
    "         .alias(\"increase_salary\"))\n",
    "salDF.show(10)\n",
    "salDF.printSchema()   # Note: Notice the data type of increase_salary. It is type String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+------+-------------------------+---------------+\n",
      "|emp_no|salary| from_date|   to_date|salary|(salary + (salary * 0.1))|increase_salary|\n",
      "+------+------+----------+----------+------+-------------------------+---------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26| 60117|                  66128.7|        66128.7|\n",
      "| 10001| 62102|1987-06-26|1988-06-25| 62102|                  68312.2|        68312.2|\n",
      "| 10001| 66074|1988-06-25|1989-06-25| 66074|                  72681.4|        72681.4|\n",
      "| 10001| 66596|1989-06-25|1990-06-25| 66596|                  73255.6|        73255.6|\n",
      "| 10001| 66961|1990-06-25|1991-06-25| 66961|                  73657.1|        73657.1|\n",
      "| 10001| 71046|1991-06-25|1992-06-24| 71046|                  78150.6|        78150.6|\n",
      "| 10001| 74333|1992-06-24|1993-06-24| 74333|                  81766.3|        81766.3|\n",
      "| 10001| 75286|1993-06-24|1994-06-24| 75286|                  82814.6|        82814.6|\n",
      "| 10001| 75994|1994-06-24|1995-06-24| 75994|                  83593.4|        83593.4|\n",
      "| 10001| 76884|1995-06-24|1996-06-23| 76884|                  84572.4|        84572.4|\n",
      "+------+------+----------+----------+------+-------------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- (salary + (salary * 0.1)): double (nullable = true)\n",
      " |-- increase_salary: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create increase_ten_percent UDF by passing return type. In the previous code the return type of UDF is String\n",
    "# but we want to specify as Float, we need to import FloatType and pass into udf method's second parameter.\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "# Create increase_ten_percent UDF \n",
    "increase_ten_percent_udf = udf(increase_ten_percent, FloatType())\n",
    "\n",
    "# Call UDF\n",
    "salDFNew = salaries.select(\"*\", \"salary\",(col(\"salary\") + col(\"salary\")*0.10),increase_ten_percent_udf(col(\"salary\"))\\\n",
    "         .alias(\"increase_salary\"))\n",
    "salDFNew.show(10)\n",
    "salDFNew.printSchema() # Note: Notice the data type of increase_salary. It is type Float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment**: Create increase_ten_percent UDF both in Python and Scala with following features:   \n",
    "* Add 10% from existing salary if employees worked more than 5 years.\n",
    "* Salary field must only be integer and long.\n",
    "* Hired date must be only string and date with 'yyyy-mm-dd' format\n",
    "* Function must check null values for either parameter and return 0 if null is found. \n",
    "* Register the function and apply it in DataFrame to calculate new column bonus_salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0\n",
      "20.0\n",
      "+------+------+----------+----------+---------------+\n",
      "|emp_no|salary| from_date|   to_date|increase_salary|\n",
      "+------+------+----------+----------+---------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|        66128.7|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|        68312.2|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|        72681.4|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|        73255.6|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|        73657.1|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|        78150.6|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|        81766.3|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|        82814.6|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|        83593.4|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|        84572.4|\n",
      "| 10001| 80013|1996-06-23|1997-06-23|        88014.3|\n",
      "| 10001| 81025|1997-06-23|1998-06-23|        89127.5|\n",
      "| 10001| 81097|1998-06-23|1999-06-23|        89206.7|\n",
      "| 10001| 84917|1999-06-23|2000-06-22|        93408.7|\n",
      "| 10001| 85112|2000-06-22|2001-06-22|        93623.2|\n",
      "| 10001| 85097|2001-06-22|2002-06-22|        93606.7|\n",
      "| 10001| 88958|2002-06-22|9999-01-01|        97853.8|\n",
      "| 10002| 65828|1996-08-03|1997-08-03|        72410.8|\n",
      "| 10002| 65909|1997-08-03|1998-08-03|        72499.9|\n",
      "| 10002| 67534|1998-08-03|1999-08-03|        74287.4|\n",
      "+------+------+----------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create, register, and call UDF in Python\n",
    "\n",
    "# Add 10% from current salary \n",
    "# if hired date is more than 5 year to current date.\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Create increase_ten_percent UDF\n",
    "def increase_ten_percent(amount, from_date):\n",
    "    hire_threshold = 5\n",
    "    # @todo: check date pattern    \n",
    "    # check date instance\n",
    "    if isinstance(from_date, str):    \n",
    "        from_date = datetime.strptime(from_date, '%Y-%m-%d')    \n",
    "    # get today\n",
    "    today = datetime.today().date()\n",
    "    # get diff year\n",
    "    diff_year = today.year - from_date.year\n",
    "    if diff_year > hire_threshold:\n",
    "        return float((amount * 0.10) + amount)\n",
    "    else:\n",
    "        return float(amount)\n",
    "\n",
    "# test function \n",
    "sal_1  = increase_ten_percent(20, \"2010-01-01\")\n",
    "print(sal_1)  # must return 22\n",
    "sal_2  = increase_ten_percent(20, \"2019-01-01\")\n",
    "print(sal_2)  # must return 20\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "#from pyspark.sql.types import DateType\n",
    "# Register increase_ten_percent UDF\n",
    "#increase_ten_percent_udf = udf(increase_ten_percent)\n",
    "increase_ten_percent_udf = udf(increase_ten_percent)\n",
    "\n",
    "# Call UDF\n",
    "salaries.select(\"*\", increase_ten_percent_udf(col(\"salary\"), \"from_date\")\\\n",
    "         .alias(\"increase_salary\")).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, register, UDF in Scala and call from Python\n",
    "\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "def increase_ten_percent(amount: Integer):\n",
    "    Integer = (amount * 0.10) + amount\n",
    "\n",
    "# test function\n",
    "increase_ten_percent(20) # must return 1\n",
    "\n",
    "# Register increase_ten_percent UDF\n",
    "val increase_ten_percent_udf = udf(increase_ten_percent(_:Integer):Integer)\n",
    "\n",
    "# Call UDF in Python not in Scala\n",
    "from pyspark.sql.functions import col\n",
    "employees.select(increase_ten_percent_udf(col(\"salary\"))).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Register UDF**\n",
    "\n",
    "The UDF shown above is accessible only for DataFrame function. It cannot be use with string expression like `employees.selectExpr(\"increase_ten_percent_udf(column_name)\")`. We can register the function as Spark SQL function which allows to use string expression as well as calling from SQL function too. The reason behind is \"*UDF registered with Spark SQL functions or expression is valid for DataFrames expression*\".\n",
    "\n",
    "Register UDF as SQL function in Scala:   \n",
    "`spark.udf.register(\"increase_ten_percent_udf\", increase_ten_percent(_:Integer):Integer)`   \n",
    "\n",
    "Register UDF as SQL function in Python:   \n",
    "`spark.udf.register(\"increase_ten_percent_udf\", increase_ten_percent)`  \n",
    "\n",
    "Now, we can use in our DataFrame with `selectExpr` shown below:   \n",
    "`employees.selectExpr(\"increase_ten_percent_udf(column_name)\")`   \n",
    "\n",
    "Although, we have created our UDF and it works as expected. The best practice is to specific the return type from function. If type doesn't matches with then Spark will return `null` value. We can also specify return type as `None` and `Option` in Python and Scala respectively.\n",
    "\n",
    "Register UDF as SQL function in Python with return type:   \n",
    "`from pyspark.sql.types import IntegerType\n",
    "spark.udf.register(\"increase_ten_percent_udf\", increase_ten_percent, IntegerType())`   \n",
    "\n",
    "Similary, we can register HIVE UDF and UDAF through Hive syntax. [Click for more detail](https://blog.cloudera.com/working-with-udfs-in-apache-spark/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+---------------+\n",
      "|emp_no|salary| from_date|   to_date|increase_salary|\n",
      "+------+------+----------+----------+---------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|        66128.7|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|        68312.2|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|        72681.4|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|        73255.6|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|        73657.1|\n",
      "+------+------+----------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.select(\"*\", increase_ten_percent_udf(col(\"salary\"), \"from_date\")\\\n",
    "         .alias(\"increase_salary\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|increase_salary|\n",
      "+---------------+\n",
      "|        66128.7|\n",
      "|        68312.2|\n",
      "|        72681.4|\n",
      "|        73255.6|\n",
      "|        73657.1|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"increase_ten_percent_udf\", increase_ten_percent)\n",
    "salaries.selectExpr(\"increase_ten_percent_udf(salary, from_date) as increase_salary\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
