{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Filtering and Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**3.1 Filtering**](#3.1-Filtering)   \n",
    "[**3.1.1 Filtering Column**](#3.1.1-Filtering-Column)   \n",
    "[**3.1.2 Filtering Row**](#3.1.2-Filtering-Row)   \n",
    "[**3.1.3 Filter single column value**](#3.1.3-Filter-single-column-value)   \n",
    "[**3.1.4 Filter multiple column with AND operator**](#3.1.4-Filter-multiple-column-with-AND-operator)   \n",
    "[**3.1.5 Filter multiple column with OR operator**](#3.1.5-Filter-multiple-column-with-OR-operator)   \n",
    "[**3.1.6 Filter with Boolean expression**](#3.1.6-Filter-with-Boolean-expression)   \n",
    "[**3.2 PySpark SQL Module**](#3.2-PySpark-SQL-Module)   \n",
    "[**3.3 Numeric Type Manipulation**](#3.3-Numeric-Type-Manipulation)   \n",
    "[**3.4 String Type Manipulation**](#3.4-String-Type-Manipulation)   \n",
    "[**3.5 Date and Timestamp Type Manipulation**](#3.5-Date-and-Timestamp-Type-Manipulation)   \n",
    "[**3.6 Complex Type Manipulation**](#3.6-Complex-Type-Manipulation)   \n",
    "[**3.6.1 Arrays Type**](#3.6.1-Arrays-Type)   \n",
    "[**3.6.2 Maps Type**](#3.6.2-Maps-Type)   \n",
    "[**3.6.3 Structs Type**](#3.6.3-Structs-Type)   \n",
    "[**3.7 Handling Nulls**](#3.7-Handling-Nulls)   \n",
    "[**3.7.1 Droping Null Values**](#3.7.1-Droping-Null-Values)   \n",
    "[**3.7.2 Filling Null Values**](#3.7.2-Filling-Null-Values)   \n",
    "[**3.7.3 Filtering Null Values**](#3.7.3-Filtering-Null-Values)   \n",
    "[**3.9 User Defined Functions**](#3.8-User-Defined-Functions)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Filtering\n",
    "**Filtering**: Filtering is the process of subsetting data for analysis and reporting. Filter can be applied both on `rows` and `columns`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Filtering Column\n",
    "Filtering column is the process of reducing i.e. dropping or removing columns/attributes from originial DataFrame. Filtering row are used to:-      \n",
    "* remove sensitive fields from data.\n",
    "* remove less important fields.\n",
    "* remove temporary fields added during data transformation or validation.\n",
    "* reduce data size for fast processing and optimization.\n",
    "\n",
    "`drop()` method is used to drop the columns from DataFrame. The argument to the `drop()`method is either a single column name or list of column name to be dropped from DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read mysql database connection string from conf/db_properties.ini\n",
    "\n",
    "config_filename = '../Chapter_2_Structured_API/Lab_1/conf/db_properties.ini'\n",
    "db_properties = {}\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_filename)\n",
    "db_prop = config['mysql']\n",
    "db_url = db_prop['url']\n",
    "db_properties['database'] = db_prop['database']\n",
    "db_properties['schema'] = db_prop['schema']\n",
    "db_properties['user'] = db_prop['user']\n",
    "db_properties['password'] = db_prop['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Chapter 3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load current_dept_emp\n",
    "current_dept_emp = spark.read.jdbc(url = db_url, table = 'current_dept_emp', properties = db_properties)\n",
    "\n",
    "# Load departments\n",
    "departments = spark.read.jdbc(url = db_url, table = 'departments', properties = db_properties)\n",
    "\n",
    "# Load dept_emp\n",
    "dept_emp = spark.read.jdbc(url = db_url, table = 'dept_emp', properties = db_properties)\n",
    "\n",
    "# Load dept_emp_latest_date\n",
    "dept_emp_latest_date = spark.read.jdbc(url = db_url, table = 'dept_emp_latest_date', properties = db_properties)\n",
    "\n",
    "# Load dept_manager\n",
    "dept_manager = spark.read.jdbc(url = db_url, table = 'dept_manager', properties = db_properties)\n",
    "\n",
    "# Load employees\n",
    "employees = spark.read.jdbc(url = db_url, table = 'employees', properties = db_properties)\n",
    "\n",
    "# Load highest_salary_employee\n",
    "highest_salary_employee = spark.read.jdbc(url = db_url, table = 'highest_salary_employee', properties = db_properties)\n",
    "\n",
    "# Load salaries\n",
    "salaries = spark.read.jdbc(url = db_url, table = 'salaries', properties = db_properties)\n",
    "\n",
    "# Load titles\n",
    "titles = spark.read.jdbc(url = db_url, table = 'titles', properties = db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Single Column**\n",
    "\n",
    "To drop single column pass the column name to `drop()` method. The example below shows dropping emp_no from employees DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+----------+----------+---------+------+----------+\n",
      "|birth_date|first_name|last_name|gender|hire_date |\n",
      "+----------+----------+---------+------+----------+\n",
      "|1953-09-02|Georgi    |Facello  |M     |1986-06-26|\n",
      "|1964-06-02|Bezalel   |Simmel   |F     |1985-11-21|\n",
      "|1959-12-03|Parto     |Bamford  |M     |1986-08-28|\n",
      "|1954-05-01|Chirstian |Koblick  |M     |1986-12-01|\n",
      "|1955-01-21|Kyoichi   |Maliniak |M     |1989-09-12|\n",
      "|1953-04-20|Anneke    |Preusig  |F     |1989-06-02|\n",
      "|1957-05-23|Tzvetan   |Zielinski|F     |1989-02-10|\n",
      "|1958-02-19|Saniya    |Kalloufi |M     |1994-09-15|\n",
      "|1952-04-19|Sumant    |Peac     |F     |1985-02-18|\n",
      "|1963-06-01|Duangkaew |Piveteau |F     |1989-08-24|\n",
      "+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop emp_no from employees DF and store new value into emp_tmpDF.\n",
    "\n",
    "emp_tmpDF = employees.drop(\"emp_no\")\n",
    "emp_tmpDF.printSchema()\n",
    "emp_tmpDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Multiple Column**\n",
    "\n",
    "To drop multipe column pass the column names to `drop()` method. The example below shows dropping emp_no, birth_date, gender and hire_date from employees DF. The argument to drop() method can be either string of column names or list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n",
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|Georgi    |Facello  |\n",
      "|Bezalel   |Simmel   |\n",
      "|Parto     |Bamford  |\n",
      "|Chirstian |Koblick  |\n",
      "|Kyoichi   |Maliniak |\n",
      "|Anneke    |Preusig  |\n",
      "|Tzvetan   |Zielinski|\n",
      "|Saniya    |Kalloufi |\n",
      "|Sumant    |Peac     |\n",
      "|Duangkaew |Piveteau |\n",
      "+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Drop emp_no, birth_date, gender and hire_date from employees DF and store new value into emp_tmpDF.\n",
    "\n",
    "emp_tmpDF = employees.drop(\"emp_no\", \"birth_date\", \"gender\" ,\"hire_date\")\n",
    "emp_tmpDF.printSchema()\n",
    "emp_tmpDF.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+----------+---------+----------+\n",
      "|first_name|last_name|hire_date |\n",
      "+----------+---------+----------+\n",
      "|Georgi    |Facello  |1986-06-26|\n",
      "|Bezalel   |Simmel   |1985-11-21|\n",
      "|Parto     |Bamford  |1986-08-28|\n",
      "|Chirstian |Koblick  |1986-12-01|\n",
      "|Kyoichi   |Maliniak |1989-09-12|\n",
      "|Anneke    |Preusig  |1989-06-02|\n",
      "|Tzvetan   |Zielinski|1989-02-10|\n",
      "|Saniya    |Kalloufi |1994-09-15|\n",
      "|Sumant    |Peac     |1985-02-18|\n",
      "|Duangkaew |Piveteau |1989-08-24|\n",
      "+----------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Drop emp_no, birth_date, gender and hire_date from employees DF and store new value into emp_tmpDF.\n",
    "\n",
    "column_list = ['emp_no', 'birth_date', 'gender']\n",
    "emp_tmpDF = employees.drop(*column_list)\n",
    "emp_tmpDF.printSchema()\n",
    "emp_tmpDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Filtering Row\n",
    "Filtering row or record is the processing of filtering unwanted record(s) from DataFrame. Filtering row are used to:-   \n",
    "* eliminiate errorneous record.\n",
    "* eliminate duplicate record.\n",
    "* eliminate null ro empty record.\n",
    "* select record based on certain business rule or logic.\n",
    "* select record for particular groups of interest.\n",
    "* select record for particular period of time.\n",
    "\n",
    "Records are always filtered based on the boolean `true` or `false` value evaluated either from the single `column` or `column expressions` value. The column expression can be constructed from multiple columns containing assignment, arithmetic, bitwise, logical, comparison etc. operators combined with various list of function available in `pyspark.sql.functions`. The logical statements built from these expression will result boolean value where the record are filtered based on it.   \n",
    "\n",
    "`where()` and `filter()` method are used to filter records. The argument to these method are `column name`, `column expression` or `boolean statements`. Both method performs similiar task. We'll use `where()` in our entire session since it is easier to remember and similar to SQL clause.   \n",
    "\n",
    "Boolean expressions uses combination of boolean operation as shown in table below. We can also use Boolean column to filter the values from DataFrame. Boolean column is constructed using Boolean expression result in DataFrame. The example is shown below in *Filter with Boolean column* section.  \n",
    "\n",
    "Table 3.1.2 (a) Boolean Operation\n",
    "\n",
    "| Operator | Description |\n",
    "| --------- | ----------- |\n",
    "| `&` | And operation |\n",
    "| `-` | Or operation |\n",
    "| `!` | Not operation |\n",
    "\n",
    "\n",
    "**Note**: `and` filter is always chained together sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Filter single column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name  |gender|hire_date |\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello    |M     |1986-06-26|\n",
      "|10909 |1954-11-11|Georgi    |Atchley    |M     |1985-04-21|\n",
      "|11029 |1962-07-12|Georgi    |Itzfeldt   |M     |1992-12-27|\n",
      "|11430 |1957-01-23|Georgi    |Klassen    |M     |1996-02-27|\n",
      "|12157 |1960-03-30|Georgi    |Barinka    |M     |1985-06-04|\n",
      "|15220 |1957-08-03|Georgi    |Panienski  |F     |1995-07-23|\n",
      "|15660 |1956-01-13|Georgi    |Hartvigsen |M     |1994-10-13|\n",
      "|15689 |1962-09-14|Georgi    |Capobianchi|M     |1995-03-11|\n",
      "|15843 |1958-07-15|Georgi    |Varley     |M     |1987-04-14|\n",
      "|16672 |1955-04-25|Georgi    |Peris      |M     |1986-03-13|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter based on single column value\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "employees.where(col(\"first_name\") == \"Georgi\")\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name  |gender|hire_date |\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello    |M     |1986-06-26|\n",
      "|10909 |1954-11-11|Georgi    |Atchley    |M     |1985-04-21|\n",
      "|11029 |1962-07-12|Georgi    |Itzfeldt   |M     |1992-12-27|\n",
      "|11430 |1957-01-23|Georgi    |Klassen    |M     |1996-02-27|\n",
      "|12157 |1960-03-30|Georgi    |Barinka    |M     |1985-06-04|\n",
      "|15220 |1957-08-03|Georgi    |Panienski  |F     |1995-07-23|\n",
      "|15660 |1956-01-13|Georgi    |Hartvigsen |M     |1994-10-13|\n",
      "|15689 |1962-09-14|Georgi    |Capobianchi|M     |1995-03-11|\n",
      "|15843 |1958-07-15|Georgi    |Varley     |M     |1987-04-14|\n",
      "|16672 |1955-04-25|Georgi    |Peris      |M     |1986-03-13|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter based on single column value\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "employees.where(\"first_name == 'Georgi'\")\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Filter multiple column with AND operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello  |M     |1986-06-26|\n",
      "|55649 |1956-01-23|Georgi    |Facello  |M     |1988-05-04|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.where(\"first_name == 'Georgi'\")\\\n",
    "         .where(\"last_name == 'Facello'\")\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello  |M     |1986-06-26|\n",
      "|55649 |1956-01-23|Georgi    |Facello  |M     |1988-05-04|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#F.when(col(\"col-1\")>0.0) & (col(\"col-2\")>0.0), 1).otherwise(0)\n",
    "\n",
    "employees.where((col(\"first_name\") == 'Georgi') & (col(\"last_name\") == 'Facello'))\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello  |M     |1986-06-26|\n",
      "|55649 |1956-01-23|Georgi    |Facello  |M     |1988-05-04|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstName = col(\"first_name\") == \"Georgi\"\n",
    "lastName = col(\"last_name\") == \"Facello\"\n",
    "\n",
    "employees.where(firstName & lastName)\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5 Filter multiple column with OR operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+----------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name |gender|hire_date |\n",
      "+------+----------+----------+----------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello   |M     |1986-06-26|\n",
      "|10327 |1954-04-01|Roded     |Facello   |M     |1987-09-18|\n",
      "|10909 |1954-11-11|Georgi    |Atchley   |M     |1985-04-21|\n",
      "|11029 |1962-07-12|Georgi    |Itzfeldt  |M     |1992-12-27|\n",
      "|11430 |1957-01-23|Georgi    |Klassen   |M     |1996-02-27|\n",
      "|12157 |1960-03-30|Georgi    |Barinka   |M     |1985-06-04|\n",
      "|12751 |1964-07-06|Nahum     |Facello   |M     |1995-01-09|\n",
      "|15220 |1957-08-03|Georgi    |Panienski |F     |1995-07-23|\n",
      "|15346 |1959-09-26|Kirk      |Facello   |F     |1991-12-07|\n",
      "|15660 |1956-01-13|Georgi    |Hartvigsen|M     |1994-10-13|\n",
      "+------+----------+----------+----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.where((col(\"first_name\") == 'Georgi') | (col(\"last_name\") == 'Facello'))\\\n",
    "         .select(\"*\")\\\n",
    "         .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10327|1954-04-01|     Roded|  Facello|     M|1987-09-18|\n",
      "| 12751|1964-07-06|     Nahum|  Facello|     M|1995-01-09|\n",
      "| 15346|1959-09-26|      Kirk|  Facello|     F|1991-12-07|\n",
      "| 15685|1958-07-12|   Kasturi|  Facello|     M|1992-03-13|\n",
      "| 18686|1962-02-23| Kwangyoen|  Facello|     F|1985-05-02|\n",
      "| 19041|1957-05-29|    Billur|  Facello|     F|1992-08-03|\n",
      "| 21947|1954-06-18|   Taisook|  Facello|     F|1991-07-30|\n",
      "| 23938|1955-07-11|     Nahum|  Facello|     M|1985-09-15|\n",
      "| 24774|1956-09-23|       Uno|  Facello|     F|1989-11-09|\n",
      "| 24806|1959-09-30|  Charmane|  Facello|     F|1989-03-17|\n",
      "| 25955|1962-10-09| Christoph|  Facello|     M|1989-03-24|\n",
      "| 27732|1955-06-04|  Girolamo|  Facello|     M|1986-06-30|\n",
      "| 30320|1953-12-21|  Kristine|  Facello|     F|1990-06-10|\n",
      "| 31107|1962-10-13|       Fai|  Facello|     M|1994-07-11|\n",
      "| 32138|1955-04-13|    Anyuan|  Facello|     F|1988-10-04|\n",
      "| 32580|1953-04-25|      Kirk|  Facello|     M|1988-05-31|\n",
      "| 36371|1961-05-29| Cristinel|  Facello|     M|1986-10-14|\n",
      "| 37261|1957-11-06|    Zijian|  Facello|     F|1986-06-30|\n",
      "| 41667|1953-12-23|     Barna|  Facello|     F|1985-09-13|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "genderFilter = col(\"gender\") == 'M'\n",
    "ageFilter = year(employees.birth_date) <= 1969\n",
    "\n",
    "employees.where(employees.last_name.isin(\"Facello\")).where(genderFilter | ageFilter).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.6 Filter with Boolean expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------+------+----------+\n",
      "|first_name|  last_name|gender|emp_no|birth_date|\n",
      "+----------+-----------+------+------+----------+\n",
      "|    Georgi|    Facello|     M| 10001|1953-09-02|\n",
      "|     Parto|    Bamford|     M| 10003|1959-12-03|\n",
      "| Chirstian|    Koblick|     M| 10004|1954-05-01|\n",
      "|   Kyoichi|   Maliniak|     M| 10005|1955-01-21|\n",
      "|    Saniya|   Kalloufi|     M| 10008|1958-02-19|\n",
      "|  Patricio|  Bridgland|     M| 10012|1960-10-04|\n",
      "| Eberhardt|     Terkki|     M| 10013|1963-06-07|\n",
      "|     Berni|      Genin|     M| 10014|1956-02-12|\n",
      "|  Guoxiang|  Nooteboom|     M| 10015|1959-08-19|\n",
      "|  Kazuhito|Cappelletti|     M| 10016|1961-05-02|\n",
      "+----------+-----------+------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get male employee's first_name, last_name, gender, emp_no above 50 year old from employees DF\n",
    "\n",
    "from pyspark.sql.functions import datediff, current_date\n",
    "\n",
    "\n",
    "genderFilter = col(\"gender\") == 'M'\n",
    "ageFilter = datediff(current_date(), col(\"birth_date\")) > 50\n",
    "\n",
    "employees.withColumn(\"male50above\", genderFilter & ageFilter)\\\n",
    "         .where(\"male50above\")\\\n",
    "         .select(\"first_name\", \"last_name\", \"gender\", \"emp_no\", \"birth_date\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create more examples for string manipulation data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 PySpark SQL Module\n",
    "\n",
    "Always keep the image below on your memory and link in your browser bookmark. This helps you to solve problem faster by finding all the resources easily which is one-stop shop for all [Spark Python API Documents](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html).  \n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "\n",
    "\n",
    "![Pysark SQL Module Image](spark_sql_module.png)\n",
    "\n",
    "\n",
    "\n",
    "If you want to deep dive into Scala then use the link below (Optional for this Course). Copy and paste if link doesn't work.\n",
    "\n",
    "* **DataSet Functions**: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset    \n",
    "* **DataFrame and SQL Functions**: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions\\$\n",
    "* **DataFrameStatFunctions**: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions\n",
    "* **DataFrameNaFunctions**: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions     \n",
    "* **Column Methods**: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Numeric Type Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+-----------------------+\n",
      "|emp_no|salary| from_date|   to_date|salayr_with_5%_increase|\n",
      "+------+------+----------+----------+-----------------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|               63122.85|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|                65207.1|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|                69377.7|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|                69925.8|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|               70309.05|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|                74598.3|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|               78049.65|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|                79050.3|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|                79793.7|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|                80728.2|\n",
      "+------+------+----------+----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.select(\"*\", ((col(\"salary\") * 0.05) + col(\"salary\")).alias(\"salary_with_5%_increase\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+------------+\n",
      "|emp_no|salary| from_date|   to_date|bonus_salary|\n",
      "+------+------+----------+----------+------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|    63622.85|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|    65707.10|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|    69877.70|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|    70425.80|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|    70809.05|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|    75098.30|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|    78549.65|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|    79550.30|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|    80293.70|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|    81228.20|\n",
      "+------+------+----------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate new salary by increasing 5% and adding $500 commission with new fields 'bonus_salary' in salaries DF.\n",
    "\n",
    "salaries.selectExpr(\"*\", \"salary * 0.05 + 500 + salary as bonus_salary\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+------------+\n",
      "|emp_no|salary| from_date|   to_date|bonus_salary|\n",
      "+------+------+----------+----------+------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|    63622.85|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|     65707.1|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|     69877.7|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|     70425.8|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|    70809.05|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|     75098.3|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|    78549.65|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|     79550.3|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|     80293.7|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|     81228.2|\n",
      "+------+------+----------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate new salary by increasing 5% and adding $500 commission with new fields 'bonus_salary' in salaries DF.\n",
    "\n",
    "bonusSalary = (col(\"salary\") * 0.05) + 500 + col(\"salary\")\n",
    "salaries.select(\"*\", bonusSalary.alias(\"bonus_salary\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|employee_age_in_months|\n",
      "+----------------------+\n",
      "|                 796.0|\n",
      "|                 667.0|\n",
      "|                 721.0|\n",
      "|                 788.0|\n",
      "|                 779.0|\n",
      "|                 800.0|\n",
      "|                 751.0|\n",
      "|                 742.0|\n",
      "|                 812.0|\n",
      "|                 679.0|\n",
      "+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the rounded age of employees calculated in month.\n",
    "\n",
    "from pyspark.sql.functions import months_between, round\n",
    "\n",
    "employees.selectExpr(\"round(months_between(current_date(), birth_date)) as age_in_months\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+------------+\n",
      "|salary|bonus_salary|ceil_salary|floor_salary|\n",
      "+------+------------+-----------+------------+\n",
      "| 60117|    63122.85|      63123|       63122|\n",
      "| 62102|     65207.1|      65208|       65207|\n",
      "| 66074|     69377.7|      69378|       69377|\n",
      "| 66596|     69925.8|      69926|       69925|\n",
      "| 66961|    70309.05|      70310|       70309|\n",
      "| 71046|     74598.3|      74599|       74598|\n",
      "| 74333|    78049.65|      78050|       78049|\n",
      "| 75286|     79050.3|      79051|       79050|\n",
      "| 75994|     79793.7|      79794|       79793|\n",
      "| 76884|     80728.2|      80729|       80728|\n",
      "+------+------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcuate the floor and ceiling value for bonus_salary.\n",
    "\n",
    "from pyspark.sql.functions import ceil, floor\n",
    "\n",
    "bonus_salary = col(\"salary\") + col(\"salary\") * 0.05\n",
    "salaries.select(\"salary\", bonus_salary.alias(\"bonus_salary\"), ceil(bonus_salary).alias(\"ceil_salary\")\\\n",
    "                , floor(bonus_salary).alias(\"floor_salary\")).show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 String Type Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----------------+\n",
      "|first_name|last_name|user_name|length(last_name)|\n",
      "+----------+---------+---------+-----------------+\n",
      "|    Georgi|  Facello|     gefa|                7|\n",
      "|   Bezalel|   Simmel|     besi|                6|\n",
      "|     Parto|  Bamford|     paba|                7|\n",
      "| Chirstian|  Koblick|     chko|                7|\n",
      "|   Kyoichi| Maliniak|     kyma|                8|\n",
      "|    Anneke|  Preusig|     anpr|                7|\n",
      "|   Tzvetan|Zielinski|     tzzi|                9|\n",
      "|    Saniya| Kalloufi|     saka|                8|\n",
      "|    Sumant|     Peac|     supe|                4|\n",
      "| Duangkaew| Piveteau|     dupi|                8|\n",
      "+----------+---------+---------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first two character from lastname followed by all character from firstname.\n",
    "# Convert all the character in lower case with column name 'user_name' from employees DF.\n",
    "\n",
    "# The link below provide the solution for using last name length. Try and fix the problem.\n",
    "#https://stackoverflow.com/questions/51140470/using-a-column-value-as-a-parameter-to-a-spark-dataframe-function\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import length, substring, lower, concat, expr\n",
    "import pandas as pd\n",
    "\n",
    "fname_max_length = 2\n",
    "lname_max_length = length(\"last_name\")\n",
    "substrFname = substring(\"first_name\", 0, fname_max_length)\n",
    "substrLname = substring(\"last_name\", 0, fname_max_length)  # Why can't we use lname_max_length? Try to find solution.\n",
    "concatNameLcase = lower(concat(substrFname, substrLname))\n",
    "employees.withColumn(\"user_name\", concatNameLcase).select(\"first_name\", \"last_name\", \"user_name\",\\\n",
    "                    lname_max_length).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "|first_name|last_name|  user_name|\n",
      "+----------+---------+-----------+\n",
      "|    Georgi|  Facello|  gefacello|\n",
      "|   Bezalel|   Simmel|   besimmel|\n",
      "|     Parto|  Bamford|  pabamford|\n",
      "| Chirstian|  Koblick|  chkoblick|\n",
      "|   Kyoichi| Maliniak| kymaliniak|\n",
      "|    Anneke|  Preusig|  anpreusig|\n",
      "|   Tzvetan|Zielinski|tzzielinski|\n",
      "|    Saniya| Kalloufi| sakalloufi|\n",
      "|    Sumant|     Peac|     supeac|\n",
      "| Duangkaew| Piveteau| dupiveteau|\n",
      "+----------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"first_name\", \"last_name\",\\\n",
    "          expr(\"lower(concat(substring(first_name, 0, 2), substring(last_name, 0, length(last_name)))) as user_name\")\\\n",
    "          ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|birth_date|   emp_dob|birth_mm_dd|\n",
      "+----------+----------+-----------+\n",
      "|1953-09-02|0000-00-00|      09-02|\n",
      "|1964-06-02|0000-00-00|      06-02|\n",
      "|1959-12-03|0000-00-00|      12-03|\n",
      "|1954-05-01|0000-00-00|      05-01|\n",
      "|1955-01-21|0000-00-00|      01-21|\n",
      "|1953-04-20|0000-00-00|      04-20|\n",
      "|1957-05-23|0000-00-00|      05-23|\n",
      "|1958-02-19|0000-00-00|      02-19|\n",
      "|1952-04-19|0000-00-00|      04-19|\n",
      "|1963-06-01|0000-00-00|      06-01|\n",
      "+----------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace employees birth_date with '0' but '-' alias with emp_dob.\n",
    "# Remove year from birth_date and rename column with birth_mm_dd\n",
    "# date_pattern = \"^\\d{4}-\\d{2}-\\d{2}$\"\n",
    "# Learn more about regular expression aka regex \n",
    "# https://www.rexegg.com/regex-quickstart.html\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "\n",
    "\n",
    "employees.select(\"birth_date\", regexp_replace(col(\"birth_date\"), '\\d', '0')\\\n",
    "                 .alias(\"emp_dob\"),\\\n",
    "                 regexp_replace(col(\"birth_date\"), \"^\\d{4}-\" , '')\\\n",
    "                 .alias(\"birth_mm_dd\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "| first_name|isSr|\n",
      "+-----------+----+\n",
      "|  Sreenivas|   1|\n",
      "|   Srinidhi|   1|\n",
      "|Sreekrishna|   1|\n",
      "+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show distinct records where employee first name contains 'Sr'\n",
    "\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "employees.selectExpr(\"first_name\", \"instr(first_name, 'Sr') as isSr\")\\\n",
    "            .where(\"isSr == 1\")\\\n",
    "            .distinct()\\\n",
    "            .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+----------------------+------+----------+----------+---------+------+----------+\n",
      "|instr(first_name, sri)|instr(first_name, geo)|instr(first_name, par)|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+----------------------+----------------------+----------------------+------+----------+----------+---------+------+----------+\n",
      "|                     0|                     0|                     0| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "|                     0|                     0|                     0| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "|                     0|                     0|                     0| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "|                     0|                     0|                     0| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "|                     0|                     0|                     0| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "|                     0|                     0|                     0| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "|                     0|                     0|                     0| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "|                     0|                     0|                     0| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "|                     0|                     0|                     0| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "|                     0|                     0|                     0| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+----------------------+----------------------+----------------------+------+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+\n",
      "|first_name|\n",
      "+----------+\n",
      "|    Georgi|\n",
      "|   Bezalel|\n",
      "|     Parto|\n",
      "| Chirstian|\n",
      "|   Kyoichi|\n",
      "|    Anneke|\n",
      "|   Tzvetan|\n",
      "|    Saniya|\n",
      "|    Sumant|\n",
      "| Duangkaew|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display employee first name that contains 'Sri, Moh, Zeh'\n",
    "# @ todo validate the result it not working as expected.\n",
    "\n",
    "from pyspark.sql.functions import instr, expr\n",
    "\n",
    "nameLike = [\"sri\", \"geo\", \"par\"]\n",
    "def name_checker(name, nameLike):\n",
    "    return instr(name, nameLike)\n",
    "\n",
    "nameContains = [name_checker(employees.first_name, n) for n in nameLike]\n",
    "nameContains.append(expr(\"*\"))  # append column\n",
    "employees.select(*nameContains).show(10)\n",
    "employees.select(*nameContains).select(\"first_name\").show(10) \n",
    "        #.where(\"instr\\(first_name\\, sri\\)\" == 1)\\  Apply filter on new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Date and Timestamp Type Manipulation\n",
    "\n",
    "Date and Timestamps plays crucial role during data modeling and consider as important attribute for tracking information. The format of date and timestamps should must be specified correctly for any database and proramming languages. While reading the schema from file, date data types can be considered as string and later converted to respective date type. Since all the application has its own date type formatting, treating string is better approach during schema-on-read.   \n",
    "* Date: stores only calendar date. Default format is `yyyy-mm-dd`.\n",
    "* Timestamps: stores date and time. Spark only supports seconds precision. While handling milliseconds and microseconds it need to treated as `longs`. Spark uses Java dates and timestamps formatting underneath. Default format is `yyyy-mm-dd hh:mm:ss`.\n",
    "\n",
    "To use own date formatting style refer to [Java SimpleDateFormat API](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------------+-----------------------+---------------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |current_date|current_timestamp      |current_date_str_type|\n",
      "+------+----------+----------+---------+------+----------+------------+-----------------------+---------------------+\n",
      "|10001 |1953-09-02|Georgi    |Facello  |M     |1986-06-26|2019-12-22  |2019-12-22 18:52:14.398|2019-12-22           |\n",
      "|10002 |1964-06-02|Bezalel   |Simmel   |F     |1985-11-21|2019-12-22  |2019-12-22 18:52:14.398|2019-12-22           |\n",
      "|10003 |1959-12-03|Parto     |Bamford  |M     |1986-08-28|2019-12-22  |2019-12-22 18:52:14.398|2019-12-22           |\n",
      "|10004 |1954-05-01|Chirstian |Koblick  |M     |1986-12-01|2019-12-22  |2019-12-22 18:52:14.398|2019-12-22           |\n",
      "|10005 |1955-01-21|Kyoichi   |Maliniak |M     |1989-09-12|2019-12-22  |2019-12-22 18:52:14.398|2019-12-22           |\n",
      "+------+----------+----------+---------+------+----------+------------+-----------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new tmp_emp DF by adding current date and timestamp from employees DF.\n",
    "\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "tmp_emp = employees.withColumn(\"current_date\", current_date())\\\n",
    "         .withColumn(\"current_timestamp\", current_timestamp())\\\n",
    "         .withColumn(\"current_date_str_type\", current_date().cast(\"string\"))\n",
    "tmp_emp.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- current_date: date (nullable = false)\n",
      " |-- current_timestamp: timestamp (nullable = false)\n",
      " |-- current_date_str_type: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_emp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_date()` is used to convert string date to date type. If the string format doesn't matches with specified date type format then it will return `null` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- str_curr_dt: date (nullable = true)\n",
      "\n",
      "+------+-----------+\n",
      "|emp_no|str_curr_dt|\n",
      "+------+-----------+\n",
      "| 10001| 2019-12-22|\n",
      "| 10002| 2019-12-22|\n",
      "| 10003| 2019-12-22|\n",
      "| 10004| 2019-12-22|\n",
      "| 10005| 2019-12-22|\n",
      "| 10006| 2019-12-22|\n",
      "| 10007| 2019-12-22|\n",
      "| 10008| 2019-12-22|\n",
      "| 10009| 2019-12-22|\n",
      "| 10010| 2019-12-22|\n",
      "+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert current_date_str_type column that was stored as string type into date type.\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "empid_hire_dt = tmp_emp.select(\"emp_no\", to_date(\"current_date_str_type\").alias(\"str_curr_dt\"))            \n",
    "empid_hire_dt.printSchema()\n",
    "empid_hire_dt.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2019-12-20')|to_date('2019-20-20')|\n",
      "+---------------------+---------------------+\n",
      "|           2019-12-20|                 null|\n",
      "|           2019-12-20|                 null|\n",
      "|           2019-12-20|                 null|\n",
      "|           2019-12-20|                 null|\n",
      "|           2019-12-20|                 null|\n",
      "+---------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert string type to date type when date format is wrong\n",
    "\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "#2019-20-20 is not a valid date so it will return null value.\n",
    "\n",
    "employees.select(to_date(lit(\"2019-12-20\")), to_date(lit(\"2019-20-20\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------------------+\n",
      "| hire_date|date_add(hire_date, 30)|date_sub(hire_date, 30)|\n",
      "+----------+-----------------------+-----------------------+\n",
      "|1986-06-26|             1986-07-26|             1986-05-27|\n",
      "|1985-11-21|             1985-12-21|             1985-10-22|\n",
      "|1986-08-28|             1986-09-27|             1986-07-29|\n",
      "|1986-12-01|             1986-12-31|             1986-11-01|\n",
      "|1989-09-12|             1989-10-12|             1989-08-13|\n",
      "|1989-06-02|             1989-07-02|             1989-05-03|\n",
      "|1989-02-10|             1989-03-12|             1989-01-11|\n",
      "|1994-09-15|             1994-10-15|             1994-08-16|\n",
      "|1985-02-18|             1985-03-20|             1985-01-19|\n",
      "|1989-08-24|             1989-09-23|             1989-07-25|\n",
      "+----------+-----------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add and subtract 30 days on hire date for all employees.\n",
    "\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "employees.select(\"hire_date\", date_add(col(\"hire_date\"), 30), date_sub(col(\"hire_date\"), 30)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "| hire_date|today_hire_day|\n",
      "+----------+--------------+\n",
      "|1986-06-26|         12232|\n",
      "|1985-11-21|         12449|\n",
      "|1986-08-28|         12169|\n",
      "|1986-12-01|         12074|\n",
      "|1989-09-12|         11058|\n",
      "+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate days difference from hire data till now.\n",
    "\n",
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "employees.select(\"hire_date\", datediff(current_date(), \"hire_date\").alias(\"today_hire_day\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|month_employed|\n",
      "+--------------+\n",
      "|  401.87096774|\n",
      "|  409.03225806|\n",
      "|  399.80645161|\n",
      "|  396.67741935|\n",
      "|  363.32258065|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+\n",
      "|month_employed|\n",
      "+--------------+\n",
      "| -401.87096774|\n",
      "| -409.03225806|\n",
      "| -399.80645161|\n",
      "| -396.67741935|\n",
      "| -363.32258065|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total month of employees hire till now\n",
    "\n",
    "from pyspark.sql.functions import months_between\n",
    "\n",
    "employees.select(months_between(current_date(), \"hire_date\").alias(\"month_employed\")).show(5) # order always matter. today in param 1\n",
    "employees.select(months_between(\"hire_date\", current_date()).alias(\"month_employed\")).show(5) # order always matter. today in param 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying own date formatting style using SimpleDateFormat. Refer to previous link show above for different formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|correct_date|incorrect_date|\n",
      "+------------+--------------+\n",
      "|  2019-12-20|          null|\n",
      "|  2019-12-20|          null|\n",
      "|  2019-12-20|          null|\n",
      "|  2019-12-20|          null|\n",
      "|  2019-12-20|          null|\n",
      "+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "ownDateFormat = \"yyyy-MM-dd\"\n",
    "employees.select(to_date(lit(\"2019-12-20\"), ownDateFormat).alias(\"correct_date\"), # correct date format\n",
    "                 to_date(lit(\"2019-20-20\"), ownDateFormat).alias(\"incorrect_date\")  # incorrect date format, month 20 doesn't exist\n",
    "                ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_timestamp()` is used to convert string date to timestamp type. If the string format doesn't matches with specified date type format then it will return `null` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "| hire_date|      hire_datetime|\n",
      "+----------+-------------------+\n",
      "|1986-06-26|1986-06-26 00:00:00|\n",
      "|1985-11-21|1985-11-21 00:00:00|\n",
      "|1986-08-28|1986-08-28 00:00:00|\n",
      "|1986-12-01|1986-12-01 00:00:00|\n",
      "|1989-09-12|1989-09-12 00:00:00|\n",
      "+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert hire date to timestamp type\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "ownDateFormat = \"yyyy-dd-MM\"\n",
    "employees.select(\"hire_date\",\n",
    "                 to_timestamp(col(\"hire_date\"), ownDateFormat).alias(\"hire_datetime\") # hours, mins and second is added.\n",
    "                ).show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical operators can be used to compare difference between two dates. String literal as date value can also be used while comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+---------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date|\n",
      "+------+----------+----------+---------+------+---------+\n",
      "+------+----------+----------+---------+------+---------+\n",
      "\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare between two dates using logical operator to filter the values.\n",
    "\n",
    "ownDateFormat = \"yyyy-dd-MM\"\n",
    "employees.where(col(\"hire_date\") > current_date()).show(10) # filter if hire_date is greater than today.\n",
    "employees.where(col(\"hire_date\") < current_date()).show(10) # filter if hire_date is less than today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+---------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date|\n",
      "+------+----------+----------+---------+------+---------+\n",
      "+------+----------+----------+---------+------+---------+\n",
      "\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|\n",
      "| 10012|1960-10-04|  Patricio|Bridgland|     M|1992-12-18|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|\n",
      "| 10012|1960-10-04|  Patricio|Bridgland|     M|1992-12-18|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare between two dates using string literal for date type with logical operator to filter the values.\n",
    "\n",
    "# display employee hired in 2019-09-15\n",
    "employees.where(col(\"hire_date\") > \"2019-09-15\").show(10)\n",
    "\n",
    "# display employee hired from jan 1st 1990.\n",
    "employees.where(col(\"hire_date\") >= \"1990-01-01\").show(3) \n",
    "\n",
    "# display employee hired between 1980-01-01 to 1990-01-01\n",
    "employees.where(col(\"hire_date\") >= \"1990-01-01\")\\\n",
    "    .where(col(\"hire_date\") >= \"1990-01-01\")\\\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+----------+------+----------+\n",
      "|emp_no|birth_date|first_name| last_name|gender| hire_date|\n",
      "+------+----------+----------+----------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|   Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|    Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|   Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|   Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi|  Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|   Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan| Zielinski|     F|1989-02-10|\n",
      "| 10009|1952-04-19|    Sumant|      Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew|  Piveteau|     F|1989-08-24|\n",
      "| 10013|1963-06-07| Eberhardt|    Terkki|     M|1985-10-20|\n",
      "| 10014|1956-02-12|     Berni|     Genin|     M|1987-03-11|\n",
      "| 10015|1959-08-19|  Guoxiang| Nooteboom|     M|1987-07-02|\n",
      "| 10018|1954-06-19|  Kazuhide|      Peha|     F|1987-04-03|\n",
      "| 10021|1960-02-20|     Ramzi|      Erde|     M|1988-02-10|\n",
      "| 10023|1953-09-29|     Bojan|Montemayor|     F|1989-12-17|\n",
      "| 10025|1958-10-31| Prasadram|    Heyers|     M|1987-08-17|\n",
      "| 10027|1962-07-10|    Divier|   Reistad|     F|1989-07-07|\n",
      "| 10029|1956-12-13|     Otmar|    Herbst|     M|1985-11-20|\n",
      "| 10033|1956-11-14|      Arif|     Merlo|     M|1987-03-18|\n",
      "| 10034|1962-12-29|     Bader|      Swan|     M|1988-09-21|\n",
      "+------+----------+----------+----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display employee hired between 1980-01-01 to 1990-01-01 using between method\n",
    "\n",
    "from pyspark.sql.functions import asc\n",
    "\n",
    "employees.where(employees.hire_date.between(\"1980-01-01\", \"1990-01-01\"))\\\n",
    "               .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Complex Type Manipulation\n",
    "\n",
    "Complex types includes arrays, maps and structs. Comparing to Python type, it is list, dict, and list-of-list. Data can be stored in complex types as a column value in Spark. For example, we can store json representation in map that stores all the attributes in single column value. The most important aspect is retrieving the data for complex types. We'll use some examples below to for each types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1 Arrays Type  \n",
    "\n",
    "Step 1: Data preparation: Let's create a new array type column in empDF from employees DF. We'll choose hire_date and split based on '-'. If we already have array type then ignore this step.   \n",
    "Step 2: We'll apply some functions to get result from array type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "| hire_date|   create_date|\n",
      "+----------+--------------+\n",
      "|1986-06-26|[1986, 06, 26]|\n",
      "|1985-11-21|[1985, 11, 21]|\n",
      "|1986-08-28|[1986, 08, 28]|\n",
      "|1986-12-01|[1986, 12, 01]|\n",
      "|1989-09-12|[1989, 09, 12]|\n",
      "|1989-06-02|[1989, 06, 02]|\n",
      "|1989-02-10|[1989, 02, 10]|\n",
      "|1994-09-15|[1994, 09, 15]|\n",
      "|1985-02-18|[1985, 02, 18]|\n",
      "|1989-08-24|[1989, 08, 24]|\n",
      "+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- create_date: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new Column with Array type \n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# create empDF DataFrame with create_date attribute\n",
    "empDF = employees.select(\"hire_date\", split(col(\"hire_date\"), \"-\").alias(\"create_date\"))\n",
    "empDF.show(10)\n",
    "empDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|create_date[0]|\n",
      "+--------------+\n",
      "|          1986|\n",
      "|          1985|\n",
      "|          1986|\n",
      "|          1986|\n",
      "|          1989|\n",
      "|          1989|\n",
      "|          1989|\n",
      "|          1994|\n",
      "|          1985|\n",
      "|          1989|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get first value from create_date\n",
    "\n",
    "empDF.selectExpr(\"create_date[0]\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|size(create_date)|\n",
      "+-----------------+\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "|                3|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the size of create_date\n",
    "\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "empDF.select(size(\"create_date\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+------+\n",
      "|   create_date|is2019|is1989|\n",
      "+--------------+------+------+\n",
      "|[1986, 06, 26]| false| false|\n",
      "|[1985, 11, 21]| false| false|\n",
      "|[1986, 08, 28]| false| false|\n",
      "|[1986, 12, 01]| false| false|\n",
      "|[1989, 09, 12]| false|  true|\n",
      "|[1989, 06, 02]| false|  true|\n",
      "|[1989, 02, 10]| false|  true|\n",
      "|[1994, 09, 15]| false| false|\n",
      "|[1985, 02, 18]| false| false|\n",
      "|[1989, 08, 24]| false|  true|\n",
      "|[1990, 01, 22]| false| false|\n",
      "|[1992, 12, 18]| false| false|\n",
      "|[1985, 10, 20]| false| false|\n",
      "|[1987, 03, 11]| false| false|\n",
      "|[1987, 07, 02]| false| false|\n",
      "|[1995, 01, 27]| false| false|\n",
      "|[1993, 08, 03]| false| false|\n",
      "|[1987, 04, 03]| false| false|\n",
      "|[1999, 04, 30]| false| false|\n",
      "|[1991, 01, 26]| false| false|\n",
      "+--------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if create_date contains certain value\n",
    "\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "empDF.select(\"create_date\",array_contains(\"create_date\", '2019').alias(\"is2019\"),\\\n",
    "      array_contains(\"create_date\", '1989').alias(\"is1989\"))\\\n",
    "     .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|   create_date|valueExploded|\n",
      "+--------------+-------------+\n",
      "|[1986, 06, 26]|         1986|\n",
      "|[1986, 06, 26]|           06|\n",
      "|[1986, 06, 26]|           26|\n",
      "|[1985, 11, 21]|         1985|\n",
      "|[1985, 11, 21]|           11|\n",
      "|[1985, 11, 21]|           21|\n",
      "|[1986, 08, 28]|         1986|\n",
      "|[1986, 08, 28]|           08|\n",
      "|[1986, 08, 28]|           28|\n",
      "|[1986, 12, 01]|         1986|\n",
      "+--------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split create_date into several record for each value contained in create_date column value.\n",
    "# To learn more about explode function check the link below used in hive function. \n",
    "# https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "empDF.select(\"create_date\", explode(\"create_date\").alias(\"valueExploded\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 Maps Type\n",
    "\n",
    "Map is similar to dictionary in Python.\n",
    "\n",
    "Step 1: Data preparation: Let's create a new map type column in empDF from employees DF. We'll choose emp_no, and first name as it's value. If we already have map type then ignore this step.   \n",
    "Step 2: We'll apply some functions to get result from map type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+\n",
      "|emp_no|first_name|              empMap|\n",
      "+------+----------+--------------------+\n",
      "| 10001|    Georgi|   [10001 -> Georgi]|\n",
      "| 10002|   Bezalel|  [10002 -> Bezalel]|\n",
      "| 10003|     Parto|    [10003 -> Parto]|\n",
      "| 10004| Chirstian|[10004 -> Chirstian]|\n",
      "| 10005|   Kyoichi|  [10005 -> Kyoichi]|\n",
      "| 10006|    Anneke|   [10006 -> Anneke]|\n",
      "| 10007|   Tzvetan|  [10007 -> Tzvetan]|\n",
      "| 10008|    Saniya|   [10008 -> Saniya]|\n",
      "| 10009|    Sumant|   [10009 -> Sumant]|\n",
      "| 10010| Duangkaew|[10010 -> Duangkaew]|\n",
      "+------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- empMap: map (nullable = false)\n",
      " |    |-- key: integer\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Map Column \n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# create empDF DataFrame with create_date attribute\n",
    "empDF = employees.select(\"emp_no\", \"first_name\", create_map(col(\"emp_no\"), col(\"first_name\")).alias(\"empMap\"))\n",
    "empDF.show(10)\n",
    "empDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------+\n",
      "|emp_no|              empMap|empMap[10001]|\n",
      "+------+--------------------+-------------+\n",
      "| 10001|   [10001 -> Georgi]|       Georgi|\n",
      "| 10002|  [10002 -> Bezalel]|         null|\n",
      "| 10003|    [10003 -> Parto]|         null|\n",
      "| 10004|[10004 -> Chirstian]|         null|\n",
      "| 10005|  [10005 -> Kyoichi]|         null|\n",
      "| 10006|   [10006 -> Anneke]|         null|\n",
      "| 10007|  [10007 -> Tzvetan]|         null|\n",
      "| 10008|   [10008 -> Saniya]|         null|\n",
      "| 10009|   [10009 -> Sumant]|         null|\n",
      "| 10010|[10010 -> Duangkaew]|         null|\n",
      "+------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access value from map column through its key.\n",
    "\n",
    "empDF.selectExpr(\"emp_no\", \"empMap\", \"empMap[10001]\").show(10) # since the key is integer we used without quote but quote can also be used.\n",
    "\n",
    "# If the key is not present for record value then it will give null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+\n",
      "|emp_no|  key|    value|\n",
      "+------+-----+---------+\n",
      "| 10001|10001|   Georgi|\n",
      "| 10002|10002|  Bezalel|\n",
      "| 10003|10003|    Parto|\n",
      "| 10004|10004|Chirstian|\n",
      "| 10005|10005|  Kyoichi|\n",
      "| 10006|10006|   Anneke|\n",
      "| 10007|10007|  Tzvetan|\n",
      "| 10008|10008|   Saniya|\n",
      "| 10009|10009|   Sumant|\n",
      "| 10010|10010|Duangkaew|\n",
      "+------+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode map column with new key and value column.\n",
    "\n",
    "empDF.selectExpr(\"emp_no\", \"explode(empMap)\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3 Structs Type   \n",
    "\n",
    "Struct can be consider as DataFrames of DataFrames. Struct type can be created by putting column names into parenthesis. ie. like declaring tuples in Python. e.g. `struct(column_1, column_2)`\n",
    "\n",
    "Step 1: Data preparation: Let's create a new struct type column in empDF from employees DF. We'll choose first_name, and last_name as it's value. If we already have struct type then ignore this step.   \n",
    "Step 2: We'll apply some functions to get result from struct type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           empStruct|\n",
      "+--------------------+\n",
      "|   [Georgi, Facello]|\n",
      "|   [Bezalel, Simmel]|\n",
      "|    [Parto, Bamford]|\n",
      "|[Chirstian, Koblick]|\n",
      "| [Kyoichi, Maliniak]|\n",
      "|   [Anneke, Preusig]|\n",
      "|[Tzvetan, Zielinski]|\n",
      "|  [Saniya, Kalloufi]|\n",
      "|      [Sumant, Peac]|\n",
      "|[Duangkaew, Pivet...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- empStruct: struct (nullable = false)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Struct Column \n",
    "\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "empDF = employees.select(struct(\"first_name\", \"last_name\").alias(\"empStruct\"))\n",
    "empDF.show(10)\n",
    "empDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|           empStruct|first_name|last_name|\n",
      "+--------------------+----------+---------+\n",
      "|   [Georgi, Facello]|    Georgi|  Facello|\n",
      "|   [Bezalel, Simmel]|   Bezalel|   Simmel|\n",
      "|    [Parto, Bamford]|     Parto|  Bamford|\n",
      "|[Chirstian, Koblick]| Chirstian|  Koblick|\n",
      "| [Kyoichi, Maliniak]|   Kyoichi| Maliniak|\n",
      "|   [Anneke, Preusig]|    Anneke|  Preusig|\n",
      "|[Tzvetan, Zielinski]|   Tzvetan|Zielinski|\n",
      "|  [Saniya, Kalloufi]|    Saniya| Kalloufi|\n",
      "|      [Sumant, Peac]|    Sumant|     Peac|\n",
      "|[Duangkaew, Pivet...| Duangkaew| Piveteau|\n",
      "+--------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get value from Struct through column name\n",
    "\n",
    "empDF.select(\"empStruct\",\\\n",
    "             \"empStruct.first_name\",\\\n",
    "             \"empStruct.last_name\")\\\n",
    "             .show(10) # show first_name and last_name value from struct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           empStruct|empStruct.first_name|empStruct.first_name|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|   [Georgi, Facello]|              Georgi|              Georgi|\n",
      "|   [Bezalel, Simmel]|             Bezalel|             Bezalel|\n",
      "|    [Parto, Bamford]|               Parto|               Parto|\n",
      "|[Chirstian, Koblick]|           Chirstian|           Chirstian|\n",
      "| [Kyoichi, Maliniak]|             Kyoichi|             Kyoichi|\n",
      "|   [Anneke, Preusig]|              Anneke|              Anneke|\n",
      "|[Tzvetan, Zielinski]|             Tzvetan|             Tzvetan|\n",
      "|  [Saniya, Kalloufi]|              Saniya|              Saniya|\n",
      "|      [Sumant, Peac]|              Sumant|              Sumant|\n",
      "|[Duangkaew, Pivet...|           Duangkaew|           Duangkaew|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get value from Struct through column name \n",
    "\n",
    "# It uses getField method\n",
    "\n",
    "empDF.select(\"empStruct\",\\\n",
    "            col(\"empStruct\").getField(\"first_name\"),\\\n",
    "            col(\"empStruct\").getField(\"first_name\"))\\\n",
    "            .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|           empStruct|first_name|last_name|\n",
      "+--------------------+----------+---------+\n",
      "|   [Georgi, Facello]|    Georgi|  Facello|\n",
      "|   [Bezalel, Simmel]|   Bezalel|   Simmel|\n",
      "|    [Parto, Bamford]|     Parto|  Bamford|\n",
      "|[Chirstian, Koblick]| Chirstian|  Koblick|\n",
      "| [Kyoichi, Maliniak]|   Kyoichi| Maliniak|\n",
      "|   [Anneke, Preusig]|    Anneke|  Preusig|\n",
      "|[Tzvetan, Zielinski]|   Tzvetan|Zielinski|\n",
      "|  [Saniya, Kalloufi]|    Saniya| Kalloufi|\n",
      "|      [Sumant, Peac]|    Sumant|     Peac|\n",
      "|[Duangkaew, Pivet...| Duangkaew| Piveteau|\n",
      "+--------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get get all Struct values using '*'\n",
    "\n",
    "empDF.selectExpr(\"empStruct\",\n",
    "    \"empStruct.*\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Handling Nulls\n",
    "\n",
    "Null values always plays vital role in all programming language. When loading the data, if data value doesn't matches with defined schema then those are always display as null values.   \n",
    "\n",
    "Null values in Spark can be:-   \n",
    "* dropped explicitly\n",
    "* filled with some values. It can be replace globally or column-wise.\n",
    "\n",
    "Several functions can be used for handling null values. Such as `coalese(), ifnull(), nullIf(), nvl(), nvl2()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+\n",
      "| 10001|1953-09-02|    Georgi|    Facello|     M|1986-06-26|             66|     true|         Yes|\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|             55|    false|        null|\n",
      "| 10003|1959-12-03|     Parto|    Bamford|     M|1986-08-28|             60|     true|         Yes|\n",
      "| 10004|1954-05-01| Chirstian|    Koblick|     M|1986-12-01|             65|     true|         Yes|\n",
      "| 10005|1955-01-21|   Kyoichi|   Maliniak|     M|1989-09-12|             64|     true|         Yes|\n",
      "| 10006|1953-04-20|    Anneke|    Preusig|     F|1989-06-02|             66|     true|         Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|  Zielinski|     F|1989-02-10|             62|     true|         Yes|\n",
      "| 10008|1958-02-19|    Saniya|   Kalloufi|     M|1994-09-15|             61|     true|         Yes|\n",
      "| 10009|1952-04-19|    Sumant|       Peac|     F|1985-02-18|             67|     true|         Yes|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|             56|    false|        null|\n",
      "| 10011|1953-11-07|      Mary|      Sluis|     F|1990-01-22|             66|     true|         Yes|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|             59|    false|        null|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|             56|    false|        null|\n",
      "| 10014|1956-02-12|     Berni|      Genin|     M|1987-03-11|             63|     true|         Yes|\n",
      "| 10015|1959-08-19|  Guoxiang|  Nooteboom|     M|1987-07-02|             60|     true|         Yes|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|             58|    false|        null|\n",
      "| 10017|1958-07-06| Cristinel|  Bouloucos|     F|1993-08-03|             61|     true|         Yes|\n",
      "| 10018|1954-06-19|  Kazuhide|       Peha|     F|1987-04-03|             65|     true|         Yes|\n",
      "| 10019|1953-01-23|   Lillian|    Haddadi|     M|1999-04-30|             66|     true|         Yes|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|             66|     true|         Yes|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             66|     true|         Yes|            Yes|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|             55|    false|        null|          false|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             65|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             64|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             66|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             62|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             61|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             67|     true|         Yes|            Yes|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|             56|    false|        null|          false|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             66|     true|         Yes|            Yes|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|             55|    false|        null|             No|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             65|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             64|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             66|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             62|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             61|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             67|     true|         Yes|            Yes|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|             56|    false|        null|             No|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if first columns value is empty or null, if it is empty then retrieve value from second column or put literal value.\n",
    "\n",
    "from pyspark.sql.functions import coalesce, months_between, floor, lit\n",
    "\n",
    "# Create new DataFrame tmpDF that contain all the columns from employees and adding new columns \"age_above_50\"\n",
    "# column 'age_above_60' is calculated field that stores whether employees age is above 60 or not.\n",
    "employees.printSchema()\n",
    "\n",
    "# assign value\n",
    "total_month = 12\n",
    "filter_age = 60\n",
    "\n",
    "# emp_current_age stores employee current age\n",
    "# isabove50 stores boolean value whether or not age is 50\n",
    "# age_above_60 stores Yes for above age 60 else null\n",
    "\n",
    "tmpDF = employees.withColumn(\"emp_current_age\",\\\n",
    "                  floor(months_between(current_date(), \"birth_date\")/total_month))\\\n",
    "                  .withColumn(\"isabove50\", col(\"emp_current_age\") >= filter_age)\\\n",
    "                  .withColumn(\"age_above_60\", expr(\"case when isabove50 then 'Yes' else null end\"))                             \n",
    "tmpDF.show(20)\n",
    "\n",
    "# coalese to filter the null values with other column value\n",
    "tmpDF = tmpDF.withColumn(\"ready_to_retire\", coalesce(\"age_above_60\", col(\"isabove50\").cast(\"string\")))\n",
    "tmpDF.show(10)\n",
    "\n",
    "# coalese to filter the null values with literal 'No' value\n",
    "tmpDF = tmpDF.withColumn(\"ready_to_retire\", coalesce(\"age_above_60\", lit(\"No\")))\n",
    "tmpDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.1 Droping Null Values\n",
    "\n",
    "`drop()` functions is used to remove rows that has null values. The default `drop()` method without parameter will drop records that has any null values. The parameter to the methods are:-   \n",
    "`any`: e.g. `drop(\"any\")`. `any` argument will drops row if row has any null values.        \n",
    "`all`: e.g. `drop(\"all\")`. `all` argument will drops row if row has all null values.     \n",
    "`any` or `all` followed by array of columns: e.g. `drop(\"all\", subset=[\"first_name\", \"last_name\"])`. Drops row only from the specified columns with `any` and `all` argument defined above.   \n",
    "\n",
    "Mostly, used for cleaning the final DataFrame after merging/joining multiple DataFrame which contains null during left, right, full join etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             66|     true|         Yes|            Yes|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             65|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             64|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             66|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             62|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             61|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             67|     true|         Yes|            Yes|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|             66|     true|         Yes|            Yes|\n",
      "| 10014|1956-02-12|     Berni|    Genin|     M|1987-03-11|             63|     true|         Yes|            Yes|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all records that has null values in tmpDF and store into emp_above60DF DF.\n",
    "\n",
    "emp_above60DF = tmpDF.na.drop()\n",
    "emp_above60DF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             66|     true|         Yes|            Yes|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             65|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             64|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             66|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             62|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             61|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             67|     true|         Yes|            Yes|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|             66|     true|         Yes|            Yes|\n",
      "| 10014|1956-02-12|     Berni|    Genin|     M|1987-03-11|             63|     true|         Yes|            Yes|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all records that has null values in any columns using 'any' parameter in tmpDF  and store in empNoNullDF DF.\n",
    "# 'any' parameter is to drop records that has any null values in the DataFrame. \n",
    "# Reason to use any: Sometime multiple columns might has null values which might not be useful during analysis.\n",
    "\n",
    "empNoNullDF = tmpDF.na.drop(\"any\")\n",
    "empNoNullDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             66|     true|         Yes|            Yes|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|             55|    false|        null|             No|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             65|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             64|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             66|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             62|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             61|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             67|     true|         Yes|            Yes|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|             56|    false|        null|             No|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all records that has null values across entire columns value using 'all' parameter in tmpDF DF. \n",
    "# For example: Due to data quality issue if incoming data is retrieve then entire record will be stored as null values\n",
    "# so we need to drop those bad records. In such case use 'all' in drop() method to delete records that has\n",
    "# null values in entire record.\n",
    "\n",
    "\n",
    "# Drop record where entire field value is null. None of the records has null values for all fields so it won't \n",
    "# drop any records.\n",
    "\n",
    "empInvalidDF = tmpDF.na.drop(\"all\")\n",
    "empInvalidDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             66|     true|         Yes|            Yes|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             65|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             64|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             66|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             62|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             61|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             67|     true|         Yes|            Yes|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|             66|     true|         Yes|            Yes|\n",
      "| 10014|1956-02-12|     Berni|    Genin|     M|1987-03-11|             63|     true|         Yes|            Yes|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all records that has null values with 'any' in tmpDF DataFrame only for first_name and age_above_60 column.\n",
    "\n",
    "empAbove60DF = tmpDF.na.drop(\"any\", subset=[\"first_name\", \"age_above_60\"])\n",
    "empAbove60DF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.2 Filling Null Values\n",
    "\n",
    "`fill()` method is used to fill records that contains null values for one or more columns with explicit user defined value. It works for all data types. `Dict` can also used to fill multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------------+\n",
      "|isabove50|      age_above_60|ready_to_retire|\n",
      "+---------+------------------+---------------+\n",
      "|     true|               Yes|            Yes|\n",
      "|    false|Still Not Above 60|             No|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|    false|Still Not Above 60|             No|\n",
      "|     true|               Yes|            Yes|\n",
      "|    false|Still Not Above 60|             No|\n",
      "|    false|Still Not Above 60|             No|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|    false|Still Not Above 60|             No|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "|     true|               Yes|            Yes|\n",
      "+---------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill null values with \"Still Not Above 60\" in employees ready_to_retire column that has null values.\n",
    "# The age_above_60 null values will be replace with \"Still Not Above 60\" literal value.\n",
    "\n",
    "empFillNullDF = tmpDF.na.fill(\"Still Not Above 60\")\n",
    "empFillNullDF.selectExpr(\"isabove50\", \"age_above_60\", \"ready_to_retire\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             66|     true|         Yes|            Yes|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|             55|    false|Wait till 60|             No|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             65|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             64|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             66|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             62|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             61|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             67|     true|         Yes|            Yes|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|             56|    false|Wait till 60|             No|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill \"XXXX\", \"0000-00-00\" and \"Wait till 60\"\n",
    "# for first_name, birth_date and age_above_60 columns respectively in tmpDF DF that has null values using\n",
    "# input from \"null_column_dict\" dict\n",
    "\n",
    "# Check the output for 'age_above_60' since only this column has null value\n",
    "\n",
    "null_column_dict = {\"first_name\": \"XXXX\", \"birth_date\": \"0000-00-00\", \"age_above_60\": 'Wait till 60', } \n",
    "empFillNullWithDictDF = tmpDF.na.fill(null_column_dict)\n",
    "empFillNullWithDictDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.3 Filtering Null Values\n",
    "\n",
    "Null values can be filtered by using `isNull()` method. If the column value is null then it return true and filter/select only records having null values.   \n",
    "\n",
    "Not null values can be filtered by using `isNotNull()` method. It perform opposite operation compared to `isNull()`. If the column value is not null then it return true and filter/select only records having not null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+----------------------+--------------------------+\n",
      "|emp_current_age|isabove50|(age_above_60 IS NULL)|(age_above_60 IS NOT NULL)|\n",
      "+---------------+---------+----------------------+--------------------------+\n",
      "|             66|     true|                 false|                      true|\n",
      "|             55|    false|                  true|                     false|\n",
      "|             60|     true|                 false|                      true|\n",
      "|             65|     true|                 false|                      true|\n",
      "|             64|     true|                 false|                      true|\n",
      "|             66|     true|                 false|                      true|\n",
      "|             62|     true|                 false|                      true|\n",
      "|             61|     true|                 false|                      true|\n",
      "|             67|     true|                 false|                      true|\n",
      "|             56|    false|                  true|                     false|\n",
      "+---------------+---------+----------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|             55|    false|        null|             No|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|             56|    false|        null|             No|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|             59|    false|        null|             No|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|             56|    false|        null|             No|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|             58|    false|        null|             No|\n",
      "| 10021|1960-02-20|     Ramzi|       Erde|     M|1988-02-10|             59|    false|        null|             No|\n",
      "| 10027|1962-07-10|    Divier|    Reistad|     F|1989-07-07|             57|    false|        null|             No|\n",
      "| 10028|1963-11-26|  Domenick|   Tempesti|     M|1991-10-22|             56|    false|        null|             No|\n",
      "| 10032|1960-08-09|     Jeong|    Reistad|     F|1990-06-20|             59|    false|        null|             No|\n",
      "| 10034|1962-12-29|     Bader|       Swan|     M|1988-09-21|             56|    false|        null|             No|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_current_age|isabove50|age_above_60|ready_to_retire|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|             66|     true|         Yes|            Yes|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|             60|     true|         Yes|            Yes|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|             65|     true|         Yes|            Yes|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|             64|     true|         Yes|            Yes|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|             66|     true|         Yes|            Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|             62|     true|         Yes|            Yes|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|             61|     true|         Yes|            Yes|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|             67|     true|         Yes|            Yes|\n",
      "| 10011|1953-11-07|      Mary|    Sluis|     F|1990-01-22|             66|     true|         Yes|            Yes|\n",
      "| 10014|1956-02-12|     Berni|    Genin|     M|1987-03-11|             63|     true|         Yes|            Yes|\n",
      "+------+----------+----------+---------+------+----------+---------------+---------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# show boolean value for isNull() and isNotNull() method\n",
    "tmpDF.select(\"emp_current_age\", \"isabove50\", col(\"age_above_60\").isNull(),col(\"age_above_60\").isNotNull()).\\\n",
    "        show(10)\n",
    "\n",
    "# filter null values from age_above_60 column\n",
    "tmpDF.select(\"*\").where(col(\"age_above_60\").isNull()).show(10)\n",
    "\n",
    "# filter not null values from age_above_60 column\n",
    "tmpDF.select(\"*\").where(col(\"age_above_60\").isNotNull()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 User Defined Functions\n",
    "\n",
    "User Defined Functions (UDFs) are the custom function for manipulation and transforming the record values. If Spark doesn't provide specific function in its module to solve the business logic or problem then we need to create our own function known as UDF. Spark supports UDFs written on multiple languges such as Java, Python, Scala etc where Java and Scala has better perfromance compared to Python during data serialization. The best practice is to write UDF in Scala and call from Python. UDFs can have one or more columns as input parameters. These functions as simliar to other native functions. The functions need to registered before using it. By default, it is registered as temporary functions which is specific only for certain SparkSession. But it can also be permanently registered.\n",
    "\n",
    "We'll create increase_ten_percent UDF both in Python and Scala to add 10% in current salary. Then register the function and apply it in DataFrame to calculate new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0\n",
      "11.0\n",
      "+------+------+----------+----------+------+---------------+\n",
      "|emp_no|salary| from_date|   to_date|salary|increase_salary|\n",
      "+------+------+----------+----------+------+---------------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26| 60117|        66128.7|\n",
      "| 10001| 62102|1987-06-26|1988-06-25| 62102|        68312.2|\n",
      "| 10001| 66074|1988-06-25|1989-06-25| 66074|        72681.4|\n",
      "| 10001| 66596|1989-06-25|1990-06-25| 66596|        73255.6|\n",
      "| 10001| 66961|1990-06-25|1991-06-25| 66961|        73657.1|\n",
      "+------+------+----------+----------+------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create, register, and call UDF in Python\n",
    "\n",
    "# Add 10% to current salary \n",
    "\n",
    "# Create increase_ten_percent UDF\n",
    "def increase_ten_percent(amount):\n",
    "    return float((amount * 0.10) + amount)\n",
    "\n",
    "# test function \n",
    "sal_1  = increase_ten_percent(20)\n",
    "print(sal_1)  # must return 22\n",
    "sal_2  = increase_ten_percent(10)\n",
    "print(sal_2)  # must return 10\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n",
    "# Register increase_ten_percent UDF\n",
    "increase_ten_percent_udf = udf(increase_ten_percent)\n",
    "\n",
    "# Call UDF\n",
    "salaries.select(\"*\", \"salary\" ,increase_ten_percent_udf(col(\"salary\"))\\\n",
    "         .alias(\"increase_salary\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment**: Create increase_ten_percent UDF both in Python and Scala with following features:   \n",
    "* Add 10% from existing salary if employees worked more than 5 years.\n",
    "* Salar field must only be integer and long.\n",
    "* Hired date must be only string and date with 'yyyy-mm-dd' format\n",
    "* Function must check null values for either parameter and return 0 if null is found. \n",
    "* Register the function and apply it in DataFrame to calculate new column bonus_salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "# Create, register, and call UDF in Python\n",
    "\n",
    "# Add 10% from current salary # if hired date is more than 5 year to current date.\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Create increase_ten_percent UDF\n",
    "def increase_ten_percent(amount, from_date):\n",
    "    hire_threshold = 5\n",
    "    # @todo: check date pattern    \n",
    "    # check date instance\n",
    "    if isinstance(from_date, str):    \n",
    "        from_date = datetime.strptime(from_date, '%Y-%m-%d')    \n",
    "    # get today\n",
    "    today = datetime.today().date()\n",
    "    # get diff year\n",
    "    diff_year = today.year - from_date.year\n",
    "    if diff_year > hire_threshold:\n",
    "        return float((amount * 0.10) + amount)\n",
    "    else:\n",
    "        return float(amount)\n",
    "\n",
    "# test function \n",
    "sal_1  = increase_ten_percent(20, \"2010-01-01\")\n",
    "print(sal_1)  # must return 22\n",
    "sal_2  = increase_ten_percent(20, \"2019-01-01\")\n",
    "print(sal_2)  # must return 20\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n",
    "# Register increase_ten_percent UDF\n",
    "increase_ten_percent_udf = udf(increase_ten_percent, DateType())\n",
    "\n",
    "# Call UDF\n",
    "salaries.select(\"*\", increase_ten_percent_udf(col(\"salary\"), \"from_date\")\\\n",
    "         .alias(\"increase_salary\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, register, UDF in Scala and call from Python\n",
    "\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "def increase_ten_percent(amount: Integer):\n",
    "    Integer = (amount * 0.10) * amount\n",
    "\n",
    "# test function\n",
    "increase_ten_percent(20) # must return 1\n",
    "\n",
    "# Register increase_ten_percent UDF\n",
    "val increase_ten_percent_udf = udf(increase_ten_percent(_:Integer):Integer)\n",
    "\n",
    "# Call UDF in Python not in Scala\n",
    "from pyspark.sql.functions import col\n",
    "employees.select(increase_ten_percent_udf(col(\"salary\"))).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**\n",
    "\n",
    "The registered UDF shown above is accessible only for DataFrame function. It cannot be use with string expression like `employees.selectExpr(\"increase_ten_percent_udf(column_name)\")`. We can register the function as Spark SQL function which allows to use string expression as well as calling from SQL function too. The reason behind is \"*UDF registered with Spark SQL functions or expression is valid for DataFrames expression*\".\n",
    "\n",
    "Register UDF as SQL function in Scala:   \n",
    "`spark.udf.register(\"increase_ten_percent_udf\", increase_ten_percent(_:Integer):Integer)`   \n",
    "\n",
    "Register UDF as SQL function in Python:   \n",
    "`spark.udf.register(\"increase_ten_percent_udf\", increase_ten_percent)`  \n",
    "\n",
    "Now, we can use in our DataFrame with `selectExpr` show below:   \n",
    "`employees.selectExpr(\"increase_ten_percent_udf(column_name)\")`   \n",
    "\n",
    "Although, we have created our UDF and it works as expected. The best practice is to specific the return type from function. If type doesn't matches with then Spark will return `null` value. We can also specify return type as `None` and `Option` in Python and Scala respectively.\n",
    "\n",
    "Register UDF as SQL function in Python with return type:   \n",
    "`from pyspark.sql.types import IntegerType\n",
    "spark.udf.register(\"increase_ten_percent_udf\", increase_ten_percent, IntegerType())`   \n",
    "\n",
    "Similary, we can register HIVE UDF and UDAF through Hive syntax. [Click for more detail](https://blog.cloudera.com/working-with-udfs-in-apache-spark/). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
