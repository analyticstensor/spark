{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**4.1 Aggregation**](#4.1-Aggregation)   \n",
    "[**4.2 Aggregate Functions**](#4.2-Aggregate-Functions)   \n",
    "[**4.3 Grouping Types**](#4.3-Grouping-Types)   \n",
    "[**4.3.1 Simple Grouping**](#4.3.1-Simple-Grouping)   \n",
    "[**4.3.2 GroupBy**](#4.3.2-GroupBy)   \n",
    "[**4.3.3 Window**](#4.3.3-Window)   \n",
    "[**4.3.4 Grouping Set**](#4.3.4-Grouping-Set)   \n",
    "[**4.3.4.1 Rollup**](#4.3.4.1-Rollup)   \n",
    "[**4.3.4.2 Cube**](#4.3.4.2-Cube)   \n",
    "[**4.4 Pivot**](#4.4-Pivot)   \n",
    "[**4.5 User Defined Aggregate Functions**](#4.5-User-Defined-Aggregate-Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Aggregation\n",
    "**Aggregation**: Aggregation is the process of summarizing the data based on one or more columns. Aggregated data provides the insights of particular groups. Aggregation is core functionality of data analytics. Data can be aggregated either by whole dataset or grouping into individual/groups of attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of aggregation are:      \n",
    "* Counting total record.\n",
    "* Counting record based specified column. For e.g. counting record based on state, zipcode, gender etc.\n",
    "* Finding minimum and maximum sales for given dataset.\n",
    "* Finding sum of total sales.\n",
    "* Finding average score in SAT.\n",
    "* Finding total sales based on year and month.\n",
    "* Finding total customer served each day based on time duration. e.g. grouped on 6-9, 9-11, 11-2 etc.\n",
    "* Finding mean, standard deviation, variance, correlation, skew etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Aggregate Functions\n",
    "Spark has many built-in aggregate functions. All the functions can be imported from `pyspark.sql.functions`. Table 4.2 shows aggregate functions for simple and complex types in Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4.2 Spark aggregate functions   \n",
    "\n",
    "| Function Name | Description | Function with Parameter |\n",
    "| ------------- | ------------- | ------------- |\n",
    "| approx_count_distinct | Returns the approximate number of distinct items in a group | approx_count_distinct(column_name) |\n",
    "| avg | Return the average values for each numeric columns for each group. Similar to `mean`. Non-numerical columns will be ignored. | avg(column_name) or avg(\\*column_name) | \n",
    "| collect_list | Returns a list of objects with duplicates. | collect_list(column_name) |\n",
    "| collect_set | Returns a set of objects by eliminating duplicates. | collect_set(column_name) |\n",
    "| corr | Returns the Pearson Correlation Coefficient for given columns (column 1 and column 2) | cor(column_1, column_2) |\n",
    "| count | Returns the number of items in a group. | count(column_name) |\n",
    "| countDistinct | Returns the distinct count for either column or list of colummns. | countDistinct(column_name) or countDistinct(\\*column_name) |\n",
    "| covar_pop | Returns the population covariance of column_1 and column_2. | covar_pop(column_name_1, column_name_2) |\n",
    "| covar_samp | Returns the sample covariance of column_1 and column_2. | covar_sample(column_name_1, column_name_2) |\n",
    "| first | Returns the first row of column. By default it will return the first values it finds. When ignoreNull is set to true then it will return first non-null values. If all vaues are null then null values will be returned. | first(column_name)  or first(column_name, ignorenulls = Boolean)\n",
    "| grouping | Indicate if a specified column in a GROUP BY list is aggregated or not, Returns 1 for aggregated or 0 or not aggregated. | grouping(column_name) |\n",
    "| grouping_id | Returns the level of grouping. i.e. (grouping(c1)<<(n-1)) + (grouping(c1)<<(n-1)) + ... + (grouping(cn)<<(cn)). |\n",
    "| kurtosis | Returns the kurtosis of the column. | kurtosis(column_name) |\n",
    "| last | Returns the last row of column. By default it will return the last values it finds. When ignoreNull is set to true then it will return last non-null values. If all vaues are null then null values will be returned.  | last(column_name) or last(column_name, ignorenulls = Boolean) | \n",
    "| max | Returns the maximum values of the column in a group. | max(column_name) |\n",
    "| mean | Returns the average values of the column in a group. | mean(column_name) |\n",
    "| min | Returns the minimun values of the column in a group | min(column_name) |\n",
    "| skewness | Returns the skewness of the values in a group | skewness(column_name) |\n",
    "| stddev | Returns the sample standard deviation of expression in a group. Similar to `stddev_samp`. | stddev(column_name) |\n",
    "| stddev_pop | Returns the population standard deviation of expression in a group.  | stddev_pop(column_name) |\n",
    "| stddev_samp | Returns the sample standard deviation of expression in a group. | stddev_samp(column_name) |\n",
    "| sum | Returns the sum of all values in a column or expression. | sum(column_name) or sum(\\*column_name) |\n",
    "| sumDistinct | Returns the sum of of distinct values in a column or expression. | sumDistinct(column_name) or sumDistinct(column_expression) |\n",
    "| var_pop | Returns the population variance of the values in a group | var_pop(column_name) |\n",
    "| var_sample | Returns the unbiased sample variance of the values in a group | var_sample(column_name) |\n",
    "| variance | Returns the unbiased sample variance of the values in a group. Similar to `var_sample`. | variance(column_name) |   \n",
    "\n",
    "$^{*}$indicate multiple column name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Grouping Types\n",
    "Spark provides several grouping types for aggregation. The grouping types are:\n",
    "*  `simple grouping`: is used to get summary of DataFrame based on specific columns. The aggregate function is used in `select` statement.\n",
    "* `groupby`: is used to specify one or more columns in aggregate functions. It also allows to specify one or more aggregate functions for specific columns.\n",
    "* `window`: is used to specify one or more columns in aggregate functions whereas the input to the functions are related to current row.\n",
    "* `grouping set`: is used to aggregate across multiple groups. `rollup` and `cube` are used for grouping set.\n",
    "    * `rollup`: is used to aggregate over combination of groups of specified columns.\n",
    "    * `cube`: is used to aggregate across every permutation of specified columns. i.e. across all combination of specified columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites: Read all the tables from employees database in MySQL into Spark.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read mysql database connection string from conf/db_properties.ini\n",
    "\n",
    "config_filename = '../Chapter_2_Structured_API/Lab_1/conf/db_properties.ini'\n",
    "db_properties = {}\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_filename)\n",
    "db_prop = config['mysql']\n",
    "db_url = db_prop['url']\n",
    "db_properties['database'] = db_prop['database']\n",
    "db_properties['schema'] = db_prop['schema']\n",
    "db_properties['user'] = db_prop['user']\n",
    "db_properties['password'] = db_prop['password']\n",
    "db_properties['serverTimezone'] = db_prop['serverTimezone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Chapter 4 Aggregate Function\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load current_dept_emp\n",
    "current_dept_emp = spark.read.jdbc(url = db_url, table = 'current_dept_emp', properties = db_properties)\n",
    "\n",
    "# Load departments\n",
    "departments = spark.read.jdbc(url = db_url, table = 'departments', properties = db_properties)\n",
    "\n",
    "# Load dept_emp\n",
    "dept_emp = spark.read.jdbc(url = db_url, table = 'dept_emp', properties = db_properties)\n",
    "\n",
    "# Load dept_emp_latest_date\n",
    "dept_emp_latest_date = spark.read.jdbc(url = db_url, table = 'dept_emp_latest_date', properties = db_properties)\n",
    "\n",
    "# Load dept_manager\n",
    "dept_manager = spark.read.jdbc(url = db_url, table = 'dept_manager', properties = db_properties)\n",
    "\n",
    "# Load employees\n",
    "employees = spark.read.jdbc(url = db_url, table = 'employees', properties = db_properties)\n",
    "\n",
    "# Load highest_salary_employee\n",
    "highest_salary_employee = spark.read.jdbc(url = db_url, table = 'highest_salary_employee', properties = db_properties)\n",
    "\n",
    "# Load salaries\n",
    "salaries = spark.read.jdbc(url = db_url, table = 'salaries', properties = db_properties)\n",
    "\n",
    "# Load titles\n",
    "titles = spark.read.jdbc(url = db_url, table = 'titles', properties = db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show Spark Session\n",
    "spark\n",
    "\n",
    "# Show employees DF schema \n",
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Simple Grouping\n",
    "\n",
    "Aggregate function are used in `select` statement. We'll apply aggregate function shown in table 4.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count**   \n",
    "`count`: It is used for counting total records in DataFrame. Records can be counted either by passing \\* or column name to `count()` method as shown below:      \n",
    "* count(\"column_name\"): If the column contains null value then those will be ignored.\n",
    "* count(\"\\*\"): If the column contains null value then those will be counted.\n",
    "* count(lit(1)): We can also pass the literal value to count each record.\n",
    "\n",
    "We can also perform basic aggregation over entire DataFrame by specifying `df.method_name()`. For example:   \n",
    "`employees.count()`. It is the quickest method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count over entire DataFrame\n",
    "employees.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|total_records_in_employees_DF|\n",
      "+-----------------------------+\n",
      "|                       300024|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count all the records from employees DF.\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "employees.select(count(\"*\").alias(\"total_records_in_employees_DF\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_count: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.select(count(\"*\").alias(\"total_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|total_records_in_employees_DF|\n",
      "+-----------------------------+\n",
      "|                       300024|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count all the records from employees DF.\n",
    "# Specify only emp_no\n",
    "\n",
    "employees.select(count(\"emp_no\").alias(\"total_records_in_employees_DF\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|total_records_in_employees_DF|\n",
      "+-----------------------------+\n",
      "|                       300024|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count all the records from employees DF.\n",
    "# with literal value\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "employees.select(count(lit(1)).alias(\"total_records_in_employees_DF\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create emp DataFrame**:   \n",
    "Since we don't have DataFrame with null values, we will create emp DataFrame from employees by adding two new column new `isabove60` and `age_above_60` with boolean and string type respectively. `age_above_60` will have  only two value either `Yes` or `null`. This helps us to compare DataFrame with and without null values in aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|emp_current_age|isabove60|age_above_60|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+\n",
      "| 10001|1953-09-02|    Georgi|    Facello|     M|1986-06-26|             67|     true|         Yes|\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|             56|    false|        null|\n",
      "| 10003|1959-12-03|     Parto|    Bamford|     M|1986-08-28|             60|     true|         Yes|\n",
      "| 10004|1954-05-01| Chirstian|    Koblick|     M|1986-12-01|             66|     true|         Yes|\n",
      "| 10005|1955-01-21|   Kyoichi|   Maliniak|     M|1989-09-12|             65|     true|         Yes|\n",
      "| 10006|1953-04-20|    Anneke|    Preusig|     F|1989-06-02|             67|     true|         Yes|\n",
      "| 10007|1957-05-23|   Tzvetan|  Zielinski|     F|1989-02-10|             63|     true|         Yes|\n",
      "| 10008|1958-02-19|    Saniya|   Kalloufi|     M|1994-09-15|             62|     true|         Yes|\n",
      "| 10009|1952-04-19|    Sumant|       Peac|     F|1985-02-18|             68|     true|         Yes|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|             57|    false|        null|\n",
      "| 10011|1953-11-07|      Mary|      Sluis|     F|1990-01-22|             67|     true|         Yes|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|             60|     true|         Yes|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|             57|    false|        null|\n",
      "| 10014|1956-02-12|     Berni|      Genin|     M|1987-03-11|             64|     true|         Yes|\n",
      "| 10015|1959-08-19|  Guoxiang|  Nooteboom|     M|1987-07-02|             61|     true|         Yes|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|             59|    false|        null|\n",
      "| 10017|1958-07-06| Cristinel|  Bouloucos|     F|1993-08-03|             62|     true|         Yes|\n",
      "| 10018|1954-06-19|  Kazuhide|       Peha|     F|1987-04-03|             66|     true|         Yes|\n",
      "| 10019|1953-01-23|   Lillian|    Haddadi|     M|1999-04-30|             67|     true|         Yes|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|             67|     true|         Yes|\n",
      "+------+----------+----------+-----------+------+----------+---------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if first columns value is empty or null, if it is empty then retrieve value from second column or put literal value.\n",
    "\n",
    "from pyspark.sql.functions import col, expr, coalesce, current_date, floor, months_between\n",
    "\n",
    "# Create new DataFrame tmpDF that contain all the columns from employees and adding new columns \"age_above_50\"\n",
    "# column 'age_above_60' is calculated field that stores whether employees age is above 60 or not.\n",
    "\n",
    "# assign value\n",
    "total_month = 12\n",
    "filter_age = 60\n",
    "\n",
    "# emp_current_age stores employee current age\n",
    "# isabove50 stores boolean value whether or not age is 50\n",
    "# age_above_60 stores Yes for above age 60 else null\n",
    "\n",
    "emp = employees.withColumn(\"emp_current_age\",\\\n",
    "                  floor(months_between(current_date(), \"birth_date\")/total_month))\\\n",
    "                  .withColumn(\"isabove60\", col(\"emp_current_age\") >= filter_age)\\\n",
    "                  .withColumn(\"age_above_60\", expr(\"case when isabove60 then 'Yes' else null end\"))                             \n",
    "emp.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the count method in emp DataFrame to see the effect that has null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|total_records_in_employees_DF|\n",
      "+-----------------------------+\n",
      "|                       300024|\n",
      "+-----------------------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|total_records_in_employees_DF|\n",
      "+-----------------------------+\n",
      "|                       300024|\n",
      "+-----------------------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|total_records_in_employees_DF|\n",
      "+-----------------------------+\n",
      "|                       203161|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select(count(\"*\").alias(\"total_records_in_employees_DF\")).show()\n",
    "emp.select(count(\"emp_no\").alias(\"total_records_in_employees_DF\")).show()\n",
    "\n",
    "# Count records in age_above_60 column that contains null value\n",
    "emp.select(count(\"age_above_60\").alias(\"total_records_in_employees_DF\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**countDistinct**\n",
    "\n",
    "`countDistinct`: It is used to count total distinct records in DataFrame. Basically, counting records excluding duplicate values. It is used mainly in specific columns instead in entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Distinct Gender|\n",
      "+---------------+\n",
      "|              2|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # count distinct gender in employees DF\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "employees.select(countDistinct(\"gender\").alias(\"Distinct Gender\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Distinct FirstName|\n",
      "+------------------+\n",
      "|              1275|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # count distinct first name in employees DF\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "employees.select(countDistinct(\"first_name\").alias(\"Distinct FirstName\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Distinct LastName|\n",
      "+-----------------+\n",
      "|             1637|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # count distinct last name in employees DF\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "employees.select(countDistinct(\"last_name\").alias(\"Distinct LastName\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Distinct FullName|\n",
      "+-----------------+\n",
      "|           279408|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # count distinct fullname in employees DF\n",
    "\n",
    "from pyspark.sql.functions import countDistinct, concat\n",
    "\n",
    "employees.select(countDistinct((concat(\"first_name\", \"last_name\"))).alias(\"Distinct FullName\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dept_emp` DataFrame stores all the department served by each employee on given timeframe. We'll find total dept_no in that DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n",
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "| 10001|   d005|1986-06-26|9999-01-01|\n",
      "| 10002|   d007|1996-08-03|9999-01-01|\n",
      "| 10003|   d004|1995-12-03|9999-01-01|\n",
      "| 10004|   d004|1986-12-01|9999-01-01|\n",
      "| 10005|   d003|1989-09-12|9999-01-01|\n",
      "| 10006|   d005|1990-08-05|9999-01-01|\n",
      "| 10007|   d008|1989-02-10|9999-01-01|\n",
      "| 10008|   d005|1998-03-11|2000-07-31|\n",
      "| 10009|   d006|1985-02-18|9999-01-01|\n",
      "| 10010|   d004|1996-11-24|2000-06-26|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_emp.printSchema()    \n",
    "dept_emp.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Distinct Department|\n",
      "+-------------------+\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # count distinct dept in dept_emp DF.\n",
    "\n",
    "dept_emp.select(countDistinct(\"dept_no\").alias(\"Distinct Department\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**approx_count_distinct**\n",
    "`approx_count_distinct`: It is used to count the approximate distinct count in the DataFrame. When the dataset is large it might consume more time to calculate distinct count, so we use `approx_count_distinct` function to count approximate distinct records. The additional parameter to this function is the maximum estimation error allowed during calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|approx_count_distinct(emp_no)|\n",
      "+-----------------------------+\n",
      "|                       298930|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate approximate distinct count in employees DF\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "employees.select(approx_count_distinct(\"emp_no\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|approx_count_distinct(emp_no)|\n",
      "+-----------------------------+\n",
      "|                       276091|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate approximate distinct count in employees DF\n",
    "# with 0.1 maximum estimation erorr\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "employees.select(approx_count_distinct(\"emp_no\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first**   \n",
    "`first`: It is used to find the first records from a DataFrame. The value is selected based on rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "|first(emp_no, false)|first(hire_date, false)|\n",
      "+--------------------+-----------------------+\n",
      "|               10001|             1986-06-26|\n",
      "+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "employees.select(first(\"emp_no\"), first(\"hire_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**last**   \n",
    "`last`: It is used to find the last records from a DataFrame. The value is selected based on rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+\n",
      "|last(emp_no, false)|last(hire_date, false)|\n",
      "+-------------------+----------------------+\n",
      "|             499999|            1997-11-30|\n",
      "+-------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import last\n",
    "\n",
    "employees.select(last(\"emp_no\"), last(\"hire_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min**   \n",
    "`min`: It is used to find the minimum value from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|minimum_salary|\n",
      "+--------------+\n",
      "|         38623|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the minimum salary from salaries DF.\n",
    "\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "salaries.select(min(\"salary\").alias(\"minimum_salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max**   \n",
    "`max`: It is used to find the maximum value from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|maximum_salary|\n",
      "+--------------+\n",
      "|        158220|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the maximun salary from salaries DF.\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "salaries.select(max(\"salary\").alias(\"maximum_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|min_salary|max_salary|\n",
      "+----------+----------+\n",
      "|     38623|    158220|\n",
      "+----------+----------+\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the minimum and maximum salary\n",
    "\n",
    "salaries.select(min(\"salary\").alias(\"min_salary\"), max(\"salary\").alias(\"max_salary\")).show()\n",
    "\n",
    "salaries.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+----------+\n",
      "|min_empNumber|max_empNumber|min_Salary|max_Salary|\n",
      "+-------------+-------------+----------+----------+\n",
      "|        10001|       499999|     38623|    158220|\n",
      "+-------------+-------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the minimum and maximum value for emp_no and salary from salaries DF.\n",
    "# Although there in no relation between two column i.e. emp_no and salary but to show different operation.\n",
    "\n",
    "salaries.select(min(\"emp_no\").alias(\"min_empNumber\"),\\\n",
    "                max(\"emp_no\").alias(\"max_empNumber\"),\\\n",
    "                min(\"salary\").alias(\"min_Salary\"),\\\n",
    "                max(\"salary\").alias(\"max_Salary\")\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sum**   \n",
    "`sum`: It is used to find the addition or summation of values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|sum of salary |\n",
      "+--------------+\n",
      "|  181480757419|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the sum of salary from salaries DF.\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "salaries.select(sum(\"salary\").alias(\"sum of salary \")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sumDistinct**   \n",
    "`sumDistinct`: It is used to find the distinct addition or summation of values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|distinct sum of salary |\n",
      "+-----------------------+\n",
      "|             7078688488|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the distinct sum of salary from salaries DF.\n",
    "\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "\n",
    "salaries.select(sumDistinct(\"salary\").alias(\"distinct sum of salary \")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+\n",
      "|sum of salary |distinct sum of salary |\n",
      "+--------------+-----------------------+\n",
      "|  181480757419|             7078688488|\n",
      "+--------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the sum and distinct sum of salary from salaries DF.\n",
    "\n",
    "salaries.select(sum(\"salary\").alias(\"sum of salary \"),\\\n",
    "                sumDistinct(\"salary\").alias(\"distinct sum of salary \")\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**avg**   \n",
    "`avg`: It is used to find the average values from the DataFrame. It is same as `mean()`. Averge can also be calcuated by `sum/count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------------------+\n",
      "|    Average Salary|       Mean Salary|(sum(salary) / count(salary))|\n",
      "+------------------+------------------+-----------------------------+\n",
      "|63810.744836143705|63810.744836143705|           63810.744836143705|\n",
      "+------------------+------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the avg salary from salaries DF.\n",
    "\n",
    "from pyspark.sql.functions import avg, mean\n",
    "\n",
    "salaries.select(avg(\"salary\").alias(\"Average Salary\"),\\\n",
    "               mean(\"salary\").alias(\"Mean Salary\"),\\\n",
    "               sum(\"salary\")/count(\"salary\"))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corr**   \n",
    "`corr` is used to measure the correlation between two columns. It measures the Pearson correlation coefficient ranged from -1 to +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|corr(salary, ((salary * 0.10) + salary))|\n",
      "+----------------------------------------+\n",
      "|                      1.0000000000000775|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the correlation between salary and 10% increase of salary.\n",
    "\n",
    "from pyspark.sql.functions import expr, corr\n",
    "\n",
    "salaries.select(corr(\"salary\", expr(\"(salary * 0.10) + salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**covar_pop**   \n",
    "`covar_pop` is used to measure the population covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|covar_pop(salary, ((salary * 0.10) + salary))|\n",
      "+---------------------------------------------+\n",
      "|                          3.143505413914679E8|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the population covariance between salary and 10% increase of salary.\n",
    "\n",
    "from pyspark.sql.functions import covar_pop\n",
    "\n",
    "salaries.select(covar_pop(\"salary\", expr(\"(salary * 0.10) + salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**covar_samp**   \n",
    "`covar_samp` is used to measure the sample covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|covar_samp(salary, ((salary * 0.10) + salary))|\n",
      "+----------------------------------------------+\n",
      "|                          3.1435065192081285E8|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the sample covariance between salary and 10% increase of salary.\n",
    "\n",
    "from pyspark.sql.functions import covar_samp\n",
    "\n",
    "salaries.select(covar_samp(\"salary\", expr(\"(salary * 0.10) + salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**variance**   \n",
    "`variance` is used to find the sample variance. Variance is the average of squared difference from the mean. It is similar to `var_samp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    var_samp(salary)|\n",
      "+--------------------+\n",
      "|2.8577331992799246E8|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the variance of salary.\n",
    "\n",
    "from pyspark.sql.functions import variance\n",
    "\n",
    "salaries.select(variance(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**var_samp**   \n",
    "`var_samp` is used to find the sample variance. Variance is the average of squared difference from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    var_samp(salary)|\n",
      "+--------------------+\n",
      "|2.8577331992799246E8|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the sample variance of salary.\n",
    "\n",
    "from pyspark.sql.functions import var_samp\n",
    "\n",
    "salaries.select(var_samp(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**var_pop**   \n",
    "`var_pop` is used to find the population variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    var_pop(salary)|\n",
      "+-------------------+\n",
      "|2.857732194467698E8|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the population variance of salary.\n",
    "\n",
    "from pyspark.sql.functions import var_pop\n",
    "\n",
    "salaries.select(var_pop(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stddev**   \n",
    "`stddev` is used to find the standard deviation.  It is similar to `stddev_samp`. Standard deviation is the square root of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|stddev_samp(salary)|\n",
      "+-------------------+\n",
      "| 16904.831259968036|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the standard deviation of salary.\n",
    "\n",
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "salaries.select(stddev(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stddev_samp**   \n",
    "`stddev_samp` is used to find the sample standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|stddev_samp(salary)|\n",
      "+-------------------+\n",
      "| 16904.831259968036|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the sample standard deviation of salary.\n",
    "\n",
    "from pyspark.sql.functions import stddev_samp\n",
    "\n",
    "salaries.select(stddev_samp(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stddev_pop**   \n",
    "`stddev_pop` is used to find the population standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|stddev_pop(salary)|\n",
      "+------------------+\n",
      "|16904.828288000142|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the population standard deviation of salary.\n",
    "\n",
    "from pyspark.sql.functions import stddev_pop\n",
    "\n",
    "salaries.select(stddev_pop(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**skewness**   \n",
    "`skewness` describes the assymmetry in a random variable's probability distribution. It measures the asymmetry of values of data around the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|  skewness(salary)|\n",
      "+------------------+\n",
      "|0.7779492524394536|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the skewness of salary.\n",
    "\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "salaries.select(skewness(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kurtosis**   \n",
    "`kurtosis` describes the realtive peakedness or flatness of a distribution compared with the normal distribution. It measure the tail of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|  kurtosis(salary)|\n",
      "+------------------+\n",
      "|0.2919969859460094|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the kurtosis of salary.\n",
    "\n",
    "from pyspark.sql.functions import kurtosis\n",
    "\n",
    "salaries.select(kurtosis(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 GroupBy\n",
    "\n",
    "Grouping is used to aggregate data based on particular groups in the dataset. It is mostly used for categorical data compared to continuous data. The data are first grouped on particular column(s) and aggregation is applied. The step for group aggregation is shown below:   \n",
    "1. groupBy(\"columns\") : It returns RelationalGroupedDataset\n",
    "2. aggregate function : It returns DataFrame\n",
    "\n",
    "Example 1: Count total employees for each gender group.      \n",
    "`employees.groupBy(\"gender\").count().show()`   \n",
    "Example 2: Count total salary for each group/category.     \n",
    "`salaries.groupBy(\"salary\").count().show()`   \n",
    "Example 3: Find total employee working in each department.      \n",
    "`dept_emp.groupBy(\"dept_no\").count().show()`   \n",
    "\n",
    "Figure 4.3.2 illustrates the aggregate with grouping for example 3.\n",
    "\n",
    "![Figure: 4.3.2 Aggregation with grouping](grouping_aggregation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_emp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fc244903898>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy returning GroupedData\n",
    "\n",
    "dept_emp.groupBy(\"dept_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dept_no: string, count: bigint]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate function returning DataFrame after applying on GroupedData\n",
    "\n",
    "dept_emp.groupBy(\"dept_no\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|dept_no|count|\n",
      "+-------+-----+\n",
      "|   d005|85707|\n",
      "|   d009|23580|\n",
      "|   d003|17786|\n",
      "|   d001|20211|\n",
      "|   d007|52245|\n",
      "|   d004|73485|\n",
      "|   d002|17346|\n",
      "|   d006|20117|\n",
      "|   d008|21126|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find total employee working in each department.\n",
    "\n",
    "dept_emp.groupBy(\"dept_no\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|gender| count|\n",
      "+------+------+\n",
      "|     F|120051|\n",
      "|     M|179973|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total employees for each gender group.     \n",
    "\n",
    "employees.groupBy(\"gender\").count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-----+\n",
      "|dept_no|year(from_date)|count|\n",
      "+-------+---------------+-----+\n",
      "|   d005|           1997| 5768|\n",
      "|   d006|           2002|  205|\n",
      "|   d002|           1992| 1119|\n",
      "|   d006|           2001|  335|\n",
      "|   d008|           1993| 1344|\n",
      "|   d005|           1999| 5740|\n",
      "|   d007|           2000|  492|\n",
      "|   d007|           1991| 3342|\n",
      "|   d006|           1988| 1218|\n",
      "|   d002|           1998| 1140|\n",
      "|   d006|           1998| 1533|\n",
      "|   d003|           2001|   82|\n",
      "|   d004|           1989| 4556|\n",
      "|   d008|           1996| 1449|\n",
      "|   d007|           1985| 3090|\n",
      "|   d004|           1993| 4714|\n",
      "|   d003|           1995| 1205|\n",
      "|   d007|           1998| 3477|\n",
      "|   d003|           1987| 1178|\n",
      "|   d003|           1986| 1117|\n",
      "|   d002|           1995| 1163|\n",
      "|   d003|           1996| 1231|\n",
      "|   d004|           1998| 5102|\n",
      "|   d006|           1986| 1109|\n",
      "|   d001|           1995| 1238|\n",
      "|   d001|           1997| 1394|\n",
      "|   d003|           1988| 1147|\n",
      "|   d001|           1998| 1480|\n",
      "|   d008|           1988| 1200|\n",
      "|   d006|           1990| 1228|\n",
      "+-------+---------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find total employee working on each department starting on same year.\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "dept_emp.groupBy(\"dept_no\", year(\"from_date\")).count().show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Expressions in Grouping**   \n",
    "\n",
    "Instead of passing aggregate function into `select` statement as expression, we can use `agg` method to accomplish same task. It allows to pass aggregate function as expression using `expr` as well as aliasing the column name. The code looks more clear and easier to undertand by using `agg` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|gender|Total Count|\n",
      "+------+-----------+\n",
      "|     F|     120051|\n",
      "|     M|     179973|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total employees for each gender group.     \n",
    "\n",
    "employees.groupBy(\"gender\")\\\n",
    "         .agg(count(\"gender\").alias(\"Total Count\"))\\\n",
    "         .show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-----------+-----------+\n",
      "|dept_no|year(from_date)|Total Count|Total Count|\n",
      "+-------+---------------+-----------+-----------+\n",
      "|   d005|           1997|       5768|       5768|\n",
      "|   d006|           2002|        205|        205|\n",
      "|   d002|           1992|       1119|       1119|\n",
      "|   d006|           2001|        335|        335|\n",
      "|   d008|           1993|       1344|       1344|\n",
      "|   d005|           1999|       5740|       5740|\n",
      "|   d007|           2000|        492|        492|\n",
      "|   d007|           1991|       3342|       3342|\n",
      "|   d006|           1988|       1218|       1218|\n",
      "|   d002|           1998|       1140|       1140|\n",
      "|   d006|           1998|       1533|       1533|\n",
      "|   d003|           2001|         82|         82|\n",
      "|   d004|           1989|       4556|       4556|\n",
      "|   d008|           1996|       1449|       1449|\n",
      "|   d007|           1985|       3090|       3090|\n",
      "|   d004|           1993|       4714|       4714|\n",
      "|   d003|           1995|       1205|       1205|\n",
      "|   d007|           1998|       3477|       3477|\n",
      "|   d003|           1987|       1178|       1178|\n",
      "|   d003|           1986|       1117|       1117|\n",
      "+-------+---------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find total employee working on each department starting on same year.\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "dept_emp.groupBy(\"dept_no\", year(\"from_date\"))\\\n",
    "        .agg(count(\"dept_no\").alias(\"Total Count\"),\n",
    "        expr(\"count(dept_no)\").alias(\"Total Count\"))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Window\n",
    "\n",
    "Window function operate on a set of rows and return a single value for each row. Window defines the set of rows for which the function operates. This function calculates a return values for all the input records based on certain group of rows known as `frame`.   \n",
    "When we use `groupby` method, all the row will be categoried into only one group. Using window functions, we can categorize row into one or more frames. Spark has three types of window function:   \n",
    "* ranking functions: rank, denseRank, percentRank, ntile, rowNumber\n",
    "* analytics functions: cumeDist, firstValue, lastValue, lag, lead\n",
    "* aggregate functions: avg, count, collect_list\n",
    "\n",
    "[More about Spark Window Functions](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)\n",
    "\n",
    "Window function uses:   \n",
    "* `partitionBy`: used to subdivide the window into partitions. Partition in window function means split or specifying data on certain group. So don't be confused with the partition describe on prior chapter.   \n",
    "* `orderBy`: used to order the partitioned data.\n",
    "\n",
    "For example: Find the average salary for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create salDF by adding hired_year column from salaries DF.\n",
    "\n",
    "salDF = salaries.withColumn(\"hired_year\", year(\"from_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- hired_year: integer (nullable = true)\n",
      "\n",
      "+------+------+----------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|hired_year|\n",
      "+------+------+----------+----------+----------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|      1986|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|      1987|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|      1988|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|      1989|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|      1990|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|      1991|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|      1992|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|      1993|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|      1994|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|      1995|\n",
      "+------+------+----------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salDF.printSchema()\n",
    "salDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|hired_year|       avg(salary)|\n",
      "+----------+------------------+\n",
      "|      1990|  57839.4601216819|\n",
      "|      1988| 55862.44652517686|\n",
      "|      1997| 64565.42639478537|\n",
      "|      1994|61727.758915310624|\n",
      "|      1991| 58803.86967667994|\n",
      "|      1989|56840.672790937606|\n",
      "|      1996|  63618.9425859676|\n",
      "|      1998| 65540.26833919891|\n",
      "|      1985|53182.358005794566|\n",
      "|      1987| 54959.62837743732|\n",
      "|      1995| 62681.04298318265|\n",
      "|      2001|  70694.9158819634|\n",
      "|      1992|59758.741593412415|\n",
      "|      2000| 68556.27813593448|\n",
      "|      1986|54084.778591564136|\n",
      "|      1999| 66525.36188720747|\n",
      "|      2002| 72683.93965798624|\n",
      "|      1993| 60753.65652228216|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the average salary for each year.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "salDF.groupBy(\"hired_year\").agg(avg(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+----------+----------------+\n",
      "|emp_no|salary| from_date|   to_date|hired_year|  Average Salary|\n",
      "+------+------+----------+----------+----------+----------------+\n",
      "| 10001| 66961|1990-06-25|1991-06-25|      1990|57839.4601216819|\n",
      "| 10004| 48271|1990-11-30|1991-11-30|      1990|57839.4601216819|\n",
      "| 10005| 82621|1990-09-12|1991-09-12|      1990|57839.4601216819|\n",
      "| 10006| 40000|1990-08-05|1991-08-05|      1990|57839.4601216819|\n",
      "| 10007| 60740|1990-02-10|1991-02-10|      1990|57839.4601216819|\n",
      "| 10009| 70889|1990-02-17|1991-02-17|      1990|57839.4601216819|\n",
      "| 10011| 42365|1990-01-22|1991-01-22|      1990|57839.4601216819|\n",
      "| 10013| 46305|1990-10-19|1991-10-19|      1990|57839.4601216819|\n",
      "| 10018| 61648|1990-04-02|1991-04-02|      1990|57839.4601216819|\n",
      "| 10021| 59700|1990-02-09|1991-02-09|      1990|57839.4601216819|\n",
      "| 10025| 50120|1990-08-16|1991-08-16|      1990|57839.4601216819|\n",
      "| 10032| 48426|1990-06-20|1991-06-20|      1990|57839.4601216819|\n",
      "| 10033| 56095|1990-03-17|1991-03-17|      1990|57839.4601216819|\n",
      "| 10035| 45629|1990-09-05|1991-09-05|      1990|57839.4601216819|\n",
      "| 10037| 40000|1990-12-05|1991-12-05|      1990|57839.4601216819|\n",
      "| 10038| 43527|1990-09-20|1991-09-20|      1990|57839.4601216819|\n",
      "| 10039| 43801|1990-01-18|1991-01-18|      1990|57839.4601216819|\n",
      "| 10041| 60824|1990-11-12|1991-11-12|      1990|57839.4601216819|\n",
      "| 10043| 49324|1990-10-20|1991-10-20|      1990|57839.4601216819|\n",
      "| 10047| 57350|1990-03-31|1991-03-31|      1990|57839.4601216819|\n",
      "+------+------+----------+----------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the average salary for each year.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# create partition by year using Window Function\n",
    "salWindow = Window.partitionBy(\"hired_year\")\n",
    "# apply aggregate function over partitioned data\n",
    "avgSal = avg(col(\"salary\")).over(salWindow)\n",
    "# select and display DataFrame\n",
    "salDF.select(\"*\",avgSal.alias(\"Average Salary\")).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+----------+----------------+\n",
      "|emp_no|salary| from_date|   to_date|hired_year|  Average Salary|\n",
      "+------+------+----------+----------+----------+----------------+\n",
      "| 10001| 66961|1990-06-25|1991-06-25|      1990|57839.4601216819|\n",
      "| 10004| 48271|1990-11-30|1991-11-30|      1990|57839.4601216819|\n",
      "| 10005| 82621|1990-09-12|1991-09-12|      1990|57839.4601216819|\n",
      "| 10006| 40000|1990-08-05|1991-08-05|      1990|57839.4601216819|\n",
      "| 10007| 60740|1990-02-10|1991-02-10|      1990|57839.4601216819|\n",
      "| 10009| 70889|1990-02-17|1991-02-17|      1990|57839.4601216819|\n",
      "| 10011| 42365|1990-01-22|1991-01-22|      1990|57839.4601216819|\n",
      "| 10013| 46305|1990-10-19|1991-10-19|      1990|57839.4601216819|\n",
      "| 10018| 61648|1990-04-02|1991-04-02|      1990|57839.4601216819|\n",
      "| 10021| 59700|1990-02-09|1991-02-09|      1990|57839.4601216819|\n",
      "| 10025| 50120|1990-08-16|1991-08-16|      1990|57839.4601216819|\n",
      "| 10032| 48426|1990-06-20|1991-06-20|      1990|57839.4601216819|\n",
      "| 10033| 56095|1990-03-17|1991-03-17|      1990|57839.4601216819|\n",
      "| 10035| 45629|1990-09-05|1991-09-05|      1990|57839.4601216819|\n",
      "| 10037| 40000|1990-12-05|1991-12-05|      1990|57839.4601216819|\n",
      "| 10038| 43527|1990-09-20|1991-09-20|      1990|57839.4601216819|\n",
      "| 10039| 43801|1990-01-18|1991-01-18|      1990|57839.4601216819|\n",
      "| 10041| 60824|1990-11-12|1991-11-12|      1990|57839.4601216819|\n",
      "| 10043| 49324|1990-10-20|1991-10-20|      1990|57839.4601216819|\n",
      "| 10047| 57350|1990-03-31|1991-03-31|      1990|57839.4601216819|\n",
      "+------+------+----------+----------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the average salary for each year.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# create partition by year using Window Function\n",
    "salWindow = Window.partitionBy(\"hired_year\")\n",
    "\n",
    "salDF.withColumn(\"Average Salary\",avg(col(\"salary\")).over(salWindow)).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+----------+----------------+------------+--------------+--------------+\n",
      "|emp_no|salary| from_date|   to_date|hired_year|  Average Salary|Total Salary|Minimun Salary|Maximum Salary|\n",
      "+------+------+----------+----------+----------+----------------+------------+--------------+--------------+\n",
      "| 10001| 66961|1990-06-25|1991-06-25|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10004| 48271|1990-11-30|1991-11-30|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10005| 82621|1990-09-12|1991-09-12|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10006| 40000|1990-08-05|1991-08-05|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10007| 60740|1990-02-10|1991-02-10|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10009| 70889|1990-02-17|1991-02-17|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10011| 42365|1990-01-22|1991-01-22|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10013| 46305|1990-10-19|1991-10-19|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10018| 61648|1990-04-02|1991-04-02|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10021| 59700|1990-02-09|1991-02-09|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10025| 50120|1990-08-16|1991-08-16|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10032| 48426|1990-06-20|1991-06-20|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10033| 56095|1990-03-17|1991-03-17|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10035| 45629|1990-09-05|1991-09-05|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10037| 40000|1990-12-05|1991-12-05|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10038| 43527|1990-09-20|1991-09-20|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10039| 43801|1990-01-18|1991-01-18|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10041| 60824|1990-11-12|1991-11-12|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10043| 49324|1990-10-20|1991-10-20|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "| 10047| 57350|1990-03-31|1991-03-31|      1990|57839.4601216819|  6626146391|         38851|        134572|\n",
      "+------+------+----------+----------+----------+----------------+------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the average, total, minimium, and maximum salary for each year.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# create partition by year using Window Function\n",
    "salWindow = Window.partitionBy(\"hired_year\")\n",
    "\n",
    "salDF.withColumn(\"Average Salary\", avg(col(\"salary\")).over(salWindow))\\\n",
    "     .withColumn(\"Total Salary\", sum(col(\"salary\")).over(salWindow))\\\n",
    "     .withColumn(\"Minimun Salary\", min(col(\"salary\")).over(salWindow))\\\n",
    "     .withColumn(\"Maximum Salary\", max(col(\"salary\")).over(salWindow)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+----------+-----------------+------------+--------------+--------------+\n",
      "|emp_no|salary| from_date|   to_date|hired_year|   Average Salary|Total Salary|Minimun Salary|Maximum Salary|\n",
      "+------+------+----------+----------+----------+-----------------+------------+--------------+--------------+\n",
      "| 10001| 71046|1991-06-25|1992-06-24|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10004| 50594|1991-11-30|1992-11-29|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10005| 83735|1991-09-12|1992-09-11|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10006| 42085|1991-08-05|1992-08-04|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10007| 62745|1991-02-10|1992-02-10|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10009| 71434|1991-02-17|1992-02-17|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10011| 44200|1991-01-22|1992-01-22|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10013| 47118|1991-10-19|1992-10-18|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10018| 61217|1991-04-02|1992-04-01|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10021| 60851|1991-02-09|1992-02-09|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10025| 50980|1991-08-16|1992-08-15|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10028| 48859|1991-10-22|1992-10-21|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10029| 63163|1991-09-18|1992-09-17|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10031| 40000|1991-09-01|1992-08-31|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10032| 49389|1991-06-20|1992-06-19|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10033| 56038|1991-03-17|1992-03-16|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10035| 48360|1991-09-05|1992-09-04|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10037| 39765|1991-12-05|1992-12-04|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10038| 46509|1991-09-20|1992-09-19|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "| 10039| 46278|1991-01-18|1992-01-18|      1991|58803.86967667994|  7798804412|         39115|        134624|\n",
      "+------+------+----------+----------+----------+-----------------+------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salDF.withColumn(\"Average Salary\",avg(col(\"salary\")).over(salWindow))\\\n",
    "     .withColumn(\"Total Salary\",sum(col(\"salary\")).over(salWindow))\\\n",
    "     .withColumn(\"Minimun Salary\",min(col(\"salary\")).over(salWindow))\\\n",
    "     .withColumn(\"Maximum Salary\",max(col(\"salary\")).over(salWindow)).where(col(\"hired_year\") == 1991).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4 Grouping Set   \n",
    "`Grouping Set` is used for complex aggregations. For example, if we want to aggregate data based on (salary, year), (salary), and (year) in a single statement then `grouping set` is used. It is only availabe in Spark SQL. To apply in DataFrame API, we need to use `rollup` and `cube`.   \n",
    "\n",
    "For example: Find the total project based on set of (team, language), (team) and (language). \n",
    "\n",
    "SELECT team, language, sum(project_id) FROM projects GROUP BY GROUPING SETS ((team, language), (team), (language));\n",
    "\n",
    "| team | language | sum(project) |   \n",
    "| ------- | -------- | ------------ |\n",
    "| A  | Python | 15 |\n",
    "| B  | Java | 2 |   \n",
    "| B  | Scala | 1 |   \n",
    "| C  | Python | 5 |   \n",
    "| C  | Scala | 9 |   \n",
    "| A  | Scala | 3 |   \n",
    "| B  | Python | 10 |     \n",
    "| C  || 14 |   \n",
    "| A  || 18 |   \n",
    "| B  || 13 |      \n",
    "|| Java  | 2 |   \n",
    "|| Python | 30 |   \n",
    "|| Scala | 13 |    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4.1 Rollup    \n",
    "`rollup` is used to perform multi-dimensional aggregation over combination of groups of specified columns. It returns RelationalGroupedDataset where aggregate function can be applied. For example, if we want to perform rollup on *(col1, col2, col3)*, it will compute subtotal for the combinations *(col1, col2, col3), (col2, col3) and (col1)*. Basically, rollup is the extension of `groupBy`.\n",
    "\n",
    "For example: Find the total salary for all hire_date and dept_name. `rollup` performs aggregation over group of  hired_date and dept_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- emp_no: long (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- from_date_x: date (nullable = true)\n",
      " |-- to_date_x: date (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date_y: date (nullable = true)\n",
      " |-- to_date_y: date (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highest_salary_employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------+\n",
      "| hire_date|       dept_name|total_sum|\n",
      "+----------+----------------+---------+\n",
      "|      null|            null|  2205189|\n",
      "|1985-09-17|            null|    97338|\n",
      "|1985-09-17|           Sales|    97338|\n",
      "|1985-10-14|            null|   192644|\n",
      "|1985-10-14|     Development|    96322|\n",
      "|1985-10-14|        Research|    96322|\n",
      "|1986-02-11|           Sales|   105565|\n",
      "|1986-02-11|            null|   105565|\n",
      "|1986-02-26|            null|   103672|\n",
      "|1986-02-26|     Development|   103672|\n",
      "|1986-03-01|            null|   104747|\n",
      "|1986-03-01|           Sales|   104747|\n",
      "|1986-09-08|           Sales|   102651|\n",
      "|1986-09-08|            null|   102651|\n",
      "|1986-11-05|           Sales|    97077|\n",
      "|1986-11-05|            null|    97077|\n",
      "|1986-11-16|            null|   100182|\n",
      "|1986-11-16|     Development|   100182|\n",
      "|1988-09-02|Customer Service|    98003|\n",
      "|1988-09-02|            null|   196006|\n",
      "|1988-09-02|           Sales|    98003|\n",
      "|1988-10-18|           Sales|    98538|\n",
      "|1988-10-18|            null|    98538|\n",
      "|1989-01-30|           Sales|   102165|\n",
      "|1989-01-30|            null|   102165|\n",
      "|1990-02-16|            null|    96471|\n",
      "|1990-02-16| Human Resources|    96471|\n",
      "|1990-04-10|            null|   109501|\n",
      "|1990-04-10|           Sales|   109501|\n",
      "|1990-12-25|           Sales|    97830|\n",
      "+----------+----------------+---------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total salary for all hire_date and dept_name.\n",
    "\n",
    "highest_salary_employee.rollup(\"hire_date\", \"dept_name\").agg(sum(\"salary\"))\\\n",
    "                       .selectExpr(\"hire_date\", \"dept_name\", \"`sum(salary)` as total_sum\")\\\n",
    "                       .orderBy(\"hire_date\").show(30)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4.2 Cube    \n",
    "`cube` is used to perform multi-dimensional aggregation across every permutation of specified columns. It returns RelationalGroupedDataset where aggregate function can be applied. `cube` perform more than `rollup`. It perform `rollup` with aggregation over all the missing combination of specified columns. The `nulls` values in all specified column gives the grand total across those columns. Basically, cube is an extension of `rollup`.\n",
    "For example, if we want to perform cube on *(col1, col2, col3)*, it will compute subtotal for the combinations *(col1, col2), (col1, col3), (col1), (col2, col3), (col2) and (col3)*.\n",
    "\n",
    "\n",
    "For example: Find the total salary across all hire_date and dept_name. `cube` performs aggregation for both hired_date and dept_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------+\n",
      "| hire_date|       dept_name|total_sum|\n",
      "+----------+----------------+---------+\n",
      "|      null| Human Resources|    96471|\n",
      "|      null|         Finance|   302829|\n",
      "|      null|        Research|    96322|\n",
      "|      null|      Production|    96646|\n",
      "|      null|       Marketing|    99651|\n",
      "|      null|           Sales|  1115091|\n",
      "|      null|     Development|   300176|\n",
      "|      null|            null|  2205189|\n",
      "|      null|Customer Service|    98003|\n",
      "|1985-09-17|           Sales|    97338|\n",
      "|1985-09-17|            null|    97338|\n",
      "|1985-10-14|        Research|    96322|\n",
      "|1985-10-14|     Development|    96322|\n",
      "|1985-10-14|            null|   192644|\n",
      "|1986-02-11|            null|   105565|\n",
      "|1986-02-11|           Sales|   105565|\n",
      "|1986-02-26|     Development|   103672|\n",
      "|1986-02-26|            null|   103672|\n",
      "|1986-03-01|            null|   104747|\n",
      "|1986-03-01|           Sales|   104747|\n",
      "|1986-09-08|           Sales|   102651|\n",
      "|1986-09-08|            null|   102651|\n",
      "|1986-11-05|            null|    97077|\n",
      "|1986-11-05|           Sales|    97077|\n",
      "|1986-11-16|            null|   100182|\n",
      "|1986-11-16|     Development|   100182|\n",
      "|1988-09-02|           Sales|    98003|\n",
      "|1988-09-02|Customer Service|    98003|\n",
      "|1988-09-02|            null|   196006|\n",
      "|1988-10-18|           Sales|    98538|\n",
      "+----------+----------------+---------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total salary across all hire_date and dept_name.\n",
    "\n",
    "highest_salary_employee.cube(\"hire_date\", \"dept_name\").agg(sum(\"salary\"))\\\n",
    "                       .selectExpr(\"hire_date\", \"dept_name\", \"`sum(salary)` as total_sum\")\\\n",
    "                       .orderBy(\"hire_date\").show(30)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**grouping_id**    \n",
    "`grouping_id` is used to specify level of aggregation while performing `cubes` and `rollups`. It helps to filter the aggregation by specifying the id.\n",
    "\n",
    "The grouping id generated for the code snippet below is shown in table.\n",
    "\n",
    "| Grouping ID | Description |\n",
    "| ----------- | ----------- |\n",
    "| 0 | Total sum of *hired_date* and *dept_name* | \n",
    "| 1 | Total sum for *hired_date* |\n",
    "| 2 | Total sum per *dept_name* |\n",
    "| 3 | Total sum regardless of *hire_date* and *dept_name* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+-----------+\n",
      "| hire_date|       dept_name|grouping_id()|sum(salary)|\n",
      "+----------+----------------+-------------+-----------+\n",
      "|      null|     Development|            2|     300176|\n",
      "|      null|            null|            3|    2205189|\n",
      "|      null| Human Resources|            2|      96471|\n",
      "|      null|           Sales|            2|    1115091|\n",
      "|      null|       Marketing|            2|      99651|\n",
      "|      null|         Finance|            2|     302829|\n",
      "|      null|        Research|            2|      96322|\n",
      "|      null|      Production|            2|      96646|\n",
      "|      null|Customer Service|            2|      98003|\n",
      "|1985-09-17|           Sales|            0|      97338|\n",
      "|1985-09-17|            null|            1|      97338|\n",
      "|1985-10-14|     Development|            0|      96322|\n",
      "|1985-10-14|            null|            1|     192644|\n",
      "|1985-10-14|        Research|            0|      96322|\n",
      "|1986-02-11|           Sales|            0|     105565|\n",
      "|1986-02-11|            null|            1|     105565|\n",
      "|1986-02-26|            null|            1|     103672|\n",
      "|1986-02-26|     Development|            0|     103672|\n",
      "|1986-03-01|            null|            1|     104747|\n",
      "|1986-03-01|           Sales|            0|     104747|\n",
      "|1986-09-08|            null|            1|     102651|\n",
      "|1986-09-08|           Sales|            0|     102651|\n",
      "|1986-11-05|           Sales|            0|      97077|\n",
      "|1986-11-05|            null|            1|      97077|\n",
      "|1986-11-16|            null|            1|     100182|\n",
      "|1986-11-16|     Development|            0|     100182|\n",
      "|1988-09-02|            null|            1|     196006|\n",
      "|1988-09-02|Customer Service|            0|      98003|\n",
      "|1988-09-02|           Sales|            0|      98003|\n",
      "|1988-10-18|           Sales|            0|      98538|\n",
      "|1988-10-18|            null|            1|      98538|\n",
      "|1989-01-30|            null|            1|     102165|\n",
      "|1989-01-30|           Sales|            0|     102165|\n",
      "|1990-02-16| Human Resources|            0|      96471|\n",
      "|1990-02-16|            null|            1|      96471|\n",
      "|1990-04-10|            null|            1|     109501|\n",
      "|1990-04-10|           Sales|            0|     109501|\n",
      "|1990-12-25|            null|            1|     195660|\n",
      "|1990-12-25|         Finance|            0|      97830|\n",
      "|1990-12-25|           Sales|            0|      97830|\n",
      "|1992-03-21|         Finance|            0|     109964|\n",
      "|1992-03-21|            null|            1|     109964|\n",
      "|1993-03-21|         Finance|            0|      95035|\n",
      "|1993-03-21|            null|            1|      95035|\n",
      "|1993-08-03|       Marketing|            0|      99651|\n",
      "|1993-08-03|            null|            1|      99651|\n",
      "|1994-03-22|           Sales|            0|     101676|\n",
      "|1994-03-22|            null|            1|     101676|\n",
      "|1997-05-19|      Production|            0|      96646|\n",
      "|1997-05-19|            null|            1|      96646|\n",
      "+----------+----------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import grouping_id\n",
    "\n",
    "highest_salary_employee.cube(\"hire_date\", \"dept_name\").agg(grouping_id(), sum(\"salary\"))\\\n",
    "                       .orderBy(\"hire_date\").show(50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Pivot       \n",
    "`pivot` is used to convert row into a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n",
      "+------+---------------+----------+----------+\n",
      "|emp_no|          title| from_date|   to_date|\n",
      "+------+---------------+----------+----------+\n",
      "| 10001|Senior Engineer|1986-06-26|9999-01-01|\n",
      "| 10002|          Staff|1996-08-03|9999-01-01|\n",
      "| 10003|Senior Engineer|1995-12-03|9999-01-01|\n",
      "| 10004|       Engineer|1986-12-01|1995-12-01|\n",
      "| 10004|Senior Engineer|1995-12-01|9999-01-01|\n",
      "| 10005|   Senior Staff|1996-09-12|9999-01-01|\n",
      "| 10005|          Staff|1989-09-12|1996-09-12|\n",
      "| 10006|Senior Engineer|1990-08-05|9999-01-01|\n",
      "| 10007|   Senior Staff|1996-02-11|9999-01-01|\n",
      "| 10007|          Staff|1989-02-10|1996-02-11|\n",
      "+------+---------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+------------------+--------+-------+---------------+------------+-------+----------------+\n",
      "| from_date|Assistant Engineer|Engineer|Manager|Senior Engineer|Senior Staff|  Staff|Technique Leader|\n",
      "+----------+------------------+--------+-------+---------------+------------+-------+----------------+\n",
      "|1992-02-21|            452136| 3127075|   null|        4668682|     8682783|3111224|          258350|\n",
      "|1991-07-30|            310032| 5318034|   null|        2867543|     3541417|5852427|           99893|\n",
      "|1987-09-15|            450848| 3556108|   null|        1248374|     1379939|5331336|          952337|\n",
      "|2000-07-03|              null|  966266|   null|        4597985|     5181754|   null|            null|\n",
      "|1995-12-01|            503224| 7361094|   null|        6556760|     4599803|3779661|          239563|\n",
      "|1987-07-08|            491818| 5513797|   null|        2043437|     1301069|4474675|          750449|\n",
      "|1989-11-10|            846903| 5757276|   null|        1294114|     1117631|5909475|          515494|\n",
      "|1985-02-09|           1774092| 3926460|   null|        2494087|      334433|3044665|         1183674|\n",
      "|1999-08-14|              null| 3109411|   null|        4158291|     6880751|3059889|         1162383|\n",
      "|1997-02-08|            295043| 3608628|   null|        6005952|     7521521|4270887|          605563|\n",
      "|1998-05-16|            227638| 6937802|   null|        5179270|     8058536|3745732|          291974|\n",
      "|1990-02-05|            274957| 4023240|   null|        1577328|     2645204|4437956|          532270|\n",
      "|1989-10-28|           1021044| 5458271|   null|         319461|     1321297|3757309|         1961784|\n",
      "|1988-03-25|            625838| 5624539|   null|        1921513|     1234678|5017384|           88682|\n",
      "|2000-03-17|              null|  261234|   null|        5477509|     4917776|   null|            null|\n",
      "|1990-09-07|            777476| 5445366|   null|        1683679|     1375624|4120893|          786696|\n",
      "|1995-10-24|            605268| 5468576|   null|        5167811|     4142328|4318415|          281046|\n",
      "|1993-07-11|             76066| 3421369|   null|        3290727|     4935874|6046565|          943828|\n",
      "|1988-03-21|           1139875| 4418866|   null|        1416289|     1127180|4989694|          923895|\n",
      "|1988-02-16|           1610934| 5407037|   null|         415796|     1156590|7332765|          189709|\n",
      "+----------+------------------+--------+-------+---------------+------------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles.printSchema()\n",
    "titles.show(10)\n",
    "titles.groupby(\"from_date\").pivot(\"title\").sum().show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 User Defined Aggregate Functions    \n",
    "User-Defined Aggregate Function (UDAF) is a custom built aggregate function. This function calculates over group of records compared to single records in UDF. Spark uses `AggregationBuffer` to store intermediate resultset during aggregate function.   \n",
    "\n",
    "To creating UDAF:   \n",
    "* Inherit `UserDefinedAggregateFunction` class \n",
    "* Implement following methods:    \n",
    "    * `inputSchema`\n",
    "    * `bufferSchema`\n",
    "    * `dataType`\n",
    "    * `deterministic`\n",
    "    * `initialize`\n",
    "    * `update`\n",
    "    * `merge`\n",
    "    * `evaluate`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
